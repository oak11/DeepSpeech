
Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions, but that several authors have defined as more specific institutions based on non-hierarchical free associations. While anti-statism is central, anarchism entails opposing authority or hierarchical organisation in the conduct of human relations, including, but not limited to, the state system.

As an anti-dogmatic philosophy, anarchism draws on many currents of thought and strategy. Anarchism does not offer a fixed body of doctrine from a single particular world view, instead fluxing and flowing as a philosophy. There are many types and traditions of anarchism, not all of which are mutually exclusive. Anarchist schools of thought can differ fundamentally, supporting anything from extreme individualism to complete collectivism. Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications. Anarchism is usually considered a radical left-wing ideology, and much of anarchist economics and anarchist legal philosophy reflect anti-authoritarian interpretations of communism, collectivism, syndicalism, mutualism, or participatory economics.

The term  is a compound word composed from the word anarchy and the suffix -ism, themselves derived respectively from the Greek , i.e. anarchy (from , anarchos, meaning "one without rulers"; from the privative prefix á¼€Î½- (an-, i.e. "without") and , archos, i.e. "leader", "ruler"; (cf. archon or , arkhÄ“, i.e. "authority", "sovereignty", "realm", "magistracy")) and the suffix  or  (-ismos, -isma, from the verbal infinitive suffix -Î¯Î¶ÎµÎ¹Î½, -izein). The first known use of this word was in 1539. Various factions within the French Revolution labelled opponents as anarchists (as Robespierre did the HÃ©bertists) although few shared many views of later anarchists.  There would be many revolutionaries of the early nineteenth century who contributed to the anarchist doctrines of the next generation, such as William Godwin and Wilhelm Weitling, but they did not use the word anarchist or anarchism in describing themselves or their beliefs.

The first political philosopher to call himself an anarchist was Pierre-Joseph Proudhon, marking the formal birth of anarchism in the mid-nineteenth century. Since the 1890s, and beginning in France, the term libertarianism has often been used as a synonym for anarchism and was used almost exclusively in this sense until the 1950s in the United States; its use as a synonym is still common outside the United States. On the other hand, some use libertarianism to refer to individualistic free-market philosophy only, referring to free-market anarchism as libertarian anarchism.

The earliest anarchist themes can be found in the 6th century BC, among the works of Taoist philosopher Laozi, and in later centuries by Zhuangzi and Bao Jingyan. Zhuangzi's philosophy has been described by various sources as anarchist. Zhuangzi wrote, "A petty thief is put in jail. A great  becomes a ruler of a Nation." Diogenes of Sinope and the Cynics, their contemporary Zeno of Citium, the founder of Stoicism, also introduced similar topics. Jesus is sometimes considered the first anarchist in the Christian anarchist tradition. Georges Lechartier wrote that "The true founder of anarchy was Jesus Christ and... the first anarchist society was that of the apostles." In early Islamic history, some manifestations of anarchic thought are found during the Islamic civil war over the Caliphate, where the Kharijites insisted that the imamate is a right for each individual within the Islamic society. Later, some Muslim scholars, such as Amer al-Basri and Abu Hanifa, led movements of boycotting the rulers, paving the way to the waqf (endowments) tradition, which served as an alternative to and asylum from the centralized authorities of the emirs. But such interpretations reverberates subversive religious conceptions like the aforementioned seemingly anarchistic Taoist teachings and that of other anti-authoritarian religious traditions creating a complex relationship regarding the question as to whether or not anarchism and religion are compatible. This is exemplified when the glorification of the state is viewed as a form of sinful idolatry.

The French renaissance political philosopher Ã‰tienne de La BoÃ©tie wrote in his most famous work the Discourse on Voluntary Servitude what some historians consider an important anarchist precedent.
The radical Protestant Christian Gerrard Winstanley and his group the Diggers are cited by various authors as proposing anarchist social measures in the 17th century in England. The term "anarchist" first entered the English language in 1642, during the English Civil War, as a term of abuse, used by Royalists against their Roundhead opponents. By the time of the French Revolution some, such as the EnragÃ©s, began to use the term positively, in opposition to Jacobin centralisation of power, seeing "revolutionary government" as oxymoronic. By the turn of the 19th century, the English word "anarchism" had lost its initial negative connotation.

Modern anarchism sprang from the secular or religious thought of the Enlightenment, particularly Jean-Jacques Rousseau's arguments for the moral centrality of freedom.

As part of the political turmoil of the 1790s in the wake of the French Revolution, William Godwin developed the first expression of modern anarchist thought. Godwin was, according to Peter Kropotkin, "the first to formulate the political and economical conceptions of anarchism, even though he did not give that name to the ideas developed in his work", while Godwin attached his anarchist ideas to an early Edmund Burke.
Godwin is generally regarded as the founder of the school of thought known as 'philosophical anarchism'. He argued in Political Justice (1793) that government has an inherently malevolent influence on society, and that it perpetuates dependency and ignorance. He thought that the spread of the use of reason to the masses would eventually cause government to wither away as an unnecessary force. Although he did not accord the state with moral legitimacy, he was against the use of revolutionary tactics for removing the government from power. Rather, he advocated for its replacement through a process of peaceful evolution.

His aversion to the imposition of a rules-based society led him to denounce, as a manifestation of the peopleâ€™s â€˜mental enslavementâ€™, the foundations of law, property rights and even the institution of marriage. He considered the basic foundations of society as constraining the natural development of individuals to use their powers of reasoning to arrive at a mutually beneficial method of social organisation. In each case, government and its institutions are shown to constrain the development of our capacity to live wholly in accordance with the full and free exercise of private judgment.

The French Pierre-Joseph Proudhon is regarded as the first self-proclaimed anarchist, a label he adopted in his groundbreaking work, What is Property?, published in 1840. It is for this reason that some claim Proudhon as the founder of modern anarchist theory. He developed the theory of spontaneous order in society, where organisation emerges without a central coordinator imposing its own idea of order against the wills of individuals acting in their own interests; his famous quote on the matter is, "Liberty is the mother, not the daughter, of order." In What is Property? Proudhon answers with the famous accusation "Property is theft." In this work, he opposed the institution of decreed "property" (propriÃ©tÃ©), where owners have complete rights to "use and abuse" their property as they wish. He contrasted this with what he called "possession," or limited ownership of resources and goods only while in more or less continuous use. Later, however, Proudhon added that "Property is Liberty," and argued that it was a bulwark against state power. His opposition to the state, organised religion, and certain capitalist practices inspired subsequent anarchists, and made him one of the leading social thinkers of his time.

The anarcho-communist Joseph DÃ©jacque was the first person to describe himself as "libertarian". Unlike Pierre-Joseph Proudhon, he argued that, "it is not the product of his or her labour that the worker has a right to, but to the satisfaction of his or her needs, whatever may be their nature." In 1844 in Germany the post-hegelian philosopher Max Stirner published the book, The Ego and Its Own, which would later be considered an influential early text of individualist anarchism. French anarchists active in the 1848 Revolution included Anselme Bellegarrigue, Ernest Coeurderoy, Joseph DÃ©jacque and Pierre Joseph Proudhon.

In Europe, harsh reaction followed the revolutions of 1848, during which ten countries had experienced brief or long-term social upheaval as groups carried out nationalist uprisings. After most of these attempts at systematic change ended in failure, conservative elements took advantage of the divided groups of socialists, anarchists, liberals, and nationalists, to prevent further revolt. In Spain RamÃ³n de la Sagra established the anarchist journal El Porvenir in La CoruÃ±a in 1845 which was inspired by ProudhonÂ´s ideas. The Catalan politician Francesc Pi i Margall became the principal translator of Proudhon's works into Spanish and later briefly became president of Spain in 1873 while being the leader of the Democratic Republican Federal Party. According to George Woodcock "These translations were to have a profound and lasting effect on the development of Spanish anarchism after 1870, but before that time Proudhonian ideas, as interpreted by Pi, already provided much of the inspiration for the federalist movement which sprang up in the early 1860's." According to the Encyclopedia Britannica "During the Spanish revolution of 1873, Pi y Margall attempted to establish a decentralized, or "cantonalist," political system on Proudhonian lines."

In 1864 the International Workingmen's Association (sometimes called the "First International") united diverse revolutionary currents including French followers of Proudhon, Blanquists, Philadelphes, English trade unionists, socialists and social democrats. Due to its links to active workers' movements, the International became a significant organisation. Karl Marx became a leading figure in the International and a member of its General Council. Proudhon's followers, the mutualists, opposed Marx's state socialism, advocating political abstentionism and small property holdings. Woodcock also reports that the American individualist anarchists Lysander Spooner and William B. Greene had been members of the First International. In 1868, following their unsuccessful participation in the League of Peace and Freedom (LPF), Russian revolutionary Mikhail Bakunin and his collectivist anarchist associates joined the First International (which had decided not to get involved with the LPF). They allied themselves with the federalist socialist sections of the International, who advocated the revolutionary overthrow of the state and the collectivization of property.

At first, the collectivists worked with the Marxists to push the First International in a more revolutionary socialist direction. Subsequently, the International became polarised into two camps, with Marx and Bakunin as their respective figureheads. Bakunin characterised Marx's ideas as centralist and predicted that, if a Marxist party came to power, its leaders would simply take the place of the ruling class they had fought against. Anarchist historian George Woodcock reports that "The annual Congress of the International had not taken place in 1870 owing to the outbreak of the Paris Commune, and in 1871 the General Council called only a special conference in London. One delegate was able to attend from Spain and none from Italy, while a technical excuse â€“ that they had split away from the FÃ©dÃ©ration Romande â€“ was used to avoid inviting Bakunin's Swiss supporters. Thus only a tiny minority of anarchists was present, and the General Council's resolutions passed almost unanimously. Most of them were clearly directed against Bakunin and his followers." In 1872, the conflict climaxed with a final split between the two groups at the Hague Congress, where Bakunin and James Guillaume were expelled from the International and its headquarters were transferred to New York. In response, the federalist sections formed their own International at the St. Imier Congress, adopting a revolutionary anarchist program.

The Paris Commune was a government that briefly ruled Paris from 18 March (more formally, from 28 March) to 28 May 1871. The Commune was the result of an uprising in Paris after France was defeated in the Franco-Prussian War. Anarchists participated actively in the establishment of the Paris Commune. They included  George Woodcock states:

The anti-authoritarian sections of the First International were the precursors of the anarcho-syndicalists, seeking to "replace the privilege and authority of the State" with the "free and spontaneous organisation of labour." In 1886, the Federation of Organized Trades and Labor Unions (FOTLU) of the United States and Canada unanimously set 1 May 1886, as the date by which the eight-hour work day would become standard.

In response, unions across the United States prepared a general strike in support of the event. On 3 May, in Chicago, a fight broke out when strikebreakers attempted to cross the picket line, and two workers died when police opened fire upon the crowd. The next day, 4 May, anarchists staged a rally at Chicago's Haymarket Square. A bomb was thrown by an unknown party near the conclusion of the rally, killing an officer. In the ensuing panic, police opened fire on the crowd and each other. Seven police officers and at least four workers were killed. Eight anarchists directly and indirectly related to the organisers of the rally were arrested and charged with the murder of the deceased officer. The men became international political celebrities among the labour movement. Four of the men were executed and a fifth committed suicide prior to his own execution. The incident became known as the Haymarket affair, and was a setback for the labour movement and the struggle for the eight-hour day. In 1890 a second attempt, this time international in scope, to organise for the eight-hour day was made. The event also had the secondary purpose of memorializing workers killed as a result of the Haymarket affair. Although it had initially been conceived as a once-off event, by the following year the celebration of International Workers' Day on May Day had become firmly established as an international worker's holiday.

In 1907, the International Anarchist Congress of Amsterdam gathered delegates from 14 different countries, among which important figures of the anarchist movement, including Errico Malatesta, Pierre Monatte, Luigi Fabbri, BenoÃ®t Broutchoux, Emma Goldman, Rudolf Rocker, and Christiaan Cornelissen. Various themes were treated during the Congress, in particular concerning the organisation of the anarchist movement, popular education issues, the general strike or antimilitarism. A central debate concerned the relation between anarchism and syndicalism (or trade unionism). Malatesta and Monatte were in particular disagreement themselves on this issue, as the latter thought that syndicalism was revolutionary and would create the conditions of a social revolution, while Malatesta did not consider syndicalism by itself sufficient. He thought that the trade-union movement was reformist and even conservative, citing as essentially bourgeois and anti-worker the phenomenon of professional union officials. Malatesta warned that the syndicalists aims were in perpetuating syndicalism itself, whereas anarchists must always have anarchy as their end and consequently refrain from committing to any particular method of achieving it.

The Spanish Workers Federation in 1881 was the first major anarcho-syndicalist movement; anarchist trade union federations were of special importance in Spain. The most successful was the ConfederaciÃ³n Nacional del Trabajo (National Confederation of Labour: CNT), founded in 1910. Before the 1940s, the CNT was the major force in Spanish working class politics, attracting 1.58million members at one point and playing a major role in the Spanish Civil War. The CNT was affiliated with the International Workers Association, a federation of anarcho-syndicalist trade unions founded in 1922, with delegates representing two million workers from 15 countries in Europe and Latin America. In Latin America in particular "The anarchists quickly became active in organizing craft and industrial workers throughout South and Central America, and until the early 1920s most of the trade unions in Mexico, Brazil, Peru, Chile, and Argentina were anarcho-syndicalist in general outlook; the prestige of the Spanish C.N.T. as a revolutionary organization was
undoubtedly to a great extent responsible for this situation. The largest and most militant of these organizations was the FederaciÃ³n Obrera Regional Argentina... it grew quickly to a membership of nearly a quarter of a million, which dwarfed the rival socialdemocratic unions."

Some anarchists, such as Johann Most, advocated publicizing violent acts of retaliation against counter-revolutionaries because "we preach not only action in and for itself, but also action as propaganda." By the 1880s, people inside and outside the anarchist movement began to use the slogan, "propaganda of the deed" to refer to individual bombings, regicides, and tyrannicides. From 1905 onwards, the Russian counterparts of these anti-syndicalist anarchist-communists become partisans of economic terrorism and illegal 'expropriations'." Illegalism as a practice emerged and within it "The acts of the anarchist bombers and assassins ("propaganda by the deed") and the anarchist burglars ("individual reappropriation") expressed their desperation and their personal, violent rejection of an intolerable society. Moreover, they were clearly meant to be exemplary invitations to revolt.". France's Bonnot Gang was the most famous group to embrace illegalism.

However, as soon as 1887, important figures in the anarchist movement distanced themselves from such individual acts. Peter Kropotkin thus wrote that year in Le RÃ©voltÃ© that "a structure based on centuries of history cannot be destroyed with a few kilos of dynamite". A variety of anarchists advocated the abandonment of these sorts of tactics in favour of collective revolutionary action, for example through the trade union movement. The anarcho-syndicalist, Fernand Pelloutier, argued in 1895 for renewed anarchist involvement in the labour movement on the basis that anarchism could do very well without "the individual dynamiter."

State repression (including the infamous 1894 French lois scÃ©lÃ©rates) of the anarchist and labour movements following the few successful bombings and assassinations may have contributed to the abandonment of these kinds of tactics, although reciprocally state repression, in the first place, may have played a role in these isolated acts. The dismemberment of the French socialist movement, into many groups and, following the suppression of the 1871 Paris Commune, the execution and exile of many communards to penal colonies, favoured individualist political expression and acts. According to some analysts, in post-war Germany, the prohibition of the Communist Party (KDP) and thus of institutional far-left political organisation may also, in the same manner, have played a role in the creation of the Red Army Faction.

Numerous heads of state were assassinated between 1881 and 1914 by members of the anarchist movement, including Tsar Alexander II of Russia, President Sadi Carnot of France, Empress Elisabeth of Austria, King Umberto I of Italy, President William McKinley of the United States, King Carlos I of Portugal and King George I of Greece. McKinley's assassin Leon Czolgosz claimed to have been influenced by anarchist and feminist Emma Goldman.

Propaganda of the deed was abandoned by the vast majority of the anarchist movement after World War I (1914â€“1918) and the 1917 October Revolution.

Anarchists participated alongside the Bolsheviks in both February and October revolutions, and were initially enthusiastic about the Bolshevik revolution. However, following a political falling out with the Bolsheviks by the anarchists and other left-wing opposition, the conflict culminated in the 1921 Kronstadt rebellion, which the new government repressed. Anarchists in central Russia were either imprisoned, driven underground or joined the victorious Bolsheviks; the anarchists from Petrograd and Moscow fled to the Ukraine. There, in the Free Territory, they fought in the civil war against the Whites (a grouping of monarchists and other opponents of the October Revolution) and then the Bolsheviks as part of the Revolutionary Insurrectionary Army of Ukraine led by Nestor Makhno, who established an anarchist society in the region for a number of months.

Expelled American anarchists Emma Goldman and Alexander Berkman were amongst those agitating in response to Bolshevik policy and the suppression of the Kronstadt uprising, before they left Russia. Both wrote accounts of their experiences in Russia, criticising the amount of control the Bolsheviks exercised. For them, Bakunin's predictions about the consequences of Marxist rule that the rulers of the new "socialist" Marxist state would become a new elite had proved all too true.

The victory of the Bolsheviks in the October Revolution and the resulting Russian Civil War did serious damage to anarchist movements internationally. Many workers and activists saw Bolshevik success as setting an example; Communist parties grew at the expense of anarchism and other socialist movements. In France and the United States, for example, members of the major syndicalist movements of the CGT and IWW left the organisations and joined the Communist International.

The revolutionary wave of 1917â€“23 saw the active participation of anarchists in varying degrees of protagonism. In the German uprising known as the German Revolution of 1918â€“1919 which established the Bavarian Soviet Republic the anarchists Gustav Landauer, Silvio Gesell and Erich MÃ¼hsam had important leadership positions within the revolutionary councilist structures. In the Italian events known as the biennio rosso the anarcho-syndicalist trade union Unione Sindacale Italiana "grew to 800,000 members and the influence of the Italian Anarchist Union (20,000 members plus Umanita Nova, its daily paper) grew accordingly... Anarchists were the first to suggest occupying workplaces. In the Mexican Revolution the Mexican Liberal Party was established and during the early 1910s it led  a series of military offensives leading to the conquest and occupation of certain towns and districts in Baja California with the leadership of anarcho-communist Ricardo Flores MagÃ³n.

In Paris, the Dielo Truda group of Russian anarchist exiles, which included Nestor Makhno, concluded that anarchists needed to develop new forms of organisation in response to the structures of Bolshevism. Their 1926 manifesto, called the Organisational Platform of the General Union of Anarchists (Draft), was supported. Platformist groups active today include the Workers Solidarity Movement in Ireland and the North Eastern Federation of Anarchist Communists of North America. Synthesis anarchism emerged as an organisational alternative to platformism that tries to join anarchists of different tendencies under the principles of anarchism without adjectives. In the 1920s this form found as its main proponents Volin and Sebastien Faure. It is the main principle behind the anarchist federations grouped around the contemporary global International of Anarchist Federations.

In the 1920s and 1930s, the rise of fascism in Europe transformed anarchism's conflict with the state. Italy saw the first struggles between anarchists and fascists. Italian anarchists played a key role in the anti-fascist organisation Arditi del Popolo, which was strongest in areas with anarchist traditions, and achieved some success in their activism, such as repelling Blackshirts in the anarchist stronghold of Parma in August 1922. The veteran Italian anarchist, Luigi Fabbri, was one of the first critical theorists of fascism, describing it as "the preventive counter-revolution."  In France, where the far right leagues came close to insurrection in the February 1934 riots, anarchists divided over a united front policy.

Anarchists in France and Italy were active in the Resistance during World War II. In Germany the anarchist Erich MÃ¼hsam was arrested on charges unknown in the early morning hours of 28 February 1933, within a few hours after the Reichstag fire in Berlin. Joseph Goebbels, the Nazi propaganda minister, labelled him as one of "those Jewish subversives." Over the next seventeen months, he would be imprisoned in the concentration camps at Sonnenburg, Brandenburg and finally, Oranienburg. On 2 February 1934, MÃ¼hsam was transferred to the concentration camp at Oranienburg when finally on the night of 9 July 1934, MÃ¼hsam was tortured and murdered by the guards, his battered corpse found hanging in a latrine the next morning.


In Spain, the national anarcho-syndicalist trade union ConfederaciÃ³n Nacional del Trabajo initially refused to join a popular front electoral alliance, and abstention by CNT supporters led to a right wing election victory. But in 1936, the CNT changed its policy and anarchist votes helped bring the popular front back to power. Months later, the former ruling class responded with an attempted coup causing the Spanish Civil War (1936â€“1939). In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain where they collectivised the land. But even before the fascist victory in 1939, the anarchists were losing ground in a bitter struggle with the Stalinists, who controlled much of the distribution of military aid to the Republican cause from the Soviet Union. The events known as the Spanish Revolution was a workers' social revolution that began during the outbreak of the Spanish Civil War in 1936 and resulted in the widespread implementation of anarchist and more broadly libertarian socialist organisational principles throughout various portions of the country for two to three years, primarily Catalonia, Aragon, Andalusia, and parts of the Levante. Much of Spain's economy was put under worker control; in anarchist strongholds like Catalonia, the figure was as high as 75%, but lower in areas with heavy Communist Party of Spain influence, as the Soviet-allied party actively resisted attempts at collectivization enactment. Factories were run through worker committees, agrarian areas became collectivised and run as libertarian communes. Anarchist historian Sam Dolgoff estimated that about eight million people participated directly or at least indirectly in the Spanish Revolution, which he claimed "came closer to realizing the ideal of the free stateless society on a vast scale than any other revolution in history."

Stalinist-led troops suppressed the collectives and persecuted both dissident Marxists and anarchists. The prominent Italian anarchist Camillo Berneri, who volunteered to fight against Franco was killed instead in Spain by gunmen associated with the Spanish Communist Party.

Anarchism sought to reorganise itself after the war and in this context the organisational debate between synthesis anarchism and platformism took importance once again especially in the anarchist movements of Italy and France. The Mexican Anarchist Federation was established in 1945 after the Anarchist Federation of the Centre united with the Anarchist Federation of the Federal District. In the early 1940s, the Antifascist International Solidarity and the Federation of Anarchist Groups of Cuba merged into the large national organisation AsociaciÃ³n Libertaria de Cuba (Cuban Libertarian Association). From 1944 to 1947, the Bulgarian Anarchist Communist Federation reemerged as part of a factory and workplace committee movement, but was repressed by the new Communist regime. In 1945 in France the FÃ©dÃ©ration Anarchiste and the anarchosyndicalist trade union ConfÃ©dÃ©ration nationale du travail was established in the next year while the also synthesist Federazione Anarchica Italiana was founded in Italy. Korean anarchists formed the League of Free Social Constructors in September 1945 and in 1946 the Japanese Anarchist Federation was founded. An International Anarchist Congress with delegates from across Europe was held in Paris in May 1948. After World War II, an appeal in the Fraye Arbeter Shtime detailing the plight of German anarchists and called for Americans to support them. By February 1946, the sending of aid parcels to anarchists in Germany was a large-scale operation. The Federation of Libertarian Socialists was founded in Germany in 1947 and Rudolf Rocker wrote for its organ, Die Freie Gesellschaft, which survived until 1953. In 1956 the Uruguayan Anarchist Federation was founded. In 1955 the Anarcho-Communist Federation of Argentina renamed itself as the Argentine Libertarian Federation. The Syndicalist Workers' Federation was a syndicalist group in active in post-war Britain, and one of Solidarity Federation's earliest predecessors. It was formed in 1950 by members of the dissolved Anarchist Federation of Britain. Unlike the AFB, which was influenced by anarcho-syndicalist ideas but ultimately not syndicalist itself, the SWF decided to pursue a more definitely syndicalist, worker-centred strategy from the outset.

Anarchism continued to influence important literary and intellectual personalities of the time, such as Albert Camus, Herbert Read, Paul Goodman, Dwight Macdonald, Allen Ginsberg, George Woodcock, Leopold Kohr, Julian Beck, John Cage and the French Surrealist group led by AndrÃ© Breton, which now openly embraced anarchism and collaborated in the FÃ©dÃ©ration Anarchiste.

Anarcho-pacifism became influential in the Anti-nuclear movement and anti war movements of the time as can be seen in the activism and writings of the English anarchist member of Campaign for Nuclear Disarmament Alex Comfort or the similar activism of the American catholic anarcho-pacifists  Ammon Hennacy and Dorothy Day. Anarcho-pacifism became a "basis for a critique of militarism on both sides of the Cold War." The resurgence of anarchist ideas during this period is well documented in Robert Graham's , Volume Two: The Emergence of the New Anarchism (1939â€“1977).


 A surge of popular interest in anarchism occurred in western nations during the 1960s and 1970s. Anarchism was influential in the Counterculture of the 1960s and anarchists actively participated in the late sixties students and workers revolts. In 1968 in Carrara, Italy the International of Anarchist Federations was founded during an international anarchist conference held there in 1968 by the three existing European federations of France (the FÃ©dÃ©ration Anarchiste), the Federazione Anarchica Italiana of Italy and the Iberian Anarchist Federation as well as the Bulgarian federation in French exile.

In the United Kingdom in the 1970s this was associated with the punk rock movement, as exemplified by bands such as Crass and the Sex Pistols. The housing and employment crisis in most of Western Europe led to the formation of communes and squatter movements like that of Barcelona, Spain. In Denmark, squatters occupied a disused military base and declared the Freetown Christiania, an autonomous haven in central Copenhagen. Since the revival of anarchism in the mid 20th century, a number of new movements and schools of thought emerged. Although feminist tendencies have always been a part of the anarchist movement in the form of anarcha-feminism, they returned with vigour during the second wave of feminism in the 1960s. Anarchist anthropologist David Graeber and anarchist historian Andrej Grubacic have posited a rupture between generations of anarchism, with those "who often still have not shaken the sectarian habits" of the 19th century contrasted with the younger activists who are "much more informed, among other elements, by indigenous, feminist, ecological and cultural-critical ideas", and who by the turn of the 21st century formed "by far the majority" of anarchists.

Around the turn of the 21st century, anarchism grew in popularity and influence as part of the anti-war, anti-capitalist, and anti-globalisation movements. Anarchists became known for their involvement in protests against the meetings of the World Trade Organization (WTO), Group of Eight, and the World Economic Forum. Some anarchist factions at these protests engaged in rioting, property destruction, and violent confrontations with police. These actions were precipitated by ad hoc, leaderless, anonymous cadres known as black blocs; other organisational tactics pioneered in this time include security culture, affinity groups and the use of decentralised technologies such as the internet. A significant event of this period was the confrontations at WTO conference in Seattle in 1999. According to anarchist scholar Simon Critchley, "contemporary anarchism can be seen as a powerful critique of the pseudo-libertarianism of contemporary neo-liberalism... One might say that contemporary anarchism is about responsibility, whether sexual, ecological or socio-economic; it flows from an experience of conscience about the manifold ways in which the West ravages the rest; it is an ethical outrage at the yawning inequality, impoverishment and disenfranchisment that is so palpable locally and globally."

International anarchist federations in existence include the International of Anarchist Federations, the International Workers' Association, and International Libertarian Solidarity.
The largest organised anarchist movement today is in Spain, in the form of the ConfederaciÃ³n General del Trabajo (CGT) and the CNT. CGT membership was estimated at around 100,000 for 2003. Other active syndicalist movements include in Sweden the Central Organisation of the Workers of Sweden and the Swedish Anarcho-syndicalist Youth Federation; the CNT-AIT in France; the Union Sindicale Italiana in Italy; in the US Workers Solidarity Alliance  and the UK Solidarity Federation and Anarchist Federation. The revolutionary industrial unionist Industrial Workers of the World, claiming 2,000 paying members, and the International Workers Association, an anarcho-syndicalist successor to the First International, also remain active.

Anarchist schools of thought had been generally grouped in two main historical traditions, individualist anarchism and social anarchism, which have some different origins, values and evolution. The individualist wing of anarchism emphasises negative liberty, i.e. opposition to state or social control over the individual, while those in the social wing emphasise positive liberty to achieve one's potential and argue that humans have needs that society ought to fulfill, "recognizing equality of entitlement". In a chronological and theoretical sense, there are classical â€“ those created throughout the 19th century â€“ and post-classical anarchist schools â€“ those created since the mid-20th century and after.

Beyond the specific factions of anarchist thought is philosophical anarchism, which embodies the theoretical stance that the state lacks moral legitimacy without accepting the imperative of revolution to eliminate it. A component especially of individualist anarchism philosophical anarchism may accept the existence of a minimal state as unfortunate, and usually temporary, "necessary evil" but argue that citizens do not have a moral obligation to obey the state when its laws conflict with individual autonomy. One reaction against sectarianism within the anarchist milieu was "anarchism without adjectives", a call for toleration first adopted by Fernando Tarrida del MÃ¡rmol in 1889 in response to the "bitter debates" of anarchist theory at the time. In abandoning the hyphenated anarchisms (i.e. collectivist-, communist-, mutualistâ€“ and individualist-anarchism), it sought to emphasise the anti-authoritarian beliefs common to all anarchist schools of thought.



Mutualism began in 18th-century English and French labour movements before taking an anarchist form associated with Pierre-Joseph Proudhon in France and others in the United States. Proudhon proposed spontaneous order, whereby organisation emerges without central authority, a "positive anarchy" where order arises when everybody does "what he wishes and only what he wishes" and where "business transactions alone produce the social order." It is important to recognize that Proudhon distinguished between ideal political possibilities and practical governance. For this reason, much in contrast to some of his theoretical statements concerning ultimate spontaneous self-governance, Proudhon was heavily involved in French parliamentary politics and allied himself not with Anarchist but Socialist factions of workers movements and, in addition to advocating state-protected charters for worker-owned cooperatives, promoted certain nationalization schemes during his life of public service.

Mutualist anarchism is concerned with reciprocity, free association, voluntary contract, federation, and credit and currency reform. According to the American mutualist William Batchelder Greene, each worker in the mutualist system would receive "just and exact pay for his work; services equivalent in cost being exchangeable for services equivalent in cost, without profit or discount." Mutualism has been retrospectively characterised as ideologically situated between individualist and collectivist forms of anarchism.Blackwell Encyclopaedia of Political Thought, Blackwell Publishing 1991 ISBN 0-631-17944-5, p. 11. Proudhon first characterised his goal as a "third form of society, the synthesis of communism and property."

Individualist anarchism refers to several traditions of thought within the anarchist movement that emphasize the individual and their will over any kinds of external determinants such as groups, society, traditions, and ideological systems. Individualist anarchism is not a single philosophy but refers to a group of individualistic philosophies that sometimes are in conflict.

In 1793, William Godwin, who has often been cited as the first anarchist, wrote Political Justice, which some consider the first expression of anarchism. Godwin, a philosophical anarchist, from a rationalist and utilitarian basis opposed revolutionary action and saw a minimal state as a present "necessary evil" that would become increasingly irrelevant and powerless by the gradual spread of knowledge. Godwin advocated individualism, proposing that all cooperation in labour be eliminated on the premise that this would be most conducive with the general good.

An influential form of individualist anarchism, called "egoism," or egoist anarchism, was expounded by one of the earliest and best-known proponents of individualist anarchism, the German Max Stirner. Stirner's The Ego and Its Own, published in 1844, is a founding text of the philosophy. According to Stirner, the only limitation on the rights of the individual is their power to obtain what they desire, without regard for God, state, or morality. To Stirner, rights were spooks in the mind, and he held that society does not exist but "the individuals are its reality". Stirner advocated self-assertion and foresaw unions of egoists, non-systematic associations continually renewed by all parties' support through an act of will, which Stirner proposed as a form of organisation in place of the state. Egoist anarchists argue that egoism will foster genuine and spontaneous union between individuals. "Egoism" has inspired many interpretations of Stirner's philosophy. It was re-discovered and promoted by German philosophical anarchist and LGBT activist John Henry Mackay.

Josiah Warren is widely regarded as the first American anarchist, and the four-page weekly paper he edited during 1833, The Peaceful Revolutionist, was the first anarchist periodical published. For American anarchist historian Eunice Minette Schuster "It is apparent... that Proudhonian Anarchism was to be found in the United States at least as early as 1848 and that it was not conscious of its affinity to the Individualist Anarchism of Josiah Warren and Stephen Pearl Andrews... William B. Greene presented this Proudhonian Mutualism in its purest and most systematic form.". Henry David Thoreau (1817â€“1862) was an important early influence in individualist anarchist thought in the United States and Europe. Thoreau was an American author, poet, naturalist, tax resister, development critic, surveyor, historian, philosopher, and leading transcendentalist. He is best known for his books Walden, a reflection upon simple living in natural surroundings, and his essay, Civil Disobedience, an argument for individual resistance to civil government in moral opposition to an unjust state. Later Benjamin Tucker fused Stirner's egoism with the economics of Warren and Proudhon in his eclectic influential publication Liberty.

From these early influences individualist anarchism in different countries attracted a small but diverse following of bohemian artists and intellectuals, free love and birth control advocates (see Anarchism and issues related to love and sex), individualist naturists nudists (see anarcho-naturism), freethought and anti-clerical activists as well as young anarchist outlaws in what became known as illegalism and individual reclamation (see European individualist anarchism and individualist anarchism in France). These authors and activists included Oscar Wilde, Emile Armand, Han Ryner, Henri Zisly, Renzo Novatore, Miguel Gimenez Igualada, Adolf Brand and Lev Chernyi among others.

Social anarchism calls for a system with common ownership of means of production and democratic control of all organisations, without any government authority or coercion. It is the largest school of thought in anarchism. Social anarchism rejects private property, seeing it as a source of social inequality (while retaining respect for personal property), and emphasises cooperation and mutual aid.


Collectivist anarchism, also referred to as "revolutionary socialism" or a form of such, is a revolutionary form of anarchism, commonly associated with Mikhail Bakunin and Johann Most. Collectivist anarchists oppose all private ownership of the means of production, instead advocating that ownership be collectivised. This was to be achieved through violent revolution, first starting with a small cohesive group through acts of violence, or propaganda by the deed, which would inspire the workers as a whole to revolt and forcibly collectivise the means of production.

However, collectivization was not to be extended to the distribution of income, as workers would be paid according to time worked, rather than receiving goods being distributed "according to need" as in anarcho-communism. This position was criticised by anarchist communists as effectively "uphold the wages system". Collectivist anarchism arose contemporaneously with Marxism but opposed the Marxist dictatorship of the proletariat, despite the stated Marxist goal of a collectivist stateless society. Anarchist, communist and collectivist ideas are not mutually exclusive; although the collectivist anarchists advocated compensation for labour, some held out the possibility of a post-revolutionary transition to a communist system of distribution according to need.

Anarchist communism (also known as anarcho-communism, libertarian communism and occasionally as free communism) is a theory of anarchism that advocates abolition of the state, markets, money, private property (while retaining respect for personal property), and capitalism in favour of common ownership of the means of production, direct democracy and a horizontal network of voluntary associations and workers' councils with production and consumption based on the guiding principle: "from each according to his ability, to each according to his need".
Some forms of anarchist communism such as insurrectionary anarchism are strongly influenced by egoism and radical individualism, believing anarcho-communism is the best social system for the realization of individual freedom. Most anarcho-communists view anarcho-communism as a way of reconciling the opposition between the individual and society.

Anarcho-communism developed out of radical socialist currents after the French revolution but was first formulated as such in the Italian section of the First International. The theoretical work of Peter Kropotkin took importance later as it expanded and developed pro-organisationalist and insurrectionary anti-organisationalist sections. To date, the best known examples of an anarchist communist society (i.e., established around the ideas as they exist today and achieving worldwide attention and knowledge in the historical canon), are the anarchist territories during the Spanish Revolution and the Free Territory during the Russian Revolution. Through the efforts and influence of the Spanish Anarchists during the Spanish Revolution within the Spanish Civil War, starting in 1936 anarchist communism existed in most of Aragon, parts of the Levante and Andalusia, as well as in the stronghold of Anarchist Catalonia before being crushed by the combined forces of the regime that won the war, Hitler, Mussolini, Spanish Communist Party repression (backed by the USSR) as well as economic and armaments blockades from the capitalist countries and the Spanish Republic itself.  During the Russian Revolution, anarchists such as Nestor Makhno worked to create and defend â€“ through the Revolutionary Insurrectionary Army of Ukraine â€“ anarchist communism in the Free Territory of the Ukraine from 1919 before being conquered by the Bolsheviks in 1921.


Anarcho-syndicalism is a branch of anarchism that focuses on the labour movement. Anarcho-syndicalists view labour unions as a potential force for revolutionary social change, replacing capitalism and the state with a new society democratically self-managed by workers. The basic principles of anarcho-syndicalism are: Workers' , Direct action and Workers' self-management
Anarcho-syndicalists believe that only direct action â€“ that is, action concentrated on directly attaining a goal, as opposed to indirect action, such as electing a representative to a government position â€“ will allow workers to liberate themselves. Moreover, anarcho-syndicalists believe that workers' organisations (the organisations that struggle against the wage system, which, in anarcho-syndicalist theory, will eventually form the basis of a new society) should be self-managing. They should not have bosses or "business agents"; rather, the workers should be able to make all the decisions that affect them themselves. Rudolf Rocker was one of the most popular voices in the anarcho-syndicalist movement. He outlined a view of the origins of the movement, what it sought, and why it was important to the future of labour in his 1938 pamphlet Anarcho-Syndicalism. The International Workers Association is an international anarcho-syndicalist federation of various labour unions from different countries. The Spanish ConfederaciÃ³n Nacional del Trabajo played and still plays a major role in the Spanish labour movement. It was also an important force in the Spanish Civil War.

Anarchism continues to generate many philosophies and movements, at times eclectic, drawing upon various sources, and syncretic, combining disparate concepts to create new philosophical approaches.

Green anarchism (or eco-anarchism) is a school of thought within anarchism that emphasizes environmental issues, with an important precedent in anarcho-naturism, and whose main contemporary currents are anarcho-primitivism and social ecology.

Anarcha-feminism (also called anarchist feminism and anarcho-feminism) combines anarchism with feminism. It generally views patriarchy as a manifestation of involuntary coercive hierarchy that should be replaced by decentralised free association. Anarcha-feminists believe that the struggle against patriarchy is an essential part of class struggle, and the anarchist struggle against the state. In essence, the philosophy sees anarchist struggle as a necessary component of feminist struggle and vice versa. L. Susan Brown claims that "as anarchism is a political philosophy that opposes all relationships of power, it is inherently feminist". Anarcha-feminism began with the late 19th century writings of early feminist anarchists such as Emma Goldman and Voltairine de Cleyre.

Anarcho-pacifism is a tendency that rejects violence in the struggle for social change (see non-violence). It developed "mostly in the Netherlands, Britain, and the United States, before and during the Second World War". Christian anarchism is a movement in political theology that combines anarchism and Christianity. Its main proponents included Leo Tolstoy, Dorothy Day, Ammon Hennacy, and Jacques Ellul.

Platformism is a tendency within the wider anarchist movement based on the organisational theories in the tradition of Dielo Truda's Organisational Platform of the General Union of Anarchists (Draft).  The document was based on the experiences of Russian anarchists in the 1917 October Revolution, which led eventually to the victory of the Bolsheviks over the anarchists and other groups. The Platform attempted to address and explain the anarchist movement's failures during the Russian Revolution.

Synthesis anarchism is a form of anarchism that tries to join anarchists of different tendencies under the principles of anarchism without adjectives. In the 1920s, this form found as its main proponents the anarcho-communists Voline and SÃ©bastien Faure.  It is the main principle behind the anarchist federations grouped around the contemporary global International of Anarchist Federations.

Post-left anarchy is a recent current in anarchist thought that promotes a critique of anarchism's relationship to traditional Left-wing politics. Some post-leftists seek to escape the confines of ideology in general also presenting a critique of organisations and morality. Influenced by the work of Max Stirner and by the Marxist Situationist International, post-left anarchy is marked by a focus on social insurrection and a rejection of leftist social organisation.

Insurrectionary anarchism is a revolutionary theory, practice, and tendency within the anarchist movement which emphasizes insurrection within anarchist practice. It is critical of formal organisations such as labour unions and federations that are based on a political programme and periodic congresses. Instead, insurrectionary anarchists advocate informal organisation and small affinity group based organisation. Insurrectionary anarchists put value in attack, permanent class conflict, and a refusal to negotiate or compromise with class enemies.

Post-anarchism is a theoretical move towards a synthesis of classical anarchist theory and poststructuralist thought, drawing from diverse ideas including post-modernism, autonomist marxism, post-left anarchy, Situationist International, and postcolonialism.

Left-wing market anarchism strongly affirm the classical liberal ideas of self-ownership and free markets, while maintaining that, taken to their logical conclusions, these ideas support strongly anti-corporatist, anti-hierarchical, pro-labor positions and anti-capitalism in economics and anti-imperialism in foreign policy.

Anarcho-capitalism advocates the elimination of the state in favour of individual sovereignty in a free market. Anarcho-capitalism developed from radical anti-state libertarianism and individualist anarchism, drawing from Austrian School economics, study of law and economics, and public choice theory. There is a strong current within anarchism which does not consider that anarcho-capitalism can be considered a part of the anarchist movement due to the fact that anarchism has historically been an anti-capitalist movement and for definitional reasons which see anarchism incompatible with capitalist forms.
Anarchism is a philosophy that embodies many diverse attitudes, tendencies and schools of thought; as such, disagreement over questions of values, ideology and tactics is common. The compatibility of capitalism, nationalism, and religion with anarchism is widely disputed. Similarly, anarchism enjoys complex relationships with ideologies such as Marxism, communism and capitalism. Anarchists may be motivated by humanism, divine authority, enlightened self-interest, veganism or any number of alternative ethical doctrines.

Phenomena such as civilization, technology (e.g. within anarcho-primitivism and insurrectionary anarchism), and the democratic process may be sharply criticised within some anarchist tendencies and simultaneously lauded in others.

On a tactical level, while propaganda of the deed was a tactic used by anarchists in the 19th century (e.g. the Nihilist movement), some contemporary anarchists espouse alternative direct action methods such as nonviolence, counter-economics and anti-state cryptography to bring about an anarchist society. About the scope of an anarchist society, some anarchists advocate a global one, while others do so by local ones. The diversity in anarchism has led to widely different use of identical terms among different anarchist traditions, which has led to many definitional concerns in anarchist theory.

Intersecting and overlapping between various schools of thought, certain topics of interest and internal disputes have proven perennial within anarchist theory.

An important current within anarchism is free love. Free love advocates sometimes traced their roots back to Josiah Warren and to experimental communities, viewed sexual freedom as a clear, direct expression of an individual's sovereignty. Free love particularly stressed women's rights since most sexual laws discriminated against women: for example, marriage laws and anti-birth control measures. The most important American free love journal was Lucifer the Lightbearer (1883â€“1907) edited by Moses Harman and Lois Waisbrooker, but also there existed Ezra Heywood and Angela Heywood's The Word (1872â€“1890, 1892â€“1893). Free Society (1895â€“1897 as The Firebrand; 1897â€“1904 as Free Society) was a major anarchist newspaper in the United States at the end of the 19th and beginning of the 20th centuries. The publication advocated free love and women's rights, and critiqued "Comstockery" â€“ censorship of sexual information. Also M. E. Lazarus was an important American individualist anarchist who promoted free love.

In New York City's Greenwich Village, bohemian feminists and socialists advocated self-realisation and pleasure for women (and also men) in the here and now. They encouraged playing with sexual roles and sexuality, and the openly bisexual radical Edna St. Vincent Millay and the lesbian anarchist Margaret Anderson were prominent among them. Discussion groups organised by the Villagers were frequented by Emma Goldman, among others. Magnus Hirschfeld noted in 1923 that Goldman "has campaigned boldly and steadfastly for individual rights, and especially for those deprived of their rights. Thus it came about that she was the first and only woman, indeed the first and only American, to take up the defense of homosexual love before the general public." In fact, before Goldman, heterosexual anarchist Robert Reitzel (1849â€“1898) spoke positively of homosexuality from the beginning of the 1890s in his Detroit-based German language journal Der arme Teufel. In Argentina anarcha-feminist Virginia Bolten published the newspaper called  (English: The Woman's Voice), which was published nine times in Rosario between 8 January 1896 and 1 January 1897, and was revived, briefly, in 1901.

In Europe the main propagandist of free love within individualist anarchism was Emile Armand. He proposed the concept of la camaraderie amoureuse to speak of free love as the possibility of voluntary sexual encounter between consenting adults. He was also a consistent proponent of polyamory. In Germany the stirnerists Adolf Brand and John Henry Mackay were pioneering campaigners for the acceptance of male bisexuality and homosexuality. Mujeres Libres was an anarchist women's organisation in Spain that aimed to empower working class women. It was founded in 1936 by LucÃ­a SÃ¡nchez Saornil, Mercedes Comaposada and Amparo Poch y GascÃ³n and had approximately 30,000 members. The organisation was based on the idea of a "double struggle" for women's liberation and social revolution and argued that the two objectives were equally important and should be pursued in parallel. In order to gain mutual support, they created networks of women anarchists. LucÃ­a SÃ¡nchez Saornil was a main founder of the Spanish anarcha-feminist federation Mujeres Libres who was open about her lesbianism. She was published in a variety of literary journals where working under a male pen name, she was able to explore lesbian themes at a time when homosexuality was criminalized and subject to censorship and punishment.

More recently, the British anarcho-pacifist Alex Comfort gained notoriety during the sexual revolution for writing the bestseller sex manual The Joy of Sex. The issue of free love has a dedicated treatment in the work of French anarcho-hedonist philosopher Michel Onfray in such works as ThÃ©orie du corps amoureux : pour une Ã©rotique solaire (2000) and L'invention du plaisir : fragments cyrÃ©aniques (2002).


 For English anarchist William Godwin education was "the main means by which change would be achieved." Godwin saw that the main goal of education should be the promotion of happiness. For Godwin education had to have "A respect for the child's autonomy which precluded any form of coercion," "A pedagogy that respected this and sought to build on the child's own motivation and initiatives," and "A concern about the child's capacity to resist an ideology transmitted through the school." In his Political Justice he criticises state sponsored schooling "on account of its obvious alliance with national government". Early American anarchist Josiah Warren advanced alternative education experiences in the libertarian communities he established. Max Stirner wrote in 1842 a long essay on education called The False Principle of our Education. In it Stirner names his educational principle "personalist," explaining that self-understanding consists in hourly self-creation. Education for him is to create "free men, sovereign characters," by which he means "eternal characters... who are therefore eternal because they form themselves each moment".

In the United States "freethought was a basically anti-christian, anti-clerical movement, whose purpose was to make the individual politically and spiritually free to decide for himself on religious matters. A number of contributors to Liberty (anarchist publication) were prominent figures in both freethought and anarchism. The individualist anarchist George MacDonald was a co-editor of Freethought and, for a time, The Truth Seeker. E.C. Walker was co-editor of the excellent free-thought / free love journal Lucifer, the Light-Bearer". "Many of the anarchists were ardent freethinkers; reprints from freethought papers such as Lucifer, the Light-Bearer, Freethought and The Truth Seeker appeared in Liberty...The church was viewed as a common ally of the state and as a repressive force in and of itself".

In 1901, Catalan anarchist and free-thinker Francesc Ferrer i GuÃ rdia established "modern" or progressive schools in Barcelona in defiance of an educational system controlled by the Catholic Church. The schools' stated goal was to "educate the working class in a rational, secular and non-coercive setting". Fiercely anti-clerical, Ferrer believed in "freedom in education", education free from the authority of church and state. Murray Bookchin wrote: "This period  was the heyday of libertarian schools and pedagogical projects in all areas of the country where Anarchists exercised some degree of influence. Perhaps the best-known effort in this field was Francisco Ferrer's Modern School (Escuela Moderna), a project which exercised a considerable influence on Catalan education and on experimental techniques of teaching generally." La Escuela Moderna, and Ferrer's ideas generally, formed the inspiration for a series of Modern Schools in the United States, Cuba, South America and London. The first of these was started in New York City in 1911. It also inspired the Italian newspaper UniversitÃ  popolare, founded in 1901. Russian christian anarchist Leo Tolstoy established a school for peasant children on his estate. Tolstoy's educational experiments were short-lived due to harassment by the Tsarist secret police. Tolstoy established a conceptual difference between education and culture. He thought that "Education is the tendency of one man to make another just like himself... Education is culture under restraint, culture is free.  when the teaching is forced upon the pupil, and when then instruction is exclusive, that is when only those subjects are taught which the educator regards as necessary". For him "without compulsion, education was transformed into culture".

A more recent libertarian tradition on education is that of unschooling and the free school in which child-led activity replaces pedagogic approaches. Experiments in Germany led to A. S. Neill founding what became Summerhill School in 1921. Summerhill is often cited as an example of anarchism in practice. However, although Summerhill and other free schools are radically libertarian, they differ in principle from those of Ferrer by not advocating an overtly political class struggle-approach.
In addition to organising schools according to libertarian principles, anarchists have also questioned the concept of schooling per se. The term deschooling was popularised by Ivan Illich, who argued that the school as an institution is dysfunctional for self-determined learning and serves the creation of a consumer society instead.


Criticisms of anarchism include moral criticisms and pragmatic criticisms. Anarchism is often evaluated as unfeasible or utopian by its critics. European history professor Carl Landauer, in his book European Socialism argued that social anarchism is unrealistic and that government is a "lesser evil" than a society without "repressive force." He also argued that "ill intentions will cease if repressive force disappears" is an "absurdity."

Anarchist economics
Anarchist symbolism
Libertarianism
Lists of anarchism topics
List of anarchist communities
List of anarchist movements by region
List of films dealing with Anarchism
Outline of anarchism
Barclay, Harold, People Without Government: An Anthropology of Anarchy (2nd ed.), Left Bank Books, 1990 ISBN 1-871082-16-1
Blumenfeld, Jacob; Bottici, Chiara; Critchley, Simon, eds., The Anarchist Turn, Pluto Press. 19 March 2013. ISBN 9780745333427
Carter, April, The Political Theory of Anarchism, Harper & Row. 1971. ISBN 978-0-06-136050-3
Gordon, Uri, Anarchy Alive!, London: Pluto Press, 2007.
Graham, Robert, ed., .
Volume One: From Anarchy to Anarchism (300CE to 1939), Black Rose Books, MontrÃ©al and London 2005. ISBN 1-55164-250-6.
Volume Two: The Anarchist Current (1939â€“2006), Black Rose Books, MontrÃ©al 2007. ISBN 978-1-55164-311-3.
Guerin, Daniel, , Monthly Review Press. 1970. ISBN 0-85345-175-3
Harper, Clifford, Anarchy: A Graphic Guide, (Camden Press, 1987): An overview, updating Woodcock's classic, and illustrated throughout by Harper's woodcut-style artwork.
McKay, Iain, ed., An Anarchist FAQ.
Volume I, AK Press, Oakland/Edinburgh 2008;  558 pages, ISBN 978-1-902593-90-6.
Volume II, AK Press, Oakland/Edinburgh 2012; 550 Pages, ISBN 978-1-84935-122-5
McLaughlin, Paul, Anarchism and Authority: A Philosophical Introduction to Classical Anarchism, AshGate. 2007. ISBN 0-7546-6196-2
Nettlau, Max, Anarchy through the times, Gordon Press. 1979. ISBN 0-8490-1397-6

Scott, James C., Two Cheers for Anarchism: Six Easy Pieces on Autonomy, Dignity, and Meaningful Work and Play, Princeton, NJ: Princeton University Press, 2012. ISBN 9780691155296.
Woodcock, George, Anarchism: A History of Libertarian Ideas and Movements (Penguin Books, 1962). ISBN 0-14-022697-4. .
Woodcock, George, ed., The Anarchist Reader (Fontana/Collins 1977; ISBN 0-00-634011-3): An anthology of writings from anarchist thinkers and activists including Proudhon, Kropotkin, Bakunin, Malatesta, Bookchin, Goldman, and many others.
Woodcock, George, ed., Demanding the Impossible: A History of Anarchism, PM Press. 2010. ISBN 1-60486-064-2



Autism is a neurodevelopmental disorder characterized by impaired social interaction, verbal and non-verbal communication, and restricted and repetitive behavior. Parents usually notice signs in the first two years of their child's life. These signs often develop gradually, though some children with autism reach their developmental milestones at a normal pace and then regress. The diagnostic criteria require that symptoms become apparent in early childhood, typically before age three.
While autism is highly heritable, researchers suspect both environmental and genetic factors as causes. In rare cases, autism is strongly associated with agents that cause birth defects. Controversies surround other proposed environmental causes; for example, the vaccine hypotheses have been disproven. Autism affects information processing in the brain by altering how nerve cells and their synapses connect and organize; how this occurs is not well understood. It is one of three recognized disorders in the autism spectrum (ASDs), the other two being Asperger syndrome, which lacks delays in cognitive development and language, and pervasive developmental disorder, not otherwise specified (commonly abbreviated as PDD-NOS), which is diagnosed when the full set of criteria for autism or Asperger syndrome are not met.
Early speech or behavioral interventions can help children with autism gain self-care, social, and communication skills. Although there is no known cure, there have been reported cases of children who recovered. Not many children with autism live independently after reaching adulthood, though some become successful. An autistic culture has developed, with some individuals seeking a cure and others believing autism should be accepted as a difference and not treated as a disorder.
As of 2010 the number of people affected is estimated at about 1â€“2 per 1,000 worldwide. It occurs four to five times more often in boys than girls. About 1.5% of children in the United States (one in 68) are diagnosed with ASD , a 30% increase from one in 88 in 2012. The rate of autism among adults aged 18 years and over in the United Kingdom is 1.1%. The number of people diagnosed has been increasing dramatically since the 1980s, partly due to changes in diagnostic practice and government-subsidized financial incentives for named diagnoses; the question of whether actual rates have increased is unresolved.

Autism is a highly variable neurodevelopmental disorder that first appears during infancy or childhood, and generally follows a steady course without remission. Overt symptoms gradually begin after the age of six months, become established by age two or three years, and tend to continue through adulthood, although often in more muted form. It is distinguished not by a single symptom, but by a characteristic triad of symptoms: impairments in social interaction; impairments in communication; and restricted interests and repetitive behavior. Other aspects, such as atypical eating, are also common but are not essential for diagnosis. Autism's individual symptoms occur in the general population and appear not to associate highly, without a sharp line separating pathologically severe from common traits.

Social deficits distinguish autism and the related autism spectrum disorders (ASD; see Classification) from other developmental disorders. People with autism have social impairments and often lack the intuition about others that many people take for granted. Noted autistic Temple Grandin described her inability to understand the social communication of neurotypicals, or people with normal neural development, as leaving her feeling "like an anthropologist on Mars".

Unusual social development becomes apparent early in childhood. Autistic infants show less attention to social stimuli, smile and look at others less often, and respond less to their own name. Autistic toddlers differ more strikingly from social norms; for example, they have less eye contact and turn-taking, and do not have the ability to use simple movements to express themselves, such as pointing at things. Three- to five-year-old children with autism are less likely to exhibit social understanding, approach others spontaneously, imitate and respond to emotions, communicate nonverbally, and take turns with others. However, they do form attachments to their primary caregivers. Most childen with autism display moderately less attachment security than neurotypical children, although this difference disappears in children with higher mental development or less severe ASD. Older children and adults with ASD perform worse on tests of face and emotion recognition.

Children with high-functioning autism suffer from more intense and frequent loneliness compared to non-autistic peers, despite the common belief that children with autism prefer to be alone. Making and maintaining friendships often proves to be difficult for those with autism. For them, the quality of friendships, not the number of friends, predicts how lonely they feel. Functional friendships, such as those resulting in invitations to parties, may affect the quality of life more deeply.

There are many anecdotal reports, but few systematic studies, of aggression and violence in individuals with ASD. The limited data suggest that, in children with intellectual disability, autism is associated with aggression, destruction of property, and tantrums.

About a third to a half of individuals with autism do not develop enough natural speech to meet their daily communication needs. Differences in communication may be present from the first year of life, and may include delayed onset of babbling, unusual gestures, diminished responsiveness, and vocal patterns that are not synchronized with the caregiver. In the second and third years, children with autism have less frequent and less diverse babbling, consonants, words, and word combinations; their gestures are less often integrated with words. Children with autism are less likely to make requests or share experiences, and are more likely to simply repeat others' words (echolalia) or reverse pronouns. Joint attention seems to be necessary for functional speech, and deficits in joint attention seem to distinguish infants with ASD: for example, they may look at a pointing hand instead of the pointed-at object, and they consistently fail to point at objects in order to comment on or share an experience. Children with autism may have difficulty with imaginative play and with developing symbols into language.

In a pair of studies, high-functioning children with autism aged 8â€“15 performed equally well as, and adults better than, individually matched controls at basic language tasks involving vocabulary and spelling. Both autistic groups performed worse than controls at complex language tasks such as figurative language, comprehension and inference. As people are often sized up initially from their basic language skills, these studies suggest that people speaking to autistic individuals are more likely to overestimate what their audience comprehends.

Autistic individuals display many forms of repetitive or restricted behavior, which the Repetitive Behavior Scale-Revised (RBS-R) categorizes as follows.
Stereotypy is repetitive movement, such as hand flapping, head rolling, or body rocking.
Compulsive behavior is intended and appears to follow rules, such as arranging objects in stacks or lines.
Sameness is resistance to change; for example, insisting that the furniture not be moved or refusing to be interrupted.
Ritualistic behavior involves an unvarying pattern of daily activities, such as an unchanging menu or a dressing ritual. This is closely associated with sameness and an independent validation has suggested combining the two factors.
Restricted behavior is limited in focus, interest, or activity, such as preoccupation with a single television program, toy or game.
Self-injury includes movements that injure or can injure the person, such as eye-poking, skin-picking, hand-biting and head-banging. 
No single repetitive or self-injurious behavior seems to be specific to autism, but only autism appears to have an elevated pattern of occurrence and severity of these behaviors.

Autistic individuals may have symptoms that are independent of the diagnosis, but that can affect the individual or the family.
An estimated 0.5% to 10% of individuals with ASD show unusual abilities, ranging from splinter skills such as the memorization of trivia to the extraordinarily rare talents of prodigious autistic savants. Many individuals with ASD show superior skills in perception and attention, relative to the general population. Sensory abnormalities are found in over 90% of those with autism, and are considered core features by some, although there is no good evidence that sensory symptoms differentiate autism from other developmental disorders. Differences are greater for under-responsivity (for example, walking into things) than for over-responsivity (for example, distress from loud noises) or for sensation seeking (for example, rhythmic movements). An estimated 60%â€“80% of autistic people have motor signs that include poor muscle tone, poor motor planning, and toe walking; deficits in motor coordination are pervasive across ASD and are greater in autism proper.

Unusual eating behavior occurs in about three-quarters of children with ASD, to the extent that it was formerly a diagnostic indicator. Selectivity is the most common problem, although eating rituals and food refusal also occur; this does not appear to result in malnutrition. Although some children with autism also have gastrointestinal symptoms, there is a lack of published rigorous data to support the theory that children with autism have more or different gastrointestinal symptoms than usual; studies report conflicting results, and the relationship between gastrointestinal problems and ASD is unclear.

Parents of children with ASD have higher levels of stress. Siblings of children with ASD report greater admiration of and less conflict with the affected sibling than siblings of unaffected children and were similar to siblings of children with Down syndrome in these aspects of the sibling relationship. However, they reported lower levels of closeness and intimacy than siblings of children with Down syndrome; siblings of individuals with ASD have greater risk of negative well-being and poorer sibling relationships as adults.

It has long been presumed that there is a common cause at the genetic, cognitive, and neural levels for autism's characteristic triad of symptoms. However, there is increasing suspicion that autism is instead a complex disorder whose core aspects have distinct causes that often co-occur.
Autism has a strong genetic basis, although the genetics of autism are complex and it is unclear whether ASD is explained more by rare mutations with major effects, or by rare multigene interactions of common genetic variants. Complexity arises due to interactions among multiple genes, the environment, and epigenetic factors which do not change DNA but are heritable and influence gene expression. Studies of twins suggest that heritability is 0.7 for autism and as high as 0.9 for ASD, and siblings of those with autism are about 25 times more likely to be autistic than the general population. However, most of the mutations that increase autism risk have not been identified. Typically, autism cannot be traced to a Mendelian (single-gene) mutation or to a single chromosome abnormality, and none of the genetic syndromes associated with ASDs have been shown to selectively cause ASD. Numerous candidate genes have been located, with only small effects attributable to any particular gene. The large number of autistic individuals with unaffected family members may result from copy number variationsâ€”spontaneous deletions or duplications in genetic material during meiosis. Hence, a substantial fraction of autism cases may be traceable to genetic causes that are highly heritable but not inherited: that is, the mutation that causes the autism is not present in the parental genome.

Several lines of evidence point to synaptic dysfunction as a cause of autism. Some rare mutations may lead to autism by disrupting some synaptic pathways, such as those involved with cell adhesion. Gene replacement studies in mice suggest that autistic symptoms are closely related to later developmental steps that depend on activity in synapses and on activity-dependent changes. All known teratogens (agents that cause birth defects) related to the risk of autism appear to act during the first eight weeks from conception, and though this does not exclude the possibility that autism can be initiated or affected later, there is strong evidence that autism arises very early in development.

Exposure to air pollution during pregnancy, especially heavy metals and particulates, may increase the risk of autism. Environmental factors that have been claimed to contribute to or exacerbate autism, or may be important in future research, include certain foods, infectious diseases, solvents, diesel exhaust, PCBs, phthalates and phenols used in plastic products, pesticides, brominated flame retardants, alcohol, smoking, illicit drugs, vaccines, and prenatal stress, although no links have been found, and some have been completely disproven.

Parents may first become aware of autistic symptoms in their child around the time of a routine vaccination. This has led to unsupported theories blaming vaccine "overload", a vaccine preservative, or the MMR vaccine for causing autism. The latter theory was supported by a litigation-funded study that has since been shown to have been "an elaborate fraud". Although these theories lack convincing scientific evidence and are biologically implausible, parental concern about a potential vaccine link with autism has led to lower rates of childhood immunizations, outbreaks of previously controlled childhood diseases in some countries, and the preventable deaths of several children.

Autism's symptoms result from maturation-related changes in various systems of the brain. How autism occurs is not well understood. Its mechanism can be divided into two areas: the pathophysiology of brain structures and processes associated with autism, and the neuropsychological linkages between brain structures and behaviors. The behaviors appear to have multiple pathophysiologies.


Unlike many other brain disorders, such as Parkinson's, autism does not have a clear unifying mechanism at either the molecular, cellular, or systems level; it is not known whether autism is a few disorders caused by mutations converging on a few common molecular pathways, or is (like intellectual disability) a large set of disorders with diverse mechanisms. Autism appears to result from developmental factors that affect many or all functional brain systems, and to disturb the timing of brain development more than the final product. Neuroanatomical studies and the associations with teratogens strongly suggest that autism's mechanism includes alteration of brain development soon after conception. This anomaly appears to start a cascade of pathological events in the brain that are significantly influenced by environmental factors. Just after birth, the brains of children with autism tend to grow faster than usual, followed by normal or relatively slower growth in childhood. It is not known whether early overgrowth occurs in all children with autism. It seems to be most prominent in brain areas underlying the development of higher cognitive specialization. Hypotheses for the cellular and molecular bases of pathological early overgrowth include the following:
An excess of neurons that causes local overconnectivity in key brain regions.
Disturbed neuronal migration during early gestation.
Unbalanced excitatoryâ€“inhibitory networks.
Abnormal formation of synapses and dendritic spines, for example, by modulation of the neurexinâ€“neuroligin cell-adhesion system, or by poorly regulated synthesis of synaptic proteins. Disrupted synaptic development may also contribute to epilepsy, which may explain why the two conditions are associated.

The immune system is thought to play an important role in autism. Children with autism have been found by researchers to have inflammation of both the peripheral and central immune systems as indicated by increased levels of pro-inflammatory cytokines and significant activation of microglia. Biomarkers of abnormal immune function have also been associated with increased impairments in behaviors that are characteristic of the core features of autism such as deficits in social interactions and communication. Interactions between the immune system and the nervous system begin early during the embryonic stage of life, and successful neurodevelopment depends on a balanced immune response. It is thought that activation of a pregnant mother's immune system such as from environmental toxicants or infection can contribute to causing autism through causing a disruption of brain development. This is supported by recent studies that have found that infection during pregnancy is associated with an increased risk of autism.

The relationship of neurochemicals to autism is not well understood; several have been investigated, with the most evidence for the role of serotonin and of genetic differences in its transport. The role of group I metabotropic glutamate receptors (mGluR) in the pathogenesis of fragile X syndrome, the most common identified genetic cause of autism, has led to interest in the possible implications for future autism research into this pathway. Some data suggests neuronal overgrowth potentially related to  an increase in several growth hormones or to impaired regulation of growth factor receptors. Also, some inborn errors of metabolism are associated with autism, but probably account for less than 5% of cases.

The mirror neuron system (MNS) theory of autism hypothesizes that distortion in the development of the MNS interferes with imitation and leads to autism's core features of social impairment and communication difficulties. The MNS operates when an animal performs an action or observes another animal perform the same action. The MNS may contribute to an individual's understanding of other people by enabling the modeling of their behavior via embodied simulation of their actions, intentions, and emotions. Several studies have tested this hypothesis by demonstrating structural abnormalities in MNS regions of individuals with ASD, delay in the activation in the core circuit for imitation in individuals with Asperger syndrome, and a correlation between reduced MNS activity and severity of the syndrome in children with ASD. However, individuals with autism also have abnormal brain activation in many circuits outside the MNS and the MNS theory does not explain the normal performance of children with autism on imitation tasks that involve a goal or object.
ASD-related patterns of low function and aberrant activation in the brain differ depending on whether the brain is doing social or nonsocial tasks.
In autism there is evidence for reduced functional connectivity of the default network, a large-scale brain network involved in social and emotional processing, with intact connectivity of the task-positive network, used in sustained attention and goal-directed thinking. In people with autism the two networks are not negatively correlated in time, suggesting an imbalance in toggling between the two networks, possibly reflecting a disturbance of self-referential thought.

The underconnectivity theory of autism hypothesizes that autism is marked by underfunctioning high-level neural connections and synchronization, along with an excess of low-level processes. Evidence for this theory has been found in functional neuroimaging studies on autistic individuals and by a brainwave study that suggested that adults with ASD have local overconnectivity in the cortex and weak functional connections between the frontal lobe and the rest of the cortex. Other evidence suggests the underconnectivity is mainly within each hemisphere of the cortex and that autism is a disorder of the association cortex.

From studies based on event-related potentials, transient changes to the brain's electrical activity in response to stimuli, there is considerable evidence for differences in autistic individuals with respect to attention, orientation to auditory and visual stimuli, novelty detection, language and face processing, and information storage; several studies have found a preference for nonsocial stimuli. For example, magnetoencephalography studies have found evidence in children with autism of delayed responses in the brain's processing of auditory signals.

In the genetic area, relations have been found between autism and schizophrenia based on duplications and deletions of chromosomes; research showed that schizophrenia and autism are significantly more common in combination with 1q21.1 deletion syndrome. Research on autism/schizophrenia relations for chromosome 15 (15q13.3), chromosome 16 (16p13.1) and chromosome 17 (17p12) are inconclusive.

Two major categories of cognitive theories have been proposed about the links between autistic brains and behavior.

The first category focuses on deficits in social cognition. Simon Baron-Cohen's empathizingâ€“systemizing theory postulates that autistic individuals can systemizeâ€”that is, they can develop internal rules of operation to handle events inside the brainâ€”but are less effective at empathizing by handling events generated by other agents. An extension, the extreme male brain theory, hypothesizes that autism is an extreme case of the male brain, defined psychometrically as individuals in whom systemizing is better than empathizing.  These theories are somewhat related to Baron-Cohen's earlier theory of mind approach, which hypothesizes that autistic behavior arises from an inability to ascribe mental states to oneself and others. The theory of mind hypothesis is supported by the atypical responses of children with autism to the Sallyâ€“Anne test for reasoning about others' motivations, and the mirror neuron system theory of autism described in Pathophysiology maps well to the hypothesis. However, most studies have found no evidence of impairment in autistic individuals' ability to understand other people's basic intentions or goals; instead, data suggests that impairments are found in understanding more complex social emotions or in considering others' viewpoints.

The second category focuses on nonsocial or general processing: the executive functions such as working memory, planning, inhibition. In his review, Kenworthy states that "the claim of executive dysfunction as a causal factor in autism is controversial", however, "it is clear that executive dysfunction plays a role in the social and cognitive deficits observed in individuals with autism". Tests of core executive processes such as eye movement tasks indicate improvement from late childhood to adolescence, but performance never reaches typical adult levels. A strength of the theory is predicting stereotyped behavior and narrow interests; two weaknesses are that executive function is hard to measure and that executive function deficits have not been found in young children with autism.

Weak central coherence theory hypothesizes that a limited ability to see the big picture underlies the central disturbance in autism. One strength of this theory is predicting special talents and peaks in performance in autistic people. A related theoryâ€”enhanced perceptual functioningâ€”focuses more on the superiority of locally oriented and perceptual operations in autistic individuals. These theories map well from the underconnectivity theory of autism.

Neither category is satisfactory on its own; social cognition theories poorly address autism's rigid and repetitive behaviors, while the nonsocial theories have difficulty explaining social impairment and communication difficulties. A combined theory based on multiple deficits may prove to be more useful.

Diagnosis is based on behavior, not cause or mechanism. Under the DSM-5, autism is characterized by persistent deficits in social communication and interaction across multiple contexts, as well as restricted, repetitive patterns of behavior, interests, or activities. These deficits are present in early childhood, typically before age three, and lead to clinically significant functional impairment. Sample symptoms include lack of social or emotional reciprocity, stereotyped and repetitive use of language or idiosyncratic language, and persistent preoccupation with unusual objects. The disturbance must not be better accounted for by Rett syndrome, intellectual disability or global developmental delay. ICD-10 uses essentially the same definition.

Several diagnostic instruments are available. Two are commonly used in autism research: the Autism Diagnostic Interview-Revised (ADI-R) is a semistructured parent interview, and the Autism Diagnostic Observation Schedule (ADOS) uses observation and interaction with the child. The Childhood Autism Rating Scale (CARS) is used widely in clinical environments to assess severity of autism based on observation of children.

A pediatrician commonly performs a preliminary investigation by taking developmental history and physically examining the child. If warranted, diagnosis and evaluations are conducted with help from ASD specialists, observing and assessing cognitive, communication, family, and other factors using standardized tools, and taking into account any associated medical conditions. A pediatric neuropsychologist is often asked to assess behavior and cognitive skills, both to aid diagnosis and to help recommend educational interventions. A differential diagnosis for ASD at this stage might also consider intellectual disability, hearing impairment, and a specific language impairment such as Landauâ€“Kleffner syndrome. The presence of autism can make it harder to diagnose coexisting psychiatric disorders such as depression.

Clinical genetics evaluations are often done once ASD is diagnosed, particularly when other symptoms already suggest a genetic cause. Although genetic technology allows clinical geneticists to link an estimated 40% of cases to genetic causes, consensus guidelines in the US and UK are limited to high-resolution chromosome and fragile X testing. A genotype-first model of diagnosis has been proposed, which would routinely assess the genome's copy number variations. As new genetic tests are developed several ethical, legal, and social issues will emerge. Commercial availability of tests may precede adequate understanding of how to use test results, given the complexity of autism's genetics. Metabolic and neuroimaging tests are sometimes helpful, but are not routine.

ASD can sometimes be diagnosed by age 14 months, although diagnosis becomes increasingly stable over the first three years of life: for example, a one-year-old who meets diagnostic criteria for ASD is less likely than a three-year-old to continue to do so a few years later. In the UK the National Autism Plan for Children recommends at most 30 weeks from first concern to completed diagnosis and assessment, though few cases are handled that quickly in practice. Although the symptoms of autism and ASD begin early in childhood, they are sometimes missed; years later, adults may seek diagnoses to help them or their friends and family understand themselves, to help their employers make adjustments, or in some locations to claim disability living allowances or other benefits.

Underdiagnosis and overdiagnosis are problems in marginal cases, and much of the recent increase in the number of reported ASD cases is likely due to changes in diagnostic practices. The increasing popularity of drug treatment options and the expansion of benefits has given providers incentives to diagnose ASD, resulting in some overdiagnosis of children with uncertain symptoms. Conversely, the cost of screening and diagnosis and the challenge of obtaining payment can inhibit or delay diagnosis. It is particularly hard to diagnose autism among the visually impaired, partly because some of its diagnostic criteria depend on vision, and partly because autistic symptoms overlap with those of common blindness syndromes or blindisms.

Autism is one of the five pervasive developmental disorders (PDD), which are characterized by widespread abnormalities of social interactions and communication, and severely restricted interests and highly repetitive behavior. These symptoms do not imply sickness, fragility, or emotional disturbance.

Of the five PDD forms, Asperger syndrome is closest to autism in signs and likely causes; Rett syndrome and childhood disintegrative disorder share several signs with autism, but may have unrelated causes; PDD not otherwise specified (PDD-NOS; also called atypical autism) is diagnosed when the criteria are not met for a more specific disorder. Unlike with autism, people with Asperger syndrome have no substantial delay in language development. The terminology of autism can be bewildering, with autism, Asperger syndrome and PDD-NOS often called the autism spectrum disorders (ASD) or sometimes the autistic disorders, whereas autism itself is often called autistic disorder, childhood autism, or infantile autism. In this article, autism refers to the classic autistic disorder; in clinical practice, though, autism, ASD, and PDD are often used interchangeably. ASD, in turn, is a subset of the broader autism phenotype, which describes individuals who may not have ASD but do have autistic-like traits, such as avoiding eye contact.

The manifestations of autism cover a wide spectrum, ranging from individuals with severe impairmentsâ€”who may be silent, developmentally disabled, and locked into hand flapping and rockingâ€”to high functioning individuals who may have active but distinctly odd social approaches, narrowly focused interests, and verbose, pedantic communication. Because the behavior spectrum is continuous, boundaries between diagnostic categories are necessarily somewhat arbitrary. Sometimes the syndrome is divided into low-, medium- or high-functioning autism (LFA, MFA, and HFA), based on IQ thresholds, or on how much support the individual requires in daily life; these subdivisions are not standardized and are controversial. Autism can also be divided into syndromal and non-syndromal autism; the syndromal autism is associated with severe or profound intellectual disability or a congenital syndrome with physical symptoms, such as tuberous sclerosis. Although individuals with Asperger syndrome tend to perform better cognitively than those with autism, the extent of the overlap between Asperger syndrome, HFA, and non-syndromal autism is unclear.

Some studies have reported diagnoses of autism in children due to a loss of language or social skills, as opposed to a failure to make progress, typically from 15 to 30 months of age. The validity of this distinction remains controversial; it is possible that regressive autism is a specific subtype, or that there is a continuum of behaviors between autism with and without regression.

Research into causes has been hampered by the inability to identify biologically meaningful subgroups within the autistic population and by the traditional boundaries between the disciplines of psychiatry, psychology, neurology and pediatrics. Newer technologies such as fMRI and diffusion tensor imaging can help identify biologically relevant phenotypes (observable traits) that can be viewed on brain scans, to help further neurogenetic studies of autism; one example is lowered activity in the fusiform face area of the brain, which is associated with impaired perception of people versus objects. It has been proposed to classify autism using genetics as well as behavior.

About half of parents of children with ASD notice their child's unusual behaviors by age 18 months, and about four-fifths notice by age 24 months. According to an article in the Journal of Autism and Developmental Disorders, failure to meet any of the following milestones "is an absolute indication to proceed with further evaluations. Delay in referral for such testing may delay early diagnosis and treatment and affect the long-term outcome".
No babbling by 12 months.
No gesturing (pointing, waving, etc.) by 12 months.
No single words by 16 months.
No two-word (spontaneous, not just echolalic) phrases by 24 months.
Any loss of any language or social skills, at any age.
US and Japanese practice is to screen all children for ASD at 18 and 24 months, using autism-specific formal screening tests. In contrast, in the UK, children whose families or doctors recognize possible signs of autism are screened. It is not known which approach is more effective. Screening tools include the Modified Checklist for Autism in Toddlers (M-CHAT), the Early Screening of Autistic Traits Questionnaire, and the First Year Inventory; initial data on M-CHAT and its predecessor CHAT on children aged 18â€“30 months suggests that it is best used in a clinical setting and that it has low sensitivity (many false-negatives) but good specificity (few false-positives). It may be more accurate to precede these tests with a broadband screener that does not distinguish ASD from other developmental disorders. Screening tools designed for one culture's norms for behaviors like eye contact may be inappropriate for a different culture. Although genetic screening for autism is generally still impractical, it can be considered in some cases, such as children with neurological symptoms and dysmorphic features.

Infection with rubella during pregnancy causes less than 1% of cases of autism;  vaccination against rubella can prevent many of those cases.

The main goals when treating children with autism are to lessen associated deficits and family distress, and to increase quality of life and functional independence. No single treatment is best and treatment is typically tailored to the child's needs. Families and the educational system are the main resources for treatment. Studies of interventions have methodological problems that prevent definitive conclusions about efficacy. Although many psychosocial interventions have some positive evidence, suggesting that some form of treatment is preferable to no treatment, the methodological quality of systematic reviews of these studies has generally been poor, their clinical results are mostly tentative, and there is little evidence for the relative effectiveness of treatment options. Intensive, sustained special education programs and behavior therapy early in life can help children acquire self-care, social, and job skills, and often improve functioning and decrease symptom severity and maladaptive behaviors; claims that intervention by around age three years is crucial are not substantiated. Available approaches include applied behavior analysis (ABA), developmental models, structured teaching, speech and language therapy, social skills therapy, and occupational therapy. There is some evidence that early intensive behavioral intervention (EIBI), an early intervention model based on ABA for 20 to 40hours a week for multiple years, is an effective treatment for some children with ASD.

Educational interventions can be effective to varying degrees in most children: intensive ABA treatment has demonstrated effectiveness in enhancing global functioning in preschool children and is well-established for improving intellectual performance of young children. Neuropsychological reports are often poorly communicated to educators, resulting in a gap between what a report recommends and what education is provided. It is not known whether treatment programs for children lead to significant improvements after the children grow up, and the limited research on the effectiveness of adult residential programs shows mixed results. The appropriateness of including children with varying severity of autism spectrum disorders in the general education population is a subject of current debate among educators and researchers.

Many medications are used to treat ASD symptoms that interfere with integrating a child into home or school when behavioral treatment fails. More than half of US children diagnosed with ASD are prescribed psychoactive drugs or anticonvulsants, with the most common drug classes being antidepressants, stimulants, and antipsychotics. Antipsychotics, such as risperidone and aripiprazole, have been found to be useful for treating irritability, repetitive behavior, and sleeplessness that often occurs with autism. There is scant reliable research about the effectiveness or safety of drug treatments for adolescents and adults with ASD. A person with ASD may respond atypically to medications, the medications can have adverse effects, and no known medication relieves autism's core symptoms of social and communication impairments. Experiments in mice have reversed or reduced some symptoms related to autism by replacing or modulating gene function, suggesting the possibility of targeting therapies to specific rare mutations known to cause autism.

Although many alternative therapies and interventions are available, few are supported by scientific studies. Treatment approaches have little empirical support in quality-of-life contexts, and many programs focus on success measures that lack predictive validity and real-world relevance. Scientific evidence appears to matter less to service providers than program marketing, training availability, and parent requests. Some alternative treatments may place the child at risk. A 2008 study found that compared to their peers, autistic boys have significantly thinner bones if on casein-free diets; in 2005, botched chelation therapy killed a five-year-old child with autism. There has been early research looking at hyperbaric treatments in children with autism.

Treatment is expensive; indirect costs are more so. For someone born in 2000, a US study estimated an average lifetime cost of $ (net present value in  dollars, inflation-adjusted from 2003 estimate), with about 10% medical care, 30% extra education and other care, and 60% lost economic productivity. Publicly supported programs are often inadequate or inappropriate for a given child, and unreimbursed out-of-pocket medical or therapy expenses are associated with likelihood of family financial problems; one 2008 US study found a 14% average loss of annual income in families of children with ASD, and a related study found that ASD is associated with higher probability that child care problems will greatly affect parental employment. US states increasingly require private health insurance to cover autism services, shifting costs from publicly funded education programs to privately funded health insurance. After childhood, key treatment issues include residential care, job training and placement, sexuality, social skills, and estate planning.

There is no known cure. Children recover occasionally, so that they lose their diagnosis of ASD; this occurs sometimes after intensive treatment and sometimes not. It is not known how often recovery happens; reported rates in unselected samples of children with ASD have ranged from 3% to 25%. Most children with autism acquire language by age five or younger, though a few have developed communication skills in later years. Most children with autism lack social support, meaningful relationships, future employment opportunities or self-determination. Although core difficulties tend to persist, symptoms often become less severe with age.

Few high-quality studies address long-term prognosis. Some adults show modest improvement in communication skills, but a few decline; no study has focused on autism after midlife. Acquiring language before age six, having an IQ above 50, and having a marketable skill all predict better outcomes; independent living is unlikely with severe autism. Most, but not all, people with autism face significant obstacles in transitioning to adulthood.

Most recent reviews tend to estimate a prevalence of 1â€“2 per 1,000 for autism and close to 6 per 1,000 for ASD, and 11 per 1,000 children in the United States for ASD as of 2008; because of inadequate data, these numbers may underestimate ASD's true rate. In 2012, the NHS estimated that the overall prevalence of autism among adults aged 18 years and over in the UK was 1.1%. Rates of PDD-NOS's has been estimated at 3.7 per 1,000, Asperger syndrome at roughly 0.6 per 1,000, and childhood disintegrative disorder at 0.02 per 1,000.

The number of reported cases of autism increased dramatically in the 1990s and early 2000s. This increase is largely attributable to changes in diagnostic practices, referral patterns, availability of services, age at diagnosis, and public awareness, though unidentified environmental risk factors cannot be ruled out. The available evidence does not rule out the possibility that autism's true prevalence has increased; a real increase would suggest directing more attention and funding toward changing environmental factors instead of continuing to focus on genetics.

Boys are at higher risk for ASD than girls. The sex ratio averages 4.3:1 and is greatly modified by cognitive impairment: it may be close to 2:1 with intellectual disability and more than 5.5:1 without. Several theories about the higher prevalence in males have been investigated, but the cause of the difference is unconfirmed; one theory is that females are underdiagnosed.

Although the evidence does not implicate any single pregnancy-related risk factor as a cause of autism, the risk of autism is associated with advanced age in either parent, and with diabetes, bleeding, and use of psychiatric drugs in the mother during pregnancy. The risk is greater with older fathers than with older mothers; two potential explanations are the known increase in mutation burden in older sperm, and the hypothesis that men marry later if they carry genetic liability and show some signs of autism. Most professionals believe that race, ethnicity, and socioeconomic background do not affect the occurrence of autism.

Several other conditions are common in children with autism. They include:
Genetic disorders. About 10â€“15% of autism cases have an identifiable Mendelian (single-gene) condition, chromosome abnormality, or other genetic syndrome, and ASD is associated with several genetic disorders.
Intellectual disability. The percentage of autistic individuals who also meet criteria for intellectual disability has been reported as anywhere from 25% to 70%, a wide variation illustrating the difficulty of assessing autistic intelligence. In comparison, for PDD-NOS the association with intellectual disability is much weaker,  and by definition, the diagnosis of Asperger's excludes intellectual disability.
Anxiety disorders are common among children with ASD; there are no firm data, but studies have reported prevalences ranging from 11% to 84%. Many anxiety disorders have symptoms that are better explained by ASD itself, or are hard to distinguish from ASD's symptoms.
Epilepsy, with variations in risk of epilepsy due to age, cognitive level, and type of language disorder.
Several metabolic defects, such as phenylketonuria, are associated with autistic symptoms.
Minor physical anomalies are significantly increased in the autistic population.
Preempted diagnoses. Although the DSM-IV rules out concurrent diagnosis of many other conditions along with autism, the full criteria for Attention deficit hyperactivity disorder (ADHD), Tourette syndrome, and other of these conditions are often present and these comorbid diagnoses are increasingly accepted.
Sleep problems affect about two-thirds of individuals with ASD at some point in childhood. These most commonly include symptoms of insomnia such as difficulty in falling asleep, frequent nocturnal awakenings, and early morning awakenings. Sleep problems are associated with difficult behaviors and family stress, and are often a focus of clinical attention over and above the primary ASD diagnosis.

A few examples of autistic symptoms and treatments were described long before autism was named. The Table Talk of Martin Luther, compiled by his notetaker, Mathesius, contains the story of a 12-year-old boy who may have been severely autistic. Luther reportedly thought the boy was a soulless mass of flesh possessed by the devil, and suggested that he be suffocated, although a later critic has cast doubt on the veracity of this report. The earliest well-documented case of autism is that of Hugh Blair of Borgue, as detailed in a 1747 court case in which his brother successfully petitioned to annul Blair's marriage to gain Blair's inheritance. The Wild Boy of Aveyron, a feral child caught in 1798, showed several signs of autism; the medical student Jean Itard treated him with a behavioral program designed to help him form social attachments and to induce speech via imitation.

The New Latin word autismus (English translation autism) was coined by the Swiss psychiatrist Eugen Bleuler in 1910 as he was defining symptoms of schizophrenia. He derived it from the Greek word autÃ³s (Î±á½Ï„ÏŒÏ‚, meaning "self"), and used it to mean morbid self-admiration, referring to "autistic withdrawal of the patient to his fantasies, against which any influence from outside becomes an intolerable disturbance".

The word autism first took its modern sense in 1938 when Hans Asperger of the Vienna University Hospital adopted Bleuler's terminology autistic psychopaths in a lecture in German about child psychology. Asperger was investigating an ASD now known as Asperger syndrome, though for various reasons it was not widely recognized as a separate diagnosis until 1981. Leo Kanner of the Johns Hopkins Hospital first used autism in its modern sense in English when he introduced the label early infantile autism in a 1943 report of 11 children with striking behavioral similarities. Almost all the characteristics described in Kanner's first paper on the subject, notably "autistic aloneness" and "insistence on sameness", are still regarded as typical of the autistic spectrum of disorders. It is not known whether Kanner derived the term independently of Asperger.

Kanner's reuse of autism led to decades of confused terminology like infantile schizophrenia, and child psychiatry's focus on maternal deprivation led to misconceptions of autism as an infant's response to "refrigerator mothers". Starting in the late 1960s autism was established as a separate syndrome by demonstrating that it is lifelong, distinguishing it from intellectual disability and schizophrenia and from other developmental disorders, and demonstrating the benefits of involving parents in active programs of therapy. As late as the mid-1970s there was little evidence of a genetic role in autism; now it is thought to be one of the most heritable of all psychiatric conditions. Although the rise of parent organizations and the destigmatization of childhood ASD have deeply affected how we view ASD, parents continue to feel social stigma in situations where their child's autistic behavior is perceived negatively by others, and many primary care physicians and medical specialists still express some beliefs consistent with outdated autism research.

The Internet has helped autistic individuals bypass nonverbal cues and emotional sharing that they find so hard to deal with, and has given them a way to form online communities and work remotely. Sociological and cultural aspects of autism have developed: some in the community seek a cure, while others believe that autism is simply another way of being.



Albedo (), or reflection coefficient, derived from Latin albedo "whiteness" (or reflected sunlight) in turn from albus "white", is the diffuse reflectivity or reflecting power of a surface.

It is the ratio of reflected radiation from the surface to incident radiation upon it. Its dimensionless nature lets it be expressed as a percentage and is measured on a scale from zero for no reflection of a perfectly black surface to 1 for perfect reflection of a white surface.

Albedo depends on the frequency of the radiation. When quoted unqualified, it usually refers to some appropriate average across the spectrum of visible light. In general, the albedo depends on the directional distribution of incident radiation, except for Lambertian surfaces, which scatter radiation in all directions according to a cosine function and therefore have an albedo that is independent of the incident distribution. In practice, a bidirectional reflectance distribution function (BRDF) may be required to accurately characterize the scattering properties of a surface, but albedo is very useful as a first approximation.

The albedo is an important concept in climatology, astronomy, and calculating reflectivity of surfaces in LEED sustainable-rating systems for buildings. The average overall albedo of Earth, its planetary albedo, is 30 to 35% because of cloud cover, but widely varies locally across the surface because of different geological and environmental features.

The term was introduced into optics by Johann Heinrich Lambert in his 1760 work Photometria.


Albedos of typical materials in visible light range from up to 0.9 for fresh snow to about 0.04 for charcoal, one of the darkest substances. Deeply shadowed cavities can achieve an effective albedo approaching the zero of a black body. When seen from a distance, the ocean surface has a low albedo, as do most forests, whereas desert areas have some of the highest albedos among landforms. Most land areas are in an albedo range of 0.1 to 0.4. The average albedo of Earth is about 0.3. This is far higher than for the ocean primarily because of the contribution of clouds.
Earth's surface albedo is regularly estimated via Earth observation satellite sensors such as NASA's MODIS instruments on board the Terra and Aqua satellites. As the total amount of reflected radiation cannot be directly measured by satellite, a mathematical model of the BRDF is used to translate a sample set of satellite reflectance measurements into estimates of directional-hemispherical reflectance and bi-hemispherical reflectance (e.g.).

Earth's average surface temperature due to its albedo and the greenhouse effect is currently about 15Â°C. If Earth were frozen entirely (and hence be more reflective) the average temperature of the planet would drop below âˆ’40Â°C. If only the continental land masses became covered by glaciers, the mean temperature of the planet would drop to about 0Â°C. In contrast, if the entire Earth is covered by waterâ€”a so-called aquaplanetâ€”the average temperature on the planet would rise to just under 27Â°C.

It has been shown that for many applications involving terrestrial albedo, the albedo at a particular solar zenith angle Î¸'i can reasonably be approximated by the proportionate sum of two terms: the directional-hemispherical reflectance at that solar zenith angle, , and the bi-hemispherical reflectance,  the proportion concerned being defined as the proportion of diffuse illumination .

Albedo  can then be given as:
Directional-hemispherical reflectance is sometimes referred to as black-sky albedo and bi-hemispherical reflectance as white-sky albedo. These terms are important because they allow the albedo to be calculated for any given illumination conditions from a knowledge of the intrinsic properties of the surface.

The albedos of planets, satellites and asteroids can be used to infer much about their properties. The study of albedos, their dependence on wavelength, lighting angle ("phase angle"), and variation in time comprises a major part of the astronomical field of photometry. For small and far objects that cannot be resolved by telescopes, much of what we know comes from the study of their albedos. For example, the absolute albedo can indicate the surface ice content of outer Solar System objects, the variation of albedo with phase angle gives information about regolith properties, whereas unusually high radar albedo is indicative of high metal content in asteroids.

Enceladus, a moon of Saturn, has one of the highest known albedos of any body in the Solar System, with 99% of EM radiation reflected. Another notable high-albedo body is Eris, with an albedo of 0.96. Many small objects in the outer Solar System and asteroid belt have low albedos down to about 0.05. A typical comet nucleus has an albedo of 0.04. Such a dark surface is thought to be indicative of a primitive and heavily space weathered surface containing some organic compounds.

The overall albedo of the Moon is around 0.12, but it is strongly directional and non-Lambertian, displaying also a strong opposition effect. Although such reflectance properties are different from those of any terrestrial terrains, they are typical of the regolith surfaces of airless Solar System bodies.

Two common albedos that are used in astronomy are the (V-band) geometric albedo (measuring brightness when illumination comes from directly behind the observer) and the Bond albedo (measuring total proportion of electromagnetic energy reflected). Their values can differ significantly, which is a common source of confusion.

In detailed studies, the directional reflectance properties of astronomical bodies are often expressed in terms of the five Hapke parameters which semi-empirically describe the variation of albedo with phase angle, including a characterization of the opposition effect of regolith surfaces.

The correlation between astronomical (geometric) albedo, absolute magnitude and diameter is:
,

where  is the astronomical albedo,  is the diameter in kilometers, and  is the absolute magnitude.


Although the albedoâ€“temperature effect is best known in colder, whiter regions on Earth, the maximum albedo is actually found in the tropics where year-round illumination is greater. The maximum is additionally in the northern hemisphere, varying between three and twelve degrees north. The minima are found in the subtropical regions of the northern and southern hemispheres, beyond which albedo increases without respect to illumination.

The intensity of albedo temperature effects depend on the amount of albedo and the level of local insolation; high albedo areas in the arctic and antarctic regions are cold due to low insolation, where areas such as the Sahara Desert, which also have a relatively high albedo, will be hotter due to high insolation.  Tropical and sub-tropical rain forest areas have low albedo, and are much hotter than their temperate forest counterparts, which have lower insolation. Because insolation plays such a big role in the heating and cooling effects of albedo, high insolation areas like the tropics will tend to show a more pronounced fluctuation in local temperature when local albedo changes. 

Albedo affects climate and drives weather. All weather is a result of the uneven heating of Earth caused by different areas of the planet having different albedos. Essentially, for the driving of weather, there are two types of albedo regions on Earth: Land and ocean. Land and ocean regions produce the four basic different types of air masses, depending on latitude and therefore insolation: Warm and dry, which form over tropical and sub-tropical land masses; warm and wet, which form over tropical and sub-tropical oceans; cold and dry which form over temperate, polar and sub-polar land masses; and cold and wet, which form over temperate, polar and sub-polar oceans. Different temperatures between the air masses result in different air pressures, and the masses develop into pressure systems. High pressure systems flow toward lower pressure, driving weather from north to south in the northern hemisphere, and south to north in the lower; however due to the spinning of Earth, the Coriolis effect further complicates flow and creates several weather/climate bands and the jet streams.

When an area's albedo changes due to snowfall, a snowâ€“temperature feedback results. A layer of snowfall increases local albedo, reflecting away sunlight, leading to local cooling. In principle, if no outside temperature change affects this area (e.g. a warm air mass), the raised albedo and lower temperature would maintain the current snow and invite further snowfall, deepening the snowâ€“temperature feedback. However, because local weather is dynamic due to the change of seasons, eventually warm air masses and a more direct angle of sunlight (higher insolation) cause melting. When the melted area reveals surfaces with lower albedo, such as grass or soil, the effect is reversed: the darkening surface lowers albedo, increasing local temperatures, which induces more melting and thus reducing the albedo further, resulting in still more heating.

Snow albedo is highly variable, ranging from as high as 0.9 for freshly fallen snow, to about 0.4 for melting snow, and as low as 0.2 for dirty snow. Over Antarctica they average a little more than 0.8. If a marginally snow-covered area warms, snow tends to melt, lowering the albedo, and hence leading to more snowmelt because more radiation is being absorbed by the snowpack (the iceâ€“albedo positive feedback). Cryoconite, powdery windblown dust containing soot, sometimes reduces albedo on glaciers and ice sheets.
Hence, small errors in albedo can lead to large errors in energy estimates, which is why it is important to measure the albedo of snow-covered areas through remote sensing techniques rather than applying a single value over broad regions.

Albedo works on a smaller scale, too. In sunlight, dark clothes absorb more heat and light-coloured clothes reflect it better, thus allowing some control over body temperature by exploiting the albedo effect of the colour of external clothing.

Albedo can affect the electrical energy output of solar photovoltaic devices. For example, the effects of a spectrally responsive albedo are illustrated by the differences between the spectrally weighted albedo of solar photovoltaic technology based on hydrogenated amorphous silicon (a-Si:H) and crystalline silicon (c-Si)-based compared to traditional spectral-integrated albedo predictions. Research showed impacts of over 10%. More recently, the analysis was extended to the effects of spectral bias due to the specular reflectivity of 22 commonly occurring surface materials (both human-made and natural) and analyzes the albedo effects on the performance of seven photovoltaic materials covering three common photovoltaic system topologies: industrial (solar farms), commercial flat rooftops and residential pitched-roof applications.

Because forests generally have a low albedo, (the majority of the ultraviolet and visible spectrum is absorbed through photosynthesis), some scientists have suggested that greater heat absorption by trees could offset some of the carbon benefits of afforestation (or offset the negative climate impacts of deforestation). In the case of evergreen forests with seasonal snow cover albedo reduction may be great enough for deforestation to cause a net cooling effect. Trees also impact climate in extremely complicated ways through evapotranspiration. The water vapor causes cooling on the land surface, causes heating where it condenses, acts a strong greenhouse gas, and can increase albedo when it condenses into clouds Scientists generally treat evapotranspiration as a net cooling impact, and the net climate impact of albedo and evapotranspiration changes from deforestation depends greatly on local climate 

In seasonally snow-covered zones, winter albedos of treeless areas are 10% to 50% higher than nearby forested areas because snow does not cover the trees as readily. Deciduous trees have an albedo value of about 0.15 to 0.18 whereas coniferous trees have a value of about 0.09 to 0.15.

Studies by the Hadley Centre have investigated the relative (generally warming) effect of albedo change and (cooling) effect of carbon sequestration on planting forests. They found that new forests in tropical and midlatitude areas tended to cool; new forests in high latitudes (e.g. Siberia) were neutral or perhaps warming.

Water reflects light very differently from typical terrestrial materials. The reflectivity of a water surface is calculated using the Fresnel equations (see graph).

At the scale of the wavelength of light even wavy water is always smooth so the light is reflected in a locally specular manner (not diffusely). The glint of light off water is a commonplace effect of this. At small angles of incident light, waviness results in reduced reflectivity because of the steepness of the reflectivity-vs.-incident-angle curve and a locally increased average incident angle.

Although the reflectivity of water is very low at low and medium angles of incident light, it becomes very high at high angles of incident light such as those that occur on the illuminated side of Earth near the terminator (early morning, late afternoon, and near the poles). However, as mentioned above, waviness causes an appreciable reduction. Because light specularly reflected from water does not usually reach the viewer, water is usually considered to have a very low albedo in spite of its high reflectivity at high angles of incident light.

Note that white caps on waves look white (and have high albedo) because the water is foamed up, so there are many superimposed bubble surfaces which reflect, adding up their reflectivities. Fresh 'black' ice exhibits Fresnel reflection.

Cloud albedo has substantial influence over atmospheric temperatures. Different types of clouds exhibit different reflectivity, theoretically ranging in albedo from a minimum of near 0 to a maximum approaching 0.8. "On any given day, about half of Earth is covered by clouds, which reflect more sunlight than land and water. Clouds keep Earth cool by reflecting sunlight, but they can also serve as blankets to trap warmth."

Albedo and climate in some areas are affected by artificial clouds, such as those created by the contrails of heavy commercial airliner traffic. A study following the burning of the Kuwaiti oil fields during Iraqi occupation showed that temperatures under the burning oil fires were as much as 10Â°C colder than temperatures several miles away under clear skies.

Aerosols (very fine particles/droplets in the atmosphere) have both direct and indirect effects on Earth's radiative balance. The direct (albedo) effect is generally to cool the planet; the indirect effect (the particles act as cloud condensation nuclei and thereby change cloud properties) is less certain. As per  the effects are:
Aerosol direct effect. Aerosols directly scatter and absorb radiation. The scattering of radiation causes atmospheric cooling, whereas absorption can cause atmospheric warming.
Aerosol indirect effect. Aerosols modify the properties of clouds through a subset of the aerosol population called cloud condensation nuclei. Increased nuclei concentrations lead to increased cloud droplet number concentrations, which in turn leads to increased cloud albedo, increased light scattering and radiative cooling (first indirect effect), but also leads to reduced precipitation efficiency and increased lifetime of the cloud (second indirect effect).
Another albedo-related effect on the climate is from black carbon particles. The size of this effect is difficult to quantify: the Intergovernmental Panel on Climate Change estimates that the global mean radiative forcing for black carbon aerosols from fossil fuels is +0.2 W mâˆ’2, with a range +0.1 to +0.4 W mâˆ’2. Black carbon is a bigger cause of the melting of the polar ice cap in the Arctic than carbon dioxide due to its effect on the albedo. in altering global surface air temperature)"); compare Zender Testimony, supra note 7, at 4 (figure 3); See J. Hansen & L. Nazarenko, supra note 18, at 426. ("The efficacy for changes of Arctic sea ice albedo is >3. In additional runs not shown here, we found that the efficacy of albedo changes in Antarctica is also >3."); See also Flanner, M.G., C.S. Zender, J.T. Randerson, and P.J. Rasch, Present-day climate forcing and response from black carbon in snow, 112 J. GEOPHYS. RES. D11202 (2007) ("The forcing is maximum coincidentally with snowmelt onset, triggering strong snow-albedo feedback in local springtime. Consequently, the "efficacy" of black carbon/snow forcing is more than three times greater than forcing by CO2.").

Human activities (e.g. deforestation, farming, and urbanization) change the albedo of various areas around the globe. However, quantification of this effect on the global scale is difficult.

Single-scattering albedo is used to define scattering of electromagnetic waves on small particles. It depends on properties of the material (refractive index); the size of the particle or particles; and the wavelength of the incoming radiation.

Cool roof
Daisyworld
Emissivity
Global dimming
Irradiance
Polar see-saw
Solar radiation management


A (named a , plural aes) is the first letter and vowel in the ISO basic Latin alphabet. It is similar to the Ancient Greek letter alpha, from which it derives.  The upper-case version consists of the two slanting sides of a triangle, crossed in the middle by a horizontal bar. The lower-case version can be written in two forms: the double-storey a and single-storey É‘. The latter is commonly used in handwriting and fonts based on it, especially fonts intended to be read by children. It is also found in italic type.

The earliest certain ancestor of "A" is aleph (also called 'aleph), the first letter of the Phoenician alphabet (which, by consisting entirely of consonants, is an abjad rather than a true alphabet). In turn, the origin of aleph may have been a pictogram of an ox head in proto-Sinaitic script influenced by Egyptian hieroglyphs, styled as a triangular head with two horns extended.
In 1600 B.C.E., the Phoenician alphabet's letter had a linear form that served as the base for some later forms. Its name must have corresponded closely to the Hebrew or Arabic aleph.
When the ancient Greeks adopted the alphabet, they had no use for the glottal stopâ€”the first phoneme of the Phoenician pronunciation of the letter, and the sound that the letter denoted in Phoenician and other Semitic languagesâ€”so they used an adaptation of the sign to represent the vowel , and gave it the similar name of alpha. In the earliest Greek inscriptions after the Greek Dark Ages, dating to the 8th century BC, the letter rests upon its side, but in the Greek alphabet of later times it generally resembles the modern capital letter, although many local varieties can be distinguished by the shortening of one leg, or by the angle at which the cross line is set.

The Etruscans brought the Greek alphabet to their civilization in the Italian Peninsula and left the letter unchanged. The Romans later adopted the Etruscan alphabet to write the Latin language, and the resulting letter was preserved in the Latin alphabet used to write many languages, including English.


During Roman times, there were many variations on the letter "A". First was the monumental or lapidary style, which was used when inscribing on stone or other "permanent" mediums. For perishable surfaces, what was used for everyday or utilitarian purposes, a cursive style was used. Due to the "perishable" nature of the surfaces, these examples are not as prevalent as the monumental. This perishable style was called cursive and numerous variations have survived, such as majuscule cursive, minuscule cursive, and semicursive minuscule. There were also variants that were intermediate between the monumental and the cursive. The known variants include the early semi-uncial, the uncial, and the later semi-uncial.

At the termination of the Roman Empire (5th century AD), several variants of the cursive minuscule appeared through Western Europe. Among these were the semicursive minuscule of Italy, the Merovingian script in France, the Visigothic script in Spain, and the Insular or Anglo-Irish semi-uncial or Anglo-Saxon majuscule, of Great Britain. By the 9th century, the Caroline script, which was very similar to the present-day form, was the principal form used in book-making, before the advent of the printing press. This form was derived through a combining of prior forms.

15th-century Italy saw the formation of the two variants that are known today. These variants, the Italics and Roman forms, were derived from the Caroline Script version. The Italics form used in most current handwriting consists of a circle and vertical stroke (), called Latin alpha or "script a". This slowly developed from the fifth-century form resembling the Greek letter tau in the hands of medieval Irish and English writers. Most printed material uses the Roman form consisting of a small loop with an arc over it (). Both derive from the majuscule (capital) form. In Greek handwriting, it was common to join the left leg and horizontal stroke into a single loop, as demonstrated by the uncial version shown. Many fonts then made the right leg vertical. In some of these, the serif that began the right leg stroke developed into an arc, resulting in the printed form, while in others it was dropped, resulting in the modern handwritten form.

In English orthography, the letter A currently represents six different vowel sounds: A by itself frequently denotes the near-open front unrounded vowel () as in pad; the open back unrounded vowel () as in father, its original, Latin and Greek, sound; a closer, further fronted sound as in "hare", which developed as the sound progressed from "father" to "ace"; in concert with a later orthographic vowel, the diphthong  as in ace and major, due to effects of the Great Vowel Shift; the more rounded form in "water" or its closely related cousin, found in "was".

The double "a" sequence is not a native English combination; however it occurs in some foreign words such as Aaron and aardvark.

"A" is the third-most-commonly used letter in English (after "E" and "T"), and the second most common in Spanish and French. In one study, on average, about 3.68% of letters used in English tend to be 'a', while the number is 6.22% in Spanish and 3.95% in French.

In most languages that use the Latin alphabet, A denotes an open unrounded vowel: , , or . An exception is Saanich, in which A (and Ã) stands for a close-mid front unrounded vowel .


In algebra, the letter "A" along with other letters at the beginning of the alphabet is used to represent known quantities, whereas the letters at the end of the alphabet (x,y,z) are used to denote unknown quantities.

In geometry, capital A, B, C etc. are used to denote segments, lines, rays, etc. A capital A is also typically used as one of the letters to represent an angle in a triangle, the lowercase a representing the side opposite angle A.

In logic, A is used to signify the universal affirmative.

In physics, A is the SI unit symbol for Ampere.

In phonetic and phonemic notation:
in the ,  is used for the open front unrounded vowel,  is used for the open central unrounded vowel and  is used for the open back unrounded vowel.
in X-SAMPA,  is used for the open front unrounded vowel and  is used for the open back unrounded vowel.


"A" is often used to denote something or someone of a better or more prestigious quality or status: A-, A or A+, the best grade that can be assigned by teachers for students' schoolwork; "A grade" for clean restaurants; A-list celebrities, etc. Such associations can have a motivating effect, as exposure to the letter A has been found to improve performance, when compared with other letters.

A is a common symbol of school and basic phonetics in the US, along with B and C.

Finally, the letter A is used to denote size, as in a narrow size shoe, or a small cup size in a brassiere.


Î‘ Î± : Greek letter alpha
Ð Ð° : Cyrillic letter A
 : Latin letter alpha / script A
 : a turned lowercase letter A, used by the International Phonetic Alphabet for the near-open central vowel
 : a turned capital letter A, used in predicate logic to specify universal quantification ("for all")
Âª : an ordinal indicator
Ã† Ã¦ : Latin AE ligature
Ã… Ã¥ : A letter used in various Scandinavian (and other) languages


1 



Alabama () is a state located in the southeastern region of the United States. It is bordered by Tennessee to the north, Georgia to the east, Florida and the Gulf of Mexico to the south, and Mississippi to the west. Alabama is the 30th-most extensive and the 23rd-most populous of the 50 United States. At , Alabama has one of the longest navigable inland waterways in the nation.

From the American Civil War until World War II, Alabama, like many Southern states, suffered economic hardship, in part because of continued dependence on agriculture. Despite the growth of major industries and urban centers, White rural interests dominated the state legislature from 1901 to the 1960s, as it did not regularly reapportion the legislature from 1901 to 1961; urban interests and African Americans were markedly under-represented. African Americans and poor whites were essentially disenfranchised altogether by the state constitution of 1901, a status that continued into the mid-1960s before being alleviated by federal legislation. Exclusion of minorities continued under at-large voting systems in most counties; some changes were made through a series of omnibus court cases in the late 1980s to establish different electoral systems.

Following World War II, Alabama experienced growth as the economy of the state changed from one primarily based on agriculture to one with diversified interests. The power of the Solid South in Congress gained the establishment or expansion of multiple United States Armed Forces installations, which helped to bridge the gap between an agricultural and industrial economy during the mid-20th century. The state economy in the 21st century is based on management, automotive, finance, manufacturing, aerospace, mineral extraction, healthcare, education, retail, and technology.

Alabama is nicknamed the Yellowhammer State, after the state bird. Alabama is also known as the "Heart of Dixie" and the Cotton State. The state tree is the Longleaf Pine, and the state flower is the Camellia. The capital of Alabama is Montgomery. The largest city by population is Birmingham, which has long been the most industrialized city, and largest city by total land area is Huntsville. The oldest city is Mobile, founded by French colonists.


The European-American naming of the Alabama River and state originates from the Alabama people, a Muskogean-speaking tribe whose members lived just below the confluence of the Coosa and Tallapoosa rivers on the upper reaches of the river. In the Alabama language, the word for an Alabama person is Albaamo (or variously Albaama or AlbÃ amo in different dialects; the plural form is Albaamaha).

The word Alabama is believed to have come from the related Choctaw language and was adopted by the Alabama tribe as their name.  The spelling of the word varies significantly among historical sources.  The first usage appears in three accounts of the Hernando de Soto expedition of 1540 with Garcilaso de la Vega using Alibamo, while the Knight of Elvas and Rodrigo Ranjel wrote Alibamu and Limamu, respectively, in efforts to transliterate the term.  As early as 1702, the French called the tribe the Alibamon, with French maps identifying the river as RiviÃ¨re des Alibamons.  Other spellings of the appellation have included Alibamu, Alabamo, Albama, Alebamon, Alibama, Alibamou, Alabamu, Allibamou.

Sources disagree on the meaning of the word. An 1842 article in the Jacksonville Republican proposed that it meant "Here We Rest." This notion was popularized in the 1850s through the writings of Alexander Beaufort Meek. Experts in the Muskogean languages have been unable to find any evidence to support such a translation.

Scholars believe the word comes from the Choctaw alba (meaning "plants" or "weeds") and amo (meaning "to cut", "to trim", or "to gather"). The meaning may have been "clearers of the thicket" or "herb gatherers", referring to clearing land for cultivation or collecting medicinal plants. The state has numerous place names of Native American origin.

Indigenous peoples of varying cultures lived in the area for thousands of years before European colonization. Trade with the northeastern tribes via the Ohio River began during the Burial Mound Period (1000BCâ€“AD700) and continued until European contact.
The agrarian Mississippian culture covered most of the state from 1000 to 1600 AD, with one of its major centers built at what is now the Moundville Archaeological Site in Moundville, Alabama. This is the second-largest complex of the classic Middle Mississippian era, after Cahokia in present-day Illinois, which was the center of the culture. Analysis of artifacts recovered from archaeological excavations at Moundville were the basis of scholars' formulating the characteristics of the Southeastern Ceremonial Complex (SECC). Contrary to popular belief, the SECC appears to have no direct links to Mesoamerican culture, but developed independently. The Ceremonial Complex represents a major component of the religion of the Mississippian peoples; it is one of the primary means by which their religion is understood.

Among the historical tribes of Native American people living in the area of present-day Alabama at the time of European contact were the Cherokee, an Iroquoian language people; and the Muskogean-speaking Alabama (Alibamu), Chickasaw, Choctaw, Creek, and Koasati. While part of the same large language family, the Muskogee tribes developed distinct cultures and languages.

With exploration in the 16th century, the Spanish were the first Europeans to reach Alabama. The expedition of Hernando de Soto passed through Mabila and other parts of the state in 1540. More than 160 years later, the French founded the first European settlement in the region at Old Mobile in 1702. The city was moved to the current site of Mobile in 1711. This area was claimed by the French from 1702 to 1763 as part of La Louisiane.

After the French lost to the British in the Seven Years' War, it became part of British West Florida from 1763 to 1783. After the United States victory in the American Revolutionary War, the territory was divided between the United States and Spain. The latter retained control of this western territory from 1783 until the surrender of the Spanish garrison at Mobile to U.S. forces on April 13, 1813.

Thomas Bassett, a loyalist to the British monarchy during the Revolutionary era, was one of the earliest White settlers in the state outside Mobile. He settled in the Tombigbee District during the early 1770s. The boundaries of the district were roughly limited to the area within a few miles of the Tombigbee River and included portions of what is today southern Clarke County, northernmost Mobile County, and most of Washington County.

What is now the counties of Baldwin and Mobile became part of Spanish West Florida in 1783, part of the independent Republic of West Florida in 1810, and was finally added to the Mississippi Territory in 1812. Most of what is now the northern two-thirds of Alabama was known as the Yazoo lands beginning during the British colonial period. It was claimed by the Province of Georgia from 1767 onwards. Following the Revolutionary War, it remained a part of Georgia, although heavily disputed.
With the exception of the area around Mobile and the Yazoo lands, what is now the lower one-third Alabama was made part of the Mississippi Territory when it was organized in 1798. The Yazoo lands were added to the territory in 1804, following the Yazoo land scandal. Spain kept a claim on its former Spanish West Florida territory in what would become the coastal counties until the Adamsâ€“OnÃ­s Treaty officially ceded it to the United States in 1819.

Prior to the admission of Mississippi as a state on December 10, 1817, the more sparsely settled eastern half of the territory was separated and named the Alabama Territory. The Alabama Territory was created by the United States Congress on March 3, 1817. St. Stephens, now abandoned, served as the territorial capital from 1817 to 1819.

The U.S. Congress selected Huntsville as the site for the first Constitutional Convention of Alabama after it was approved to become the 22nd state. From July 5 to August 2, 1819, delegates met to prepare the new state constitution. Huntsville served as the temporary capital of Alabama from 1819 to 1820, when the seat of state government was moved to Cahaba in Dallas County.
Cahaba, now a ghost town, was the first permanent state capital from 1820 to 1825.  Alabama Fever was already underway when the state was admitted to the Union, with settlers and land speculators pouring into the state to take advantage of fertile land suitable for cotton cultivation. Part of the frontier in the 1820s and 1830s, its constitution provided for universal suffrage for white men.

Southeastern planters and traders from the Upper South brought slaves with them as the cotton plantations in Alabama expanded. The economy of the central Black Belt (named for its dark, productive soil) was built around large cotton plantations whose owners' wealth grew largely from slave labor. The area also drew many poor, disfranchised people who became subsistence farmers. Alabama had a population estimated at under 10,000 people in 1810, but it had increased to more than 300,000 people by 1830.  Most Native American tribes were completely removed from the state within a few years of the passage of the Indian Removal Act by Congress in 1830.
From 1826 to 1846, Tuscaloosa served as the capital of Alabama. On January 30, 1846, the Alabama legislature announced that it had voted to move the capital city from Tuscaloosa to Montgomery. The first legislative session in the new capital met in December 1847. A new capitol building was erected under the direction of Stephen Decatur Button of Philadelphia. The first structure burned down in 1849, but was rebuilt on the same site in 1851. This second capitol building in Montgomery remains to the present day. It was designed by Barachias Holt of Exeter, Maine.

By 1860, the population had increased to a total of 964,201 people, of which nearly half, 435,080 were enslaved African Americans, and 2,690 were free people of color. On January 11, 1861, Alabama declared its secession from the Union.  After remaining an independent republic for a few days, it joined the Confederate States of America. The Confederacy's capital was initially located at Montgomery.  Alabama was heavily involved in the American Civil War.  Although comparatively few battles were fought in the state, Alabama contributed about 120,000 soldiers to the war effort.
A company of cavalry soldiers from Huntsville, Alabama joined Nathan Bedford Forrest's battalion in Hopkinsville, Kentucky.  The company wore new uniforms with yellow trim on the sleeves, collar and coat tails. This led to them being greeted with "Yellowhammer", and the name later was applied to all Alabama troops in the Confederate Army.

Alabama's slaves were freed by the 13th Amendment in 1865.  Alabama was under military rule from the end of the war in May 1865 until its official restoration to the Union in 1868. From 1867 to 1874, with most White citizens barred temporarily from voting and freedmen enfranchised, many African Americans emerged as political leaders in the state. Alabama was represented in Congress during this period by three African-American congressmen: Jeremiah Haralson, Benjamin S. Turner, and James T. Rapier.

Following the war, the state remained chiefly agricultural, with an economy tied to cotton. During Reconstruction, state legislators ratified a new state constitution in 1868 that created the state's first public school system and expanded women's rights. Legislators funded numerous public road and railroad projects, although these were plagued with allegations of fraud and misappropriation. Organized insurgent, resistance groups tried to suppress the freedmen and Republicans. Besides the short-lived original Ku Klux Klan, these included the Pale Faces, Knights of the White Camellia, Red Shirts, and the White League.

Reconstruction in Alabama ended in 1874, when the Democrats regained control of the legislature and governor's office through an election dominated by fraud and violence. They wrote another constitution in 1875, and the legislature passed the Blaine Amendment, prohibiting public money from being used to finance religious-affiliated schools. The same year, legislation was approved that called for racially segregated schools. Railroad passenger cars were segregated in 1891. After disfranchising most African Americans and many poor whites in the 1901 constitution, the Alabama legislature passed more Jim Crow laws at the beginning of the 20th century to impose segregation in everyday life.

The new 1901 Constitution of Alabama included provisions for voter registration that effectively disenfranchised large portions of the population, including nearly all African Americans and Native Americans, and tens of thousands of poor whites, through making voter registration difficult, requiring a poll taxes and literacy test. By 1903, only 2,980 African Americans were registered in Alabama, although at least 74,000 were literate. This compared to more than 181,000 African Americans eligible to vote in 1900. The numbers dropped even more in later decades.

While the planter class had persuaded poor Whites to vote for this legislative effort to suppress black voting, the new restrictions resulted in their disenfranchisement as well, due mostly to the imposition of a cumulative poll tax.  By 1941, whites constituted a slight majority of those disenfranchised by these laws: 600,000 Whites vs. 520,000 African-Americans. Nearly all African Americans had lost the ability to vote. Despite numerous legal challenges that succeeded in overturning certain provisions, the state legislature would create new ones to maintain disenfranchisement. The exclusion of blacks from the political system persisted until after passage of federal civil rights legislation in the 1965 to enforce their constitutional rights as citizens.

The 1901 constitution required racial segregation of public schools. It also restated that interracial marriage was illegal, as it had been prohibited in 1867. Into the 1950s, the state legislature passed additional racial segregation laws related to public facilities: jails were segregated in 1911; hospitals in 1915; toilets, hotels, and restaurants in 1928; and bus stop waiting rooms in 1945.

The rural-dominated Alabama legislature consistently underfunded schools and services for the disenfranchised African Americans, but it did not relieve them of paying taxes. Partially as a response to chronic underfunding of education for African Americans in the South, the Rosenwald Fund began funding the construction of what came to be known as Rosenwald Schools. In Alabama these schools were designed and the construction partially financed with Rosenwald funds, which paid one-third of the construction costs. The fund required the local community and state to raise matching funds to pay the rest. Black residents effectively taxed themselves twice, by raising additional monies to supply matching funds for such schools, which were built in many rural areas. They often donated land and labor as well.
Beginning in 1913, the first 80 Rosenwald Schools were built in Alabama for African-American children. A total of 387 schools, seven teachers' houses, and several vocational buildings were completed by 1937 in the state. Several of the surviving school buildings in the state are now listed on the National Register of Historic Places.

Continued racial discrimination and lynchings, agricultural depression, and the failure of the cotton crops due to boll weevil infestation led tens of thousands of African Americans from rural Alabama and other states to seek opportunities in northern and midwestern cities during the early decades of the 20th century as part of the Great Migration out of the South. Reflecting this emigration, the population growth rate in Alabama (see "Historical Populations" table below) dropped by nearly half from 1910 to 1920.

At the same time, many rural people, both White and African American, migrated to the city of Birmingham to work in new industrial jobs. Birmingham experienced such rapid growth that it was called "The Magic City". By the 1920s, Birmingham was the 19th-largest city in the United States and had more than 30% of the state's population. Heavy industry and mining were the basis of its economy. Its residents were under-represented for decades in the state legislature, which refused to redistrict after each decennial census according to population changes, as it was required by the state constitution. This did not change until the late 1960s, following a lawsuit and court order to establish the principle of apportionment as "one man, one vote".

Beginning in the 1940s, when the courts started taking the first steps to recognize the voting rights of black voters, the Alabama legislature took several counter -steps designed to disfranchise black voters. The legislature passed, and the voters ratified , a state constitutional amendment that gave local registrars greater latitude to disqualify voter registration applicants. Black citizens in Mobile successfully challenged this amendment as a violation of the Fifteenth Amendment. The legislature also changed the boundaries of Tuskegee to a 28-sided figure designed to fence out blacks from the city limits. The Supreme Court unanimously held that this racial "gerrymandering" violated the Constitution. In 1961, ... the Alabama legislature also intentionally diluted the effect of the black vote by instituting numbered place requirements for local elections.

Industrial development related to the demands of World War II brought a level of prosperity to the state not seen since before the Civil War.  Rural workers poured into the largest cities in the state for better jobs and a higher standard of living. One example of this massive influx of workers occurred in Mobile. Between 1940 and 1943, more than 89,000 people moved into the city to work for war-related industries. Cotton and other cash crops faded in importance as the state developed a manufacturing and service base.

Despite massive population changes in the state from 1901 to 1961, the rural-dominated legislature refused to reapportion House and Senate seats based on population, as required by the state constitution to follow the results of decennial censuses. They held on to old representation to maintain political and economic power in agricultural areas. In addition, the state legislature gerrymandered the few Birmingham legislative seats to ensure election by persons living outside Birmingham.

One result was that Jefferson County, containing Birmingham's industrial and economic powerhouse, contributed more than one-third of all tax revenue to the state, but did not receive a proportional amount in services. Urban interests were consistently underrepresented in the legislature. A 1960 study noted that because of rural domination, "a minority of about 25 per cent of the total state population is in majority control of the Alabama legislature."

A class action suit initiated on behalf of plaintiffs in Lowndes County, Alabama challenged the state legislature's lack of redistricting for congressional seats. In 1962 White v. Crook, Judge Frank M. Johnson ordered the state to redistrict. United States Supreme Court cases of Baker v. Carr (1962) and Reynolds v. Sims (1964) ruled that the principle of "one man, one vote" needed to be the basis of both houses of state legislatures as well, and that their districts had to be based on population, rather than geographic counties, as Alabama had used for its senate.

In 1972, for the first time since 1901, the legislature completed the first congressional redistricting based on the decennial census. This benefited the urban areas that had developed, as well as all in the population who had been underrepresented for more than 60 years. Other changes were made to implement representative state house and senate districts.

African Americans continued to press in the 1950s and 1960s to end disenfranchisement and segregation in the state through the Civil Rights Movement, including legal challenges. In 1954, the US Supreme Court ruled in Brown v. Board of Education that public schools had to be desegregated, but Alabama was slow to comply. During the 1960s, under Governor George Wallace, Alabama resisted compliance with federal demands for desegregation.The civil rights movement had notable events in Alabama, including the Montgomery Bus Boycott (1955â€“56), Freedom Rides in 1961, and 1965 Selma to Montgomery marches. These contributed to Congressional passage and enactment of the Civil Rights Act of 1964 and Voting Rights Act of 1965 by the U.S. Congress.

Legal segregation ended in the states in 1964, but Jim Crow customs often continued until specifically challenged in court.

Despite recommendations of a 1973 Alabama Constitutional Commission, the state legislature did not approve an amendment to establish home rule for counties. There is very limited home rule, but the legislature is deeply involved in passing legislation that applies to county-level functions and policies. This both deprives local residents of the ability to govern themselves and distracts the legislature from statewide issues.
 
Alabama has made some changes since the late 20th century and has used new types of voting to increase representation. In the 1980s, an omnibus redistricting case, Dillard v. Crenshaw County, challenged the at-large voting for representative seats of 180 Alabama jurisdictions, including counties and school boards. At-large voting had diluted the votes of any minority in a county, as the majority tended to take all seats. Despite African Americans making up a significant minority in the state, they had been unable to elect any representatives in most of the at-large jurisdictions.

As part of settlement of this case, five Alabama cites and counties, including Chilton County, adopted a system of cumulative voting for election of representatives in multi-seat jurisdictions. This has resulted in more proportional representation for voters. In another form of proportional representation, 23 jurisdictions use limited voting, as in Conecuh County. In 1982, limited voting was first tested in Conecuh County. Together use of these systems has increased the number of African Americans and women being elected to local offices, resulting in governments that are more representative of their citizens.

Alabama is the thirtieth-largest state in the United States with  of total area: 3.2% of the area is water, making Alabama 23rd in the amount of surface water, also giving it the second-largest inland waterway system in the U.S.  About three-fifths of the land area is a gentle plain with a general descent towards the Mississippi River and the Gulf of Mexico. The North Alabama region is mostly mountainous, with the Tennessee River cutting a large valley and creating numerous creeks, streams, rivers, mountains, and lakes.

Alabama is bordered by the states of Tennessee to the north, Georgia to the east, Florida to the south, and Mississippi to the west. Alabama has coastline at the Gulf of Mexico, in the extreme southern edge of the state.  The state ranges in elevation from sea level at Mobile Bay to over 1,800feet (550m) in the Appalachian Mountains in the northeast.

The highest point is Mount Cheaha, at a height of .  Alabama's land consists of  of forest or 67% of total land area. Suburban Baldwin County, along the Gulf Coast, is the largest county in the state in both land area and water area.

Areas in Alabama administered by the National Park Service include Horseshoe Bend National Military Park near Alexander City; Little River Canyon National Preserve near Fort Payne; Russell Cave National Monument in Bridgeport; Tuskegee Airmen National Historic Site in Tuskegee; and Tuskegee Institute National Historic Site near Tuskegee.

Additionally, Alabama has four National Forests: Conecuh, Talladega, Tuskegee, and William B. Bankhead.  Alabama also contains the Natchez Trace Parkway, the Selma To Montgomery National Historic Trail, and the Trail Of Tears National Historic Trail. A notable natural wonder in Alabama is "Natural Bridge" rock, the longest natural bridge east of the Rockies, located just south of Haleyville.

A -wide meteorite impact crater is located in Elmore County, just north of Montgomery. This is the Wetumpka crater, the site of "Alabama's greatest natural disaster."  A -wide meteorite hit the area about 80million years ago. The hills just east of downtown Wetumpka showcase the eroded remains of the impact crater that was blasted into the bedrock, with the area labeled the Wetumpka crater or astrobleme ("star-wound") because of the concentric rings of fractures and zones of shattered rock that can be found beneath the surface. In 2002, Christian Koeberl with the Institute of Geochemistry University of Vienna published evidence and established the site as the 157th recognized impact crater on Earth.

The state is classified as humid subtropical (Cfa) under the Koppen Climate Classification. The average annual temperature is 64Â°F (18Â°C). Temperatures tend to be warmer in the southern part of the state with its proximity to the Gulf of Mexico, while the northern parts of the state, especially in the Appalachian Mountains in the northeast, tend to be slightly cooler.  Generally, Alabama has very hot summers and mild winters with copious precipitation throughout the year. Alabama receives an average of  of rainfall annually and enjoys a lengthy growing season of up to 300days in the southern part of the state.

Summers in Alabama are among the hottest in the U.S., with high temperatures averaging over  throughout the summer in some parts of the state. Alabama is also prone to tropical storms and even hurricanes. Areas of the state far away from the Gulf are not immune to the effects of the storms, which often dump tremendous amounts of rain as they move inland and weaken.

South Alabama reports many thunderstorms. The Gulf Coast, around Mobile Bay, averages between 70 and 80 days per year with thunder reported. This activity decreases somewhat further north in the state, but even the far north of the state reports thunder on about 60days per year. Occasionally, thunderstorms are severe with frequent lightning and large hail; the central and northern parts of the state are most vulnerable to this type of storm. Alabama ranks ninth in the number of deaths from lightning and tenth in the number of deaths from lightning strikes per capita.

Alabama, along with Oklahoma, has the most reported EF5 tornadoes of any state, according to statistics from the National Climatic Data Center for the period January 1, 1950, to June 2013. Several long-tracked F5 tornadoes have contributed to Alabama reporting more tornado fatalities than any other state, even surpassing Texas which has a much larger area within Tornado Alley. The state suffered tremendous damage in the Super Outbreak of April 1974, and the April 25â€“28, 2011 tornado outbreak. The outbreak in April 2011 produced a record amount of tornadoes in the state. The tally reached 62.
The peak season for tornadoes varies from the northern to southern parts of the state. Alabama is one of the few places in the world that has a secondary tornado season in November and December, along with the spring severe weather season. The northern part of the stateâ€”along the Tennessee Valleyâ€”is one of the areas in the U.S. most vulnerable to violent tornadoes. The area of Alabama and Mississippi most affected by tornadoes is sometimes referred to as Dixie Alley, as distinct from the Tornado Alley of the Southern Plains.

Winters are generally mild in Alabama, as they are throughout most of the southeastern U.S., with average January low temperatures around  in Mobile and around  in Birmingham. Although snow is a rare event in much of Alabama, areas of the state north of Montgomery may receive a dusting of snow a few times every winter, with an occasional moderately heavy snowfall every few years. Historic snowfall events include New Year's Eve 1963 snowstorm and the 1993 Storm of the Century. The annual average snowfall for the Birmingham area is  per year. In the southern Gulf coast, snowfall is less frequent, sometimes going several years without any snowfall.

Alabama's highest temperature of  was recorded on September 5, 1925 in the unincorporated community of Centerville. The record low of  occurred on January 30, 1966 in New Market.
Alabama is home to a diverse array of flora and fauna, due largely to a variety of habitats that range from the Tennessee Valley, Appalachian Plateau, and Ridge-and-Valley Appalachians of the north to the Piedmont, Canebrake and Black Belt of the central region to the Gulf Coastal Plain and beaches along the Gulf of Mexico in the south. The state is usually ranked among the top in nation for its range of overall biodiversity.

Alabama once boasted huge expanses of pine forest, which still form the largest proportion of forests in the state. It currently ranks fifth in the nation for the diversity of its flora. It is home to nearly 4,000 pteridophyte and spermatophyte plant species.

Indigenous animal species in the state include 62 mammal species, 93 reptile species, 73 amphibian species, roughly 307 native freshwater fish species, and 420 bird species that spend at least part of their year within the state.  Invertebrates include 83 crayfish species and 383 mollusk species. 113 of these mollusk species have never been collected outside the state.

The United States Census Bureau estimates that the population of Alabama was 4,849,377 on July 1, 2014, which represents an increase of 69,641, or 1.46%, since the 2010 Census. This includes a natural increase since the last census of 121,054 people (that is 502,457 births minus 381,403 deaths) and an increase due to net migration of 104,991 people into the state.

Immigration from outside the U.S. resulted in a net increase of 31,180 people, and migration within the country produced a net gain of 73,811 people.  The state had 108,000 foreign-born (2.4% of the state population), of which an estimated 22.2% were illegal immigrants (24,000).

The center of population of Alabama is located in Chilton County, outside the town of Jemison.

According to the 2010 Census, Alabama had a population of 4,779,736.  The racial composition of the state was 68.5% White (67.0% Non-Hispanic White and 1.5% Hispanic White), 26.2% Black or African American, 3.9% Hispanic or Latino of any race, 1.1% Asian, 0.6% American Indian and Alaska Native, 0.1% Native Hawaiian and Other Pacific Islander, 2.0% from Some Other Race, and 1.5% from Two or More Races. In 2011, 46.6% of Alabama's population younger than age 1 were minorities.

The largest reported ancestry groups in Alabama are: African American (26.2%), English (23.6%), Irish (7.7%), German (5.7%), and Scots-Irish (2.0%).  Those citing "American" ancestry in Alabama are generally of English or British ancestry; many Anglo-Americans identify as having American ancestry because their roots have been in North America for so long, in some cases since the 1600s. Demographers estimate that a minimum of 20â€“23% of people in Alabama are of predominantly English ancestry and that the figure is likely higher. In the 1980 census, 41% of the people in Alabama identified as being of English ancestry, making them the largest ethnic group at the time.
Based on historic migration and settlement patterns in the southern colonies and states, demographers estimated there are more people in Alabama of Scots-Irish origins than self-reported.  Many people in Alabama claim Irish ancestry because of the term Scots-Irish but, based on historic immigration and settlement, their ancestors were more likely Protestant Scots-Irish coming from northern Ireland, where they had been for a few generations as part of the English colonization. The Scots-Irish were the largest non-English immigrant group from the British Isles before the American Revolution, and many settled in the South, later moving into the Deep South as it was developed.

In 1984, under the Davisâ€“Strong Act, the state legislature established the Alabama Indian Affairs Commission. Native American groups within the state had increasingly been demanding recognition as ethnic groups and seeking an end to discrimination. Given the long history of slavery and associated racial segregation, the Native American peoples, who have sometimes been of mixed race, have insisted on having their cultural identification respected. In the past, their self-identification was often overlooked as the state tried to impose a binary breakdown of society into white and black.

The state has officially recognized nine American Indian tribes in the state, descended mostly from the Five Civilized Tribes of the American Southeast. These are:
Poarch Band of Creek Indians (who also have federal recognition),
MOWA Band of Choctaw Indians,
Star Clan of Muscogee Creeks,
Echota Cherokee Tribe of Alabama,
Cherokee Tribe of Northeast Alabama,
Cher-O-Creek Intra Tribal Indians,
Ma-Chis Lower Creek Indian Tribe,
Piqua Shawnee Tribe, and
Ani-Yun-Wiya Nation.

The state government has promoted recognition of Native American contributions to the state, including the designation in 2000 for Columbus Day to be jointly celebrated as American Indian Heritage Day.

Sources: Census.gov
Sources: Census.gov

96.1% of all Alabama residents five years old or older spoke only English at home in 2000, a minor decrease from  97.1% in 1990. Alabama English is predominantly Southern, and is related to South Midland speech which was taken across the border from Tennessee. In the major Southern speech region, there is the decreasing loss of the final /r/, for example the /boyd/ pronunciation of 'bird.' In the northern third of the state, there is a South Midland 'arm' and 'barb' rhyming with 'form' and 'orb.' Unique words in Alabama English include: redworm (earthworm), peckerwood (woodpecker), snake doctor and snake feeder (dragonfly), tow sack (burlap bag), plum peach (clingstone), French harp (harmonica), and dog irons (andirons).
In the 2008 American Religious Identification Survey, 86% of Alabama respondents reported their religion as Christian, including 6% Catholic, and 11% as having no religion. The composition of other traditions is 0.5% Mormon, 0.5% Jewish, 0.5% Muslim, 0.5% Buddhist, and 0.5% Hindu.

Alabama is located in the middle of the Bible Belt, a region of numerous Protestant Christians. Alabama has been identified as one of the most religious states in the United States, with about 58% of the population attending church regularly. A majority of people in the state identify as Evangelical Protestant. , the three largest denominational groups in Alabama are the Southern Baptist Convention, The United Methodist Church, and non-denominational Evangelical Protestant.

In Alabama, the Southern Baptist Convention has the highest number of adherents with 1,380,121; this is followed by the United Methodist Church with 327,734 adherents, non-denominational Evangelical Protestant with 220,938 adherents, and the Catholic Church with 150,647 adherents. Many Baptist and Methodist congregations became established in the Great Awakening of the early 19th century, when preachers proselytized across the South. The Assemblies of God had almost 60,000 members, the Churches of Christ had nearly 120,000 members. The Presbyterian churches, strongly associated with Scots-Irish immigrants of the 18th century and their descendants, had a combined membership around 75,000 (PCA-28,009 members in 108 congregations, PC(USA)-26,247 members in 147 congregations, the Cumberland Presbyterian Church-6,000 members in 59 congregations, the Cumberland Presbyterian Church in America-5,000 members and 50 congregations plus the EPC and Associate Reformed Presbyterians with 230 members and 9 congregations).

In a 2007 survey, nearly 70% of respondents could name all four of the Christian Gospels. Of those who indicated a religious preference, 59% said they possessed a "full understanding" of their faith and needed no further learning.  In a 2007 poll, 92% of Alabamians reported having at least some confidence in churches in the state.

Although in much smaller numbers, many other religious faiths are represented in the state as well, including Judaism, Islam, Hinduism, Buddhism, Sikhism, the BahÃ¡'Ã­ Faith, and Unitarian Universalism.

Jews have been present in what is now Alabama since 1763, during the colonial era of Mobile, when Sephardic Jews immigrated from London. The oldest Jewish congregation in the state is Congregation Sha'arai Shomayim in Mobile. It was formally recognized by the state legislature on January 25, 1844.  Later immigrants in the nineteenth and twentieth centuries tended to be Ashkenazy Jews from eastern Europe. Jewish denominations in the state include two Orthodox, four Conservative, ten Reform, and one Humanistic synagogue.

Muslims have been increasing in Alabama, with 31 mosques built by 2011, many by African-American converts. Islam was a traditional religion in West Africa, from where many slaves were brought to the colonies and the United States during the centuries of the slave trade.

Several Hindu temples and cultural centers in the state have been founded by Indian immigrants and their descendants, the most well-known being the Shri Swaminarayan Mandir in Birmingham, the Hindu Temple and Cultural Center of Birmingham in Pelham, the Hindu Cultural Center of North Alabama in Capshaw, and the Hindu Mandir and Cultural Center in Tuscaloosa.

There are six Dharma centers and organizations for Theravada Buddhists. Most monastic Buddhist temples are concentrated in southern Mobile County, near Bayou La Batre. This area has attracted an influx of refugees from Cambodia, Laos, and Vietnam during the 1970s and thereafter. The four temples within a ten-mile radius of Bayou La Batre, include Chua Chanh Giac, Wat Buddharaksa, and Wat Lao Phoutthavihan.

The first community of adherents of the Baha'i Faith in Alabama was founded in 1896 by Paul K. Dealy who moved from Chicago to Fairhope to participate in the growth of Fairhope as a utopian community.  The first community of Baha'is in Alabama was racially integrated from the beginning due to the Faith's principles.  Today there is an exhibit honoring Dealy in Haifa, Israel at the world center of the Baha'i Faith.  Baha'i Centers in Alabama exist in Birmingham, Alabama, Huntsville, Alabama, and Florence, Alabama.

A Centers for Disease Control and Prevention study in 2008 showed that obesity in Alabama was a problem, with most counties having over 29% of adults obese, except for ten which had a rate between 26% and 29%. Residents of the state, along with those in five other states, were least likely in the nation to be physically active during leisure time. Alabama, and the southeastern U.S. in general, has one of the highest incidences of adult onset diabetes in the country, exceeding 10% of adults.

The state has invested in aerospace, education, health care, banking, and various heavy industries, including automobile manufacturing, mineral extraction, steel production and fabrication. By 2006, crop and animal production in Alabama was valued at $1.5 billion. In contrast to the primarily agricultural economy of the previous century, this was only about 1% of the state's gross domestic product. The number of private farms has declined at a steady rate since the 1960s, as land has been sold to developers, timber companies, and large farming conglomerates.

Non-agricultural employment in 2008 was 121,800 in management occupations; 71,750 in business and financial operations; 36,790 in computer-related and mathematical occupation; 44,200 in architecture and engineering; 12,410 in life, physical, and social sciences; 32,260 in community and social services; 12,770 in legal occupations; 116,250 in education, training, and library services; 27,840 in art, design and media occupations; 121,110 in healthcare; 44,750 in fire fighting, law enforcement, and security; 154,040 in food preparation and serving; 76,650 in building and grounds cleaning and maintenance; 53,230 in personal care and services; 244,510 in sales; 338,760 in office and administration support; 20,510 in farming, fishing, and forestry; 120,155 in construction and mining, gas, and oil extraction; 106,280 in installation, maintenance, and repair; 224,110 in production; and 167,160 in transportation and material moving.

According to the U.S. Bureau of Economic Analysis, the 2008 total gross state product was $170billion, or $29,411 per capita. Alabama's 2012 GDP increased 1.2% from the previous year. The single largest increase came in the area of information.  In 2010, per capita income for the state was $22,984.

The state's seasonally adjusted unemployment rate was 6.4% in February 2014.  This compared to a nationwide seasonally adjusted rate of 6.7%.

The five employers that employed the most employees in Alabama in April 2011 were:
The next twenty largest employers, , included:
Alabama's agricultural outputs include poultry and eggs, cattle, fish, plant nursery items, peanuts, cotton, grains such as corn and sorghum, vegetables, milk, soybeans, and peaches. Although known as "The Cotton State," Alabama ranks between eighth and tenth in national cotton production, according to various reports, with Texas, Georgia and Mississippi comprising the top three.

Alabama's industrial outputs include iron and steel products (including cast-iron and steel pipe); paper, lumber, and wood products; mining (mostly coal); plastic products; cars and trucks; and apparel. In addition, Alabama produces aerospace and electronic products, mostly in the Huntsville area, the location of NASA's George C. Marshall Space Flight Center and the U.S. Army Materiel Command, headquartered at Redstone Arsenal.
A great deal of Alabama's economic growth since the 1990s has been due to the state's expanding automotive manufacturing industry. Located in the state are Honda Manufacturing of Alabama, Hyundai Motor Manufacturing Alabama, Mercedes-Benz U.S. International, and Toyota Motor Manufacturing Alabama, as well as their various suppliers. Since 1993, the automobile industry has generated more than 67,800 new jobs in the state. Alabama currently ranks 4th in the nation for vehicle exports.

Automakers accounted for approximately a third of the industrial expansion in the state in 2012.  The eight models produced at the state's auto factories totaled combined sales of 74,335 vehicles for 2012.  The strongest model sales during this period were the Hyundai Elantra compact car, the Mercedes-Benz GL-Class sport utility vehicle and the Honda Ridgeline sport utility truck.
Steel producers Outokumpu, Nucor, SSAB, ThyssenKrupp, and U.S. Steel have facilities in Alabama and employ over 10,000 people. In May 2007, German steelmaker ThyssenKrupp selected Calvert in Mobile County for a 4.65 billion combined stainless and carbon steel processing facility.  ThyssenKrupp's stainless steel division, Inoxum, including the stainless portion of the Calvert plant, was sold to Finnish stainless steel company Outokumpu in 2012.  The remaining portion of the ThyssenKrupp plant had final bids submitted by ArcelorMittal and Nippon Steel for $1.6 billion in March 2013.  Companhia SiderÃºrgica Nacional submitted a combined bid for the mill at Calvert, plus a majority stake in the ThyssenKrupp mill in Brazil, for $3.8 billion. In July 2013, the plant was sold to ArcelorMittal and Nippon Steel.

The Hunt Refining Company, a subsidiary of Hunt Consolidated, Inc., is based in Tuscaloosa and operates a refinery there. The company also operates terminals in Mobile, Melvin, and Moundville. JVC America, Inc. operates an optical disc replication and packaging plant in Tuscaloosa.

The Goodyear Tire and Rubber Company operates a large plant in Gadsden that employs about 1,400 people.  It has been in operation since 1929.

Construction of an Airbus A320 family aircraft assembly plant in Mobile was formally announced by Airbus CEO Fabrice BrÃ©gier from the Mobile Convention Center on July 2, 2012. The plans include a $600 million factory at the Brookley Aeroplex for the assembly of the A319, A320 and A321 aircraft. Construction began in 2013, with plans for it to become operable by 2015 and produce up to 50 aircraft per year by 2017. The assembly plant is the company's first factory to be built within the United States. It was announced on February 1, 2013 that Airbus had hired Alabama-based Hoar Construction to oversee construction of the facility.

An estimated 20 million tourists visit the state each year. Over 100,000 of these are from other countries, including from Canada, the United Kingdom, Germany and Japan. In 2006, 22.3 million tourists spent $8.3 billion providing an estimated 162,000 jobs in the state.

UAB Hospital is the only Level I trauma center in Alabama.  UAB is the largest state government employer in Alabama, with a workforce of about 18,000.

Alabama has the headquarters of Regions Financial Corporation, BBVA Compass, Superior Bancorp and the former Colonial Bancgroup. Birmingham-based Compass Banchshares was acquired by Spanish-based BBVA in September 2007, although the headquarters of BBVA Compass remains in Birmingham. In November 2006, Regions Financial completed its merger with AmSouth Bancorporation, which was also headquartered in Birmingham. SouthTrust Corporation, another large bank headquartered in Birmingham, was acquired by Wachovia in 2004 for $14.3 billion.

The city still has major operations for Wachovia and its now post-operating bank Wells Fargo, which includes a regional headquarters, an operations center campus and a $400 million data center. Nearly a dozen smaller banks are also headquartered in the Birmingham, such as Superior Bancorp, ServisFirst and New South Federal Savings Bank. Birmingham also serves as the headquarters for several large investment management companies, including Harbert Management Corporation.

Telecommunications provider AT, formerly BellSouth, has a major presence in Alabama with several large offices in Birmingham. The company has over 6,000 employees and more than 1,200 contract employees.

Many commercial technology companies are headquartered in Huntsville, such as the network access company ADTRAN, computer graphics company Intergraph, design and manufacturer of IT infrastructure Avocent, and telecommunications provider Deltacom. Cinram manufactures and distributes 20th Century Fox DVDs and Blu-ray Discs out of their Huntsville plant.

Rust International has grown to include Brasfield & Gorrie, BE, Hoar Construction and B.L. Harbert International, which all routinely are included in the Engineering News-Record lists of top design, international construction, and engineering firms. (Rust International was acquired in 2000 by Washington Group International, which was in turn acquired by San-Francisco based URS Corporation in 2007.)


The foundational document for Alabama's government is the Alabama Constitution, which was ratified in 1901. At almost 800 amendments and 310,000 words, it is by some accounts the world's longest constitution and is roughly forty times the length of the United States Constitution.

There has been a significant movement to rewrite and modernize Alabama's constitution. Critics suggest that Alabama's constitution highly centralizes power in Montgomery and leaves practically no power in local hands. Most counties do not have home rule. Any policy changes proposed around the state must be approved by the entire Alabama legislature and, frequently, by state referendum. One criticism of the current constitution claims that its complexity and length intentionally codify segregation and racism.
Alabama's government is divided into three coequal branches. The legislative branch is the Alabama Legislature, a bicameral assembly composed of the Alabama House of Representatives, with 105 members, and the Alabama Senate, with 35 members. The Legislature is responsible for writing, debating, passing, or defeating state legislation. The Republican Party currently holds a majority in both houses of the Legislature. The Legislature has the power to override a gubernatorial veto by a simple majority (most state Legislatures require a two-thirds majority to override a veto).

Until 1964, the state elected state senators by county, with one per county. It had not redistricted congressional districts since passage of its constitution in 1901; as a result, urbanized areas were grossly underrepresented. It had not changed legislative districts to reflect the decennial censuses, either. In Reynolds v. Sims (1964), the US Supreme Court implemented the principle of "one man, one vote", ruling that congressional districts had to be reapportioned based on censuses (as the state already had in its constitution but had not implemented.) Further, it ruled that both houses of bicameral state legislatures had to be apportioned by population, as there was no constitutional basis for states to have geographically based systems. At that time, Alabama and many other states had to change their legislative districting, as many across the country had systems that underrepresented urban areas and districts. This had caused decades of underinvestment in such areas. For instance, Birmingham and Jefferson County taxes had supplied one-third of the state budget, but Jefferson County received only 1/67th of state services in funding. Through the legislative delegations, the Alabama legislature kept control of county governments.

The executive branch is responsible for the execution and oversight of laws. It is headed by the Governor of Alabama. Other members of executive branch include the cabinet, the Attorney General of Alabama, the Alabama Secretary of State, the Alabama State Treasurer, and the State Auditor of Alabama. The current governor of the state is Republican Robert Bentley. The lieutenant governor is Republican Kay Ivey.

The judicial branch is responsible for interpreting the Constitution and applying the law in state criminal and civil cases. The state's highest court is the Supreme Court of Alabama. Alabama uses partisan elections to choose judges, and since the 1980s judicial campaigns have become increasingly politicized. The current chief justice of the Alabama Supreme Court is Republican Roy Moore. All sitting justices on the Alabama Supreme Court are members of the Republican Party. There are two intermediate appellate courts, the Court of Civil Appeals and the Court of Criminal Appeals, and four trial courts: the circuit court (trial court of general jurisdiction), and the district, probate, and municipal courts.

The members of the Legislature take office immediately after the November elections. Statewide officials such as the governor, lieutenant governor, attorney general, and other constitutional officers take office the following January.

Alabama levies a 2, 4, or 5 percent personal income tax, depending upon the amount earned and filing status. Taxpayers are allowed to deduct their federal income tax from their Alabama state tax, and can do so even if taking the standard deduction. Taxpayers who file itemized deductions are also allowed to deduct the Federal Insurance Contributions Act tax (Social Security and Medicare tax).

The state's general sales tax rate is 4%.  Sales tax rates for cities and counties are also added to purchases. For example, the total sales tax rate in Mobile is 10% and there is an additional restaurant tax of 1%, which means that a diner in Mobile would pay an 11% tax on a meal. , sales and excise taxes in Alabama account for 51% of all state and local revenue, compared with an average of about 36% nationwide. Alabama is one of seven states that levy a tax on food at the same rate as other goods, and one of two states (the other being neighboring Mississippi) which fully taxes groceries without any offsetting relief for low-income families. (Most states exempt groceries from sales tax, or have a lower rate for grocieries).

Alabama's income tax on poor working families is among the highest in the United States. Alabama is the only state that levies income tax on a family of four with income as low as $4,600, which is barely one-quarter of the federal poverty line. Alabama's threshold is the lowest among the 41 states and the District of Columbia with income taxes.

The corporate income tax rate is currently 6.5%. The overall federal, state, and local tax burden in Alabama ranks the state as the second least tax-burdened state in the country.  Property taxes are the lowest in the U.S. The current state constitution requires a voter referendum to raise property taxes.

Since Alabama's tax structure largely depends on consumer spending, it is subject to high variable budget structure. For example, in 2003 Alabama had an annual budget deficit as high as $670million.

Alabama has 67 counties. Each county has its own elected legislative branch, usually called the county commission. It also has limited executive authority in the county. Because of the constraints of the Alabama Constitution, only seven counties (Jefferson, Lee, Mobile, Madison, Montgomery, Shelby, and Tuscaloosa) in the state have limited home rule. Instead, most counties in the state must lobby the Local Legislation Committee of the state legislature to get simple local policies approved, ranging from waste disposal to land use zoning. The cumbersome process results in local jurisdictions being unable to manage their problems, and the state legislators are buried in local county issues.

The state legislature has retained power over local governments by refusing to pass a constitutional amendment establishing home rule for counties, as recommended by the 1973 Alabama Constitutional Commission. Legislative delegations retain certain powers over each county. United States Supreme Court decisions in Baker v. Carr (1964) required that both houses have districts established on the basis of population, and redistricted after each census, in order to implement the principle of "one man, one vote". Before that, each county was represented by one state senator, leading to underrepresentation in the state senate for more urbanized, populous counties.

"The lack of home rule for counties in Alabama has resulted in the proliferation of local legislation permitting counties to do things not authorized by the state constitution. Alabama's constitution has been amended more than 700 times, and almost one-third of the amendments are local in nature, applying to only one county or city. A significant part of each legislative session is spent on local legislation, taking away time and attention of legislators from issues of statewide importance."

On November 9, 2011, Jefferson County, which was $4 billion in debt at the time, declared bankruptcy. This is the second-largest Chapter 9 (municipal) bankruptcy in the United States, after the Detroit bankruptcy. Jefferson County emerged from bankruptcy in December 2013 following the approval of a bankruptcy plan by the United States bankruptcy court for the Northern District of Alabama.
 
Alabama is an alcoholic beverage control state, meaning that the state government holds a monopoly on the sale of alcohol. The Alabama Alcoholic Beverage Control Board controls the sale and distribution of alcoholic beverages in the state. Twenty-five of the 67 counties are "dry counties" which ban the sale of alcohol, and there are many dry municipalities even in counties which permit alcohol sales.
During Reconstruction following the American Civil War, Alabama was occupied by federal troops of the Third Military District under General John Pope. In 1874, the political coalition of white Democrats known as the Redeemers took control of the state government from the Republicans, in part by suppressing the African-American vote through violence, fraud and intimidation.

After 1890, a coalition of White Democratic politicians passed laws to segregate and disenfranchise African American residents, a process completed in provisions of the 1901 constitution. Provisions which disenfranchised African Americans resulted in excluding many poor Whites. By 1941 more Whites than African Americans had been disenfranchised: 600,000 to 520,000. The total effects were greater on the African-American community, as almost all of its citizens were disfranchised and relegated to separate and unequal treatment under the law.

From 1901 through the 1960s, the state did not redraw election districts as population grew and shifted within the state during urbanization and industrialization of certain areas. As counties were the basis of election districts, the result was a rural minority that dominated state politics through nearly three-quarters of the century, until a series of federal court cases required redistricting in 1972 to achieve what is called "one man, one vote."

Alabama state politics gained nationwide and international attention in the 1950s and 1960s during the American Civil Rights Movement, when Whites bureaucratically, and at times, violently resisted protests for electoral and social reform. Democrat George Wallace, the state's only four-term governor, was a controversial figure who vowed to maintain segregation. Only after passage of the Federal Civil Rights Act of 1964 and Voting Rights Act of 1965 did African Americans regain the ability to exercise suffrage, among other civil rights. In many jurisdictions, they continued to be excluded from representation by at-large electoral systems, which allowed the majority of the population to dominate elections. Some changes at the county level have occurred following court challenges to establish single-member districts that enable a more diverse representation among county boards.

In 2007, the Alabama Legislature passed, and Republican Governor Bob Riley signed a resolution expressing "profound regret" over slavery and its lingering impact. In a symbolic ceremony, the bill was signed in the Alabama State Capitol, which housed Congress of the Confederate States of America.

In 2010, Republicans won control of both houses of the legislature for the first time in 136 years, after a nearly complete realignment of political parties, who represent different visions in the 21st century.

With the disfranchisement of African Americans in 1901, the state became part of the "Solid South", a system in which the Democratic Party operated as effectively the only viable political party in every Southern state. For nearly 100years, local and state elections in Alabama were decided in the Democratic Party primary, with generally only token Republican challengers running in the General Election. Since the mid to late-20th century, however, there has been a realignment among the two major political parties, and white conservatives started shifting to the Republican Party. In Alabama, majority-white districts are now expected to regularly elect Republican candidates to federal, state and local office.

Members of the nine seats on the Alabama Supreme Court and all ten seats on the state appellate courts are elected to office. Until 1994, no Republicans held any of the court seats. In that general election, the then-incumbent Chief Justice of Alabama, Ernest C. Hornsby, refused to leave office after losing the election by approximately 3,000 votes to Republican Perry O. Hooper, Sr.. Hornsby sued Alabama and defiantly remained in office for nearly a year before finally giving up the seat after losing in court. This ultimately led to a collapse of support for Democrats at the ballot box in the next three or four election cycles. The Democrats lost the last of the nineteen court seats in August 2011 with the resignation of the last Democrat on the bench.

In the early 21st century, Republicans hold all seven of the statewide elected executive branch offices. Republicans hold six of the eight elected seats on the Alabama State Board of Education. In 2010, Republicans took large majorities of both chambers of the state legislature, giving them control of that body for the first time in 136 years. The last remaining statewide Democrat, who served on the Alabama Public Service Commission was defeated in 2012.

Only two Republican Lieutenant Governors have been elected since the end of Reconstruction, when Republicans generally represented Reconstruction government, including the newly emancipated freedmen who had gained the franchise. The two GOP Lt. Governors were Steve Windom (1999-2003) and the current Lt. Governor, Kay Ivey, who was elected in 2010 and re-elected in 2014.

Many local offices (County Commissioners, Boards of Education, Tax Assessors, Tax Collectors, etc.) in the state are still held by Democrats. Many rural counties have voters who are majority Democrats, resulting in local elections being decided in the Democratic primary. Similarly many metropolitan and suburban counties are majority-Republican and elections are effectively decided in the Republican Primary, although there are exceptions.

Alabama's 67 County Sheriffs are elected in partisan, at-large races, and Democrats still retain the majority of those posts. The current split is 42 Democrats, 24 Republicans, and one Independent (Choctaw). However, most of the Democratic sheriffs preside over rural and less populated counties. The majority of Republican sheriffs have been elected in the more urban/suburban and heavily populated counties. Two Alabama counties (Montgomery and Calhoun) with a population of over 100,000 have Democratic sheriffs; and five Alabama counties with a population of under 75,000 have Republican sheriffs (Autauga, Coffee, Dale, Coosa, and Blount). , the state of Alabama has one female sheriff, in Morgan County, Alabama, and nine African-American sheriffs.

The state's two U.S. senators are Jefferson B. Sessions III and Richard C. Shelby, both Republicans. Shelby was originally elected to the Senate as a Democrat in 1986 and re-elected in 1992, but switched parties immediately following the November 1994 general election.

In the U.S. House of Representatives, the state is represented by seven members, six of whom are Republicans: (Bradley Byrne, Mike D. Rogers, Robert Aderholt, Morris J. Brooks, Martha Roby, and Gary Palmer) and one Democrat: Terri Sewell.
Public primary and secondary education in Alabama is under the purview of the Alabama State Board of Education as well as local oversight by 67 county school boards and 60 city boards of education. Together, 1,496 individual schools provide education for 744,637 elementary and secondary students.

Public school funding is appropriated through the Alabama Legislature through the Education Trust Fund. In FY 2006â€“2007, Alabama appropriated $3,775,163,578 for primary and secondary education. That represented an increase of $444,736,387 over the previous fiscal year. In 2007, over 82percent of schools made adequate yearly progress (AYP) toward student proficiency under the National No Child Left Behind law, using measures determined by the state of Alabama.

While Alabama's public education system has improved in recent decades, it lags behind in achievement compared to other states. According to U.S. Census data, Alabama's high school graduation rateâ€”75%â€”is the fourth lowest in the U.S. (after Kentucky, Louisiana and Mississippi).  The largest educational gains were among people with some college education but without degrees.

Alabama's programs of higher education include 14 four-year public universities, two-year community colleges, and 17 private, undergraduate and graduate universities. In the state are three medical schools (University of Alabama School of Medicine, University of South Alabama and Alabama College of Osteopathic Medicine), two veterinary colleges (Auburn University and Tuskegee University), a dental school (University of Alabama School of Dentistry), an optometry college (University of Alabama at Birmingham), two pharmacy schools (Auburn University and Samford University), and five law schools (University of Alabama School of Law, Birmingham School of Law, Cumberland School of Law, Miles Law School, and the Thomas Goode Jones School of Law). Public, post-secondary education in Alabama is overseen by the Alabama Commission on Higher Education and the Alabama Department of Postsecondary Education. Colleges and universities in Alabama offer degree programs from two-year associate degrees to a multitude of doctoral level programs.
The largest single campus is the University of Alabama, located in Tuscaloosa, with 33,602 enrolled for fall 2012. Troy University was the largest institution in the state in 2010, with an enrollment of 29,689 students across four Alabama campuses (Troy, Dothan, Montgomery, and Phenix City), as well as sixty learning sites in seventeen other states and eleven other countries. The oldest institutions are the public University of North Alabama in Florence and the Catholic Church-affiliated Spring Hill College in Mobile, both founded in 1830.

Accreditation of academic programs is through the Southern Association of Colleges and Schools (SACS) as well as other subject-focused national and international accreditation agencies such as the Association for Biblical Higher Education (ABHE), the Council on Occupational Education (COE), and the Accrediting Council for Independent Colleges and Schools (ACICS).

According to the 2011 U.S. News & World Report, Alabama had three universities ranked in the top 100 Public Schools in America (University of Alabama at 31, Auburn University at 36, and University of Alabama at Birmingham at 73).

According to the 2012 U.S. News & World Report, Alabama had four tier 1 universities (University of Alabama, Auburn University, University of Alabama at Birmingham and University of Alabama in Huntsville).


College football is popular in Alabama, particularly the University of Alabama and Auburn University. In the 2013 season, Alabama averaged over 100,000 fans per game and Auburn averaged over 80,000 fans, both numbers among the top 20 in the nation in average attendance. Bryant-Denny Stadium serves as the home of the University of Alabama football team. It has a seating capacity of 101,821, and is the fifth largest stadium in America.  Jordan-Hare Stadium is the home field of the Auburn University football team and has a seating capacity of 87,451.

Legion Field is home for the UAB Blazers football program and the Papajohns.com Bowl. It seats 80,601.  Ladd-Peebles Stadium in Mobile is the home of the University of South Alabama football team, and serves as the home of the NCAA Senior Bowl, GoDaddy.com Bowl, and Alabama-Mississippi All Star Classic; the stadium seats 40,646.  In 2009, Bryant-Denny Stadium and Jordan-Hare Stadium became the homes of the Alabama High School Athletic Association state football championship games, after previously being held at Legion Field in Birmingham.

Alabama has several professional and semi-professional sports teams, including four minor league baseball teams.
The Talladega Superspeedway motorsports complex hosts a series of NASCAR events. It has a seating capacity of 143,000 and is the thirteenth largest stadium in the world and sixth largest stadium in America. Also, the Barber Motorsports Park has hosted IndyCar Series and Rolex Sports Car Series races.

The ATP Birmingham was a World Championship Tennis tournament held from 1973 to 1980.

Alabama has hosted several professional golf tournaments, such as the 1984 and 1990 PGA Championship at Shoal Creek (PGA Tour), the Mobile LPGA Tournament of Champions, Airbus LPGA Classic and Yokohama Tire LPGA Classic (LPGA Tour), and The Tradition (Champions Tour).

Major airports with sustained commercial operations in Alabama include Birmingham-Shuttlesworth International Airport (BHM), Huntsville International Airport (HSV), Dothan Regional Airport (DHN), Mobile Regional Airport (MOB), Montgomery Regional Airport (MGM), and Muscle Shoals â€“ Northwest Alabama Regional Airport (MSL).

For rail transport, Amtrak schedules the Crescent, a daily passenger train, running from New York to New Orleans with stops at Anniston, Birmingham, and Tuscaloosa.

Alabama has five major interstate roads that cross the state: Interstate65 (I-65) travels northâ€“south roughly through the middle of the state; I-20/I-59 travel from the central west Mississippi state line to Birmingham, where I-59 continues to the north-east corner of the state and I-20 continues east towards Atlanta; I-85 originates in Montgomery and travels east-northeast to the Georgia state line, providing a main thoroughfare to Atlanta; and I-10 traverses the southernmost portion of the state, traveling from west to east through Mobile. Another interstate, I-22, is currently under construction. When completed, it will connect Birmingham with Memphis, Tennessee. In addition, there are currently five auxiliary interstate routes in the state: I-165 in Mobile, I-359 in Tuscaloosa, I-459 around Birmingham, I-565 in Decatur and Huntsville, and I-759 in Gadsden. A sixth route, I-685, will be formed when I-85 is rerouted along a new southern bypass of Montgomery. A proposed northern bypass of Birmingham will be designated as I-422. Since a direct connection from I-22 to I-422 will not be possible, I-222 has been proposed, as well.

Several U.S. Highways also pass through the state, such as U.S. Route11 (US-11), US-29, US-31, US-43, US-45, US-72, US-78, US-80, US-82, US-84, US-90, US-98, US-231, US-278, US-280, US-331, US-411, and US-431.

There are four toll roads in the state: Montgomery Expressway in Montgomery; Tuscaloosa Bypass in Tuscaloosa; Emerald Mountain Expressway in Wetumpka; and Beach Express in Orange Beach.

The Port of Mobile, Alabama's only saltwater port, is a large seaport on the Gulf of Mexico with inland waterway access to the Midwest by way of the Tennessee-Tombigbee Waterway. The Port of Mobile was ranked 12th by tons of traffic in the United States during 2009.  The newly expanded container terminal at the Port of Mobile was ranked as the 25th busiest for container traffic in the nation during 2011.  The state's other ports are on rivers with access to the Gulf of Mexico.

Water ports of Alabama, listed from north to south:

Outline of Alabama â€“ organized list of topics about Alabama
Index of Alabama-related articles

For a detailed bibliography, see the History of Alabama.
Atkins, Leah Rawls, Wayne Flynt, William Warren Rogers, and David Ward. Alabama: The History of a Deep South State (1994)
Flynt, Wayne. Alabama in the Twentieth Century (2004)
Owen Thomas M. History of Alabama and Dictionary of Alabama Biography 4 vols. 1921.
Jackson, Harvey H. Inside Alabama: A Personal History of My State (2004)
Mohl, Raymond A. "Latinization in the Heart of Dixie: Hispanics in Late-twentieth-century Alabama" Alabama Review 2002 55(4): 243â€“274. ISSN 0002-4341
Peirce, Neal R. The Deep South States of America: People, Politics, and Power in the Seven Deep South States (1974). Information on politics and economics 1960â€“72.
Williams, Benjamin Buford. A Literary History of Alabama: The Nineteenth Century 1979.
WPA. Guide to Alabama (1939)

 â€“ Official State Government Website

, at the Alabama Department of Archives and History
 â€“ Alabama Department of Tourism and Travel
, a digital repository of materials on Alabama's history, culture, places, and people
 â€“ at the Alabama Legislature site

 from the U.S. Census Bureau
 from the U.S. Department of Agriculture


In Greek mythology, Achilles (; , Akhilleus, ) was a Greek hero of the Trojan War and the central character and greatest warrior of Homer's Iliad. His mother was the nymph Thetis, and his father, Peleus, was the king of the Myrmidons.

Achillesâ€™ most notable feat during the Trojan War was the slaying of the Trojan hero Hector outside the gates of Troy. Although the death of Achilles is not presented in the Iliad, other sources concur that he was killed near the end of the Trojan War by Paris, who shot him in the heel with an arrow. Later legends (beginning with a poem by Statius in the 1st century AD) state that Achilles was invulnerable in all of his body except for his heel. Because of his death from a small wound in the heel, the term Achilles' heel has come to mean a person's point of weakness.

Achilles' name can be analyzed as a combination of  (akhos) "grief" and  (laos) "a people, tribe, nation." In other words, Achilles is an embodiment of the grief of the people, grief being a theme raised numerous times in the Iliad (frequently by Achilles). Achilles' role as the hero of grief forms an ironic juxtaposition with the conventional view of Achilles as the hero of ÎºÎ»Î­Î¿Ï‚ kleos ("glory", usually glory in war).

Laos has been construed by Gregory Nagy, following Leonard Palmer, to mean "a corps of soldiers", a muster. With this derivation, the name would have a double meaning in the poem: when the hero is functioning rightly, his men bring grief to the enemy, but when wrongly, his men get the grief of war. The poem is in part about the misdirection of anger on the part of leadership.

R. S. P. Beekes has suggested a Pre-Greek origin of the name.

The name Achilleus was a common and attested name among the Greeks soon after the 7th century BC. It was also turned into the female form á¼ˆÏ‡Î¹Î»Î»ÎµÎ¯Î± (AchilleÃ­a) attested in Attica in the 4th century BC (IG IIÂ² 1617) and, in the form Achillia, on a stele in Halicarnassus as the name of a female gladiator fighting an "Amazon".

Achilles was the son of the Nereid Thetis and Peleus, the king of the Myrmidons. Zeus and Poseidon had been rivals for the hand of Thetis until Prometheus, the fore-thinker, warned Zeus of a prophecy that Thetis would bear a son greater than his father. For this reason, the two gods withdrew their pursuit, and had her wed Peleus.

There is a tale which offers an alternative version of these events: in Argonautica (iv.760) Zeus' sister and wife Hera alludes to Thetis' chaste resistance to the advances of Zeus, that Thetis was so loyal to Hera's marriage bond that she coolly rejected him. Thetis, although a daughter of the sea-god Nereus, was also brought up by Hera, further explaining her resistance to the advances of Zeus.
According to the Achilleid, written by Statius in the 1st century AD, and to no surviving previous sources, when Achilles was born Thetis tried to make him immortal, by dipping him in the river Styx. However, he was left vulnerable at the part of the body by which she held him, his heel (see Achilles heel, Achilles' tendon). It is not clear if this version of events was known earlier. In another version of this story, Thetis anointed the boy in ambrosia and put him on top of a fire, to burn away the mortal parts of his body. She was interrupted by Peleus and abandoned both father and son in a rage.

However, none of the sources before Statius makes any reference to this general invulnerability. To the contrary, in the Iliad Homer mentions Achilles being wounded: in Book 21 the Paeonian hero Asteropaeus, son of Pelagon, challenged Achilles by the river Scamander. He cast two spears at once, one grazed Achilles' elbow, "drawing a spurt of blood".

Also, in the fragmentary poems of the Epic Cycle in which we can find description of the hero's death, Cypria (unknown author), Aithiopis by Arctinus of Miletus, Little Iliad by Lesche of Mytilene, Iliou persis by Arctinus of Miletus, there is no trace of any reference to his general invulnerability or his famous weakness (heel); in the later vase paintings presenting Achilles' death, the arrow (or in many cases, arrows) hit his body.

Peleus entrusted Achilles to Chiron the Centaur, on Mt. Pelion, to be reared.

The first two lines of the Iliad read:
Sing, Goddess, of the rage of Peleus' son Achilles,
the accursed rage that brought great suffering to the Achaeans.

Achilles' consuming rage is at times wavering, but at other times he cannot be cooled. The humanization of Achilles by the events of the war is an important theme of the narrative.

According to the Iliad, Achilles arrived at Troy with 50 ships, each carrying 50 Myrmidons (Book 2). He appointed five leaders (each leader commanding 500 Myrmidons): Menesthius, Eudorus, Peisander, Phoenix and Alcimedon (Book 16).

When the Greeks left for the Trojan War, they accidentally stopped in Mysia, ruled by King Telephus. In the resulting battle, Achilles gave Telephus a wound that would not heal; Telephus consulted an oracle, who stated that "he that wounded shall heal". Guided by the oracle, he arrived at Argos, where Achilles healed him in order that he might become their guide for the voyage to Troy. 

According to other reports in Euripides' lost play about Telephus, he went to Aulis pretending to be a beggar and asked Achilles to heal his wound. Achilles refused, claiming to have no medical knowledge. Alternatively, Telephus held Orestes for ransom, the ransom being Achilles' aid in healing the wound. Odysseus reasoned that the spear had inflicted the wound; therefore, the spear must be able to heal it. Pieces of the spear were scraped off onto the wound and Telephus was healed. 

According to the Cypria (the part of the Epic Cycle that tells the events of the Trojan War before Achilles' Wrath), when the Achaeans desired to return home, they were restrained by Achilles, who afterwards attacked the cattle of Aeneas, sacked neighboring cities and killed Troilus.

In Dares Phrygius' Account of the Destruction of Troy, the Latin summary through which the story of Achilles was transmitted to medieval Europe, Troilus was a young Trojan prince, the youngest of King Priam's (or sometimes Apollo) and Hecuba's five legitimate sons. Despite his youth, he was one of the main Trojan war leaders. Prophecies linked Troilus' fate to that of Troy and so he was ambushed in an attempt to capture him. Yet Achilles, struck by the beauty of both Troilus and his sister Polyxena, and overcome with lust, directed his sexual attentions on the youthâ€“ who refusing to yield found instead himself decapitated upon an altar-omphalos of Apollo. Later versions of the story suggested Troilus was accidentally killed by Achilles in an over-ardent lovers' embrace. In this version of the myth, Achilles' death therefore came in retribution for this sacrilege. Ancient writers treated Troilus as the epitome of a dead child mourned by his parents. Had Troilus lived to adulthood, the First Vatican Mythographer claimed, Troy would have been invincible.

Homer's Iliad is the most famous narrative of Achilles' deeds in the Trojan War. Achilles' wrath is the central theme of the poem. The Homeric epic only covers a few weeks of the war, and does not narrate Achilles' death. It begins with Achilles' withdrawal from battle after he is dishonored by Agamemnon, the commander of the Achaean forces. Agamemnon had taken a woman named Chryseis as his slave. Her father Chryses, a priest of Apollo, begs Agamemnon to return her to him. Agamemnon refuses and Apollo sends a plague amongst the Greeks. The prophet Calchas correctly determines the source of the troubles but will not speak unless Achilles vows to protect him. Achilles does so and Calchas declares Chryseis must be returned to her father. Agamemnon consents, but then commands that Achilles' battle prize Briseis be brought to him to replace Chryseis. Angry at the dishonor of having his plunder and glory taken away (and as he says later, because he loved Briseis), with the urging of his mother Thetis, Achilles refuses to fight or lead his troops alongside the other Greek forces. At this same time, burning with rage over Agamemnon's theft, Achilles prays to Thetis to convince Zeus to help the Trojans gain ground in the war, so that he may regain his honor.

As the battle turns against the Greeks, thanks to the influence of Zeus, Nestor declares that the Trojans are winning because Agamemnon has angered Achilles, and urges the king to appease the warrior. Agamemnon agrees and sends Odysseus and two other chieftains, Ajax and Phoenix, to Achilles with the offer of the return of Briseis and other gifts. Achilles rejects all Agamemnon offers him, and simply urges the Greeks to sail home as he was planning to do.

The Trojans, led by Hector, subsequently push the Greek army back toward the beaches and assault the Greek ships. With the Greek forces on the verge of absolute destruction, Patroclus leads the Myrmidons into battle wearing Achilles' armor, though Achilles remains at his camp. Patroclus succeeds in pushing the Trojans back from the beaches, but is killed by Hector before he can lead a proper assault on the city of Troy.
After receiving the news of the death of Patroclus from Antilochus, the son of Nestor, Achilles grieves over his beloved companion's death and holds many funeral games in his honor. His mother Thetis comes to comfort the distraught Achilles. She persuades Hephaestus to make new armor for him, in place of the armor that Patroclus had been wearing which was taken by Hector. The new armor includes the Shield of Achilles, described in great detail in the poem.

Enraged over the death of Patroclus, Achilles ends his refusal to fight and takes the field killing many men in his rage but always seeking out Hector. Achilles even engages in battle with the river god Scamander who becomes angry that Achilles is choking his waters with all the men he has killed. The god tries to drown Achilles but is stopped by Hera and Hephaestus. Zeus himself takes note of Achilles' rage and sends the gods to restrain him so that he will not go on to sack Troy itself before the time allotted for its destruction, seeming to show that the unhindered rage of Achilles can defy fate itself. Finally, Achilles finds his prey. Achilles chases Hector around the wall of Troy three times before Athena, in the form of Hector's favorite and dearest brother, Deiphobus, persuades Hector to stop running and fight Achilles face to face. After Hector realizes the trick, he knows the battle is inevitable. Wanting to go down fighting, he charges at Achilles with his only weapon, his sword, but misses. Accepting his fate, Hector begs Achilles, not to spare his life, but to treat his body with respect after killing him. Achilles tells Hector it is hopeless to expect that of him, declaring that "my rage, my fury would drive me now to hack your flesh away and eat you raw â€“ such agonies you have caused me". Achilles then kills Hector and drags his corpse by its heels behind his chariot during Patroclus' funeral games.

With the assistance of the god Hermes, Hector's father, Priam, goes to Achilles' tent to plead with Achilles for the return of Hector's body so that he can be buried. Achilles relents and promises a truce for the duration of the funeral. The poem ends with a description of Hector's funeral, with the doom of Troy and Achilles himself still to come.

Achilles, after his temporary truce with Priam, fought and killed the Amazonian warrior queen Penthesilea, but later grieved over her death. At first, he was so distracted by her beauty, he did not fight as intensely as usual. Once he realized that his distraction was endangering his life, he refocused and killed her.

Following the death of Patroclus, Achilles' closest companion was Nestor's son Antilochus. When Memnon, king of Ethiopia slew Antilochus, Achilles once more obtained revenge on the battlefield, killing Memnon. The fight between Achilles and Memnon over Antilochus echoes that of Achilles and Hector over Patroclus, except that Memnon (unlike Hector) was also the son of a goddess.

Many Homeric scholars argued that episode inspired many details in the Iliads description of the death of Patroclus and Achilles' reaction to it. The episode then formed the basis of the cyclic epic Aethiopis, which was composed after the Iliad, possibly in the 7th century B.C. The Aethiopis is now lost, except for scattered fragments quoted by later authors.
The death of Achilles, as predicted by Hector with his dying breath, was brought about by Paris with an arrow (to the heel according to Statius). In some versions, the god Apollo guided Paris' arrow. Some retellings also state that Achilles was scaling the gates of Troy and was hit with a poisoned arrow.
All of these versions deny Paris any sort of valor, owing to the common conception that Paris was a coward and not the man his brother Hector was, and Achilles remained undefeated on the battlefield. His bones were mingled with those of Patroclus, and funeral games were held. He was represented in the Aethiopis as living after his death in the island of Leuke at the mouth of the river Danube.

Another version of Achilles' death is that he fell deeply in love with one of the Trojan princesses, Polyxena. Achilles asks Priam for Polyxena's hand in marriage. Priam is willing because it would mean the end of the war and an alliance with the world's greatest warrior. But while Priam is overseeing the private marriage of Polyxena and Achilles, Paris, who would have to give up Helen if Achilles married his sister, hides in the bushes and shoots Achilles with a divine arrow, killing him.

Achilles was cremated and his ashes buried in the same urn as those of Patroclus.

Paris was later killed by Philoctetes using the enormous bow of Heracles.

Achilles' armor was the object of a feud between Odysseus and Telamonian Ajax (Ajax the greater). They competed for it by giving speeches on why they were the bravest after Achilles to their Trojan prisoners, who after considering both men came to a consensus in favor of Odysseus. Furious, Ajax cursed Odysseus, which earned the ire of Athena. Athena temporarily made Ajax so mad with grief and anguish that he began killing sheep, thinking them his comrades. After a while, when Athena lifted his madness and Ajax realized that he had actually been killing sheep, Ajax was left so ashamed that he committed suicide. Odysseus eventually gave the armor to Neoptolemus, the son of Achilles.

A relic claimed to be Achilles' bronze-headed spear was for centuries preserved in the temple of Athena on the acropolis of Phaselis, Lycia, a port on the Pamphylian Gulf. The city was visited in 333 BC by Alexander the Great, who envisioned himself as the new Achilles and carried the Iliad with him, but his court biographers do not mention the spear. However, it was shown in the time of Pausanias in the 2nd century AD.

Numerous paintings on pottery have suggested a tale not mentioned in the literary traditions. At some point in the war, Achilles and Ajax were playing a board game (petteia). They were absorbed in the game and oblivious to the surrounding battle. The Trojans attacked and reached the heroes, who were saved only by an intervention of Athena.

The exact nature of Achilles' relationship with Patroclus has been a subject of dispute in both the classical period and modern times. In the Iliad, it appears to be the model of a deep and loyal friendship. Homer does not suggest that Achilles and his close friend Patroclus were lovers. Despite there being no direct evidence in the text of the Iliad that Achilles and Patroclus were lovers, this theory was expressed by some later authors. Commentators from classical antiquity to the present have often interpreted the relationship through the lens of their own cultures. In 5th-century BC Athens, the intense bond was often viewed in light of the Greek custom of paiderasteia. In Plato's Symposium, the participants in a dialogue about love assume that Achilles and Patroclus were a couple; Phaedrus argues that Achilles was the younger and more beautiful one so he was the beloved and Patroclus was the lover. But ancient Greek had no words to distinguish heterosexual and homosexual, and it was assumed that a man could both desire handsome young men and have sex with women.

There was an archaic heroic cult of Achilles on the White Island, Leuce, in the Black Sea off the modern coasts of Romania and Ukraine, with a temple and an oracle which survived into the Roman period.

In the lost epic  Aithiopis, a continuation of the Iliad attributed to Arktinus of Miletos, Achillesâ€™ mother Thetis returned to mourn him and removed his ashes from the pyre and took them to Leuce at the mouths of the Danube. There the Achaeans raised a tumulus for him and celebrated funeral games.

Pliny's Natural History mentions a tumulus that is no longer evident (Insula Akchillis tumulo eius viri clara), on the island consecrated to him, located at a distance of fifty Roman miles from Peuce by the Danube Delta, and the temple there. Pausanias has been told that the island is "covered with forests and full of animals, some wild, some tame. In this island there is also Achillesâ€™ temple and his statue". Ruins of a square temple 30 meters to a side, possibly that dedicated to Achilles, were discovered by Captain Kritzikly in 1823, but there has been no modern archeological work done on the island.

Pomponius Mela tells that Achilles is buried in the island named Achillea, between Boristhene and Ister. The Greek geographer Dionysius Periegetus of Bithynia, who lived at the time of Domitian, writes that the island was called Leuce "because the wild animals which live there are white. It is said that there, in Leuce island, reside the souls of Achilles and other heroes, and that they wander through the uninhabited valleys of this island; this is how Jove rewarded the men who had distinguished themselves through their virtues, because through virtue they had acquired everlasting honor".

The Periplus of the Euxine Sea gives the following details: "It is said that the goddess Thetis raised this island from the sea, for her son Achilles, who dwells there. Here is his temple and his statue, an archaic work. This island is not inhabited, and goats graze on it, not many, which the people who happen to arrive here with their ships, sacrifice to Achilles. In this temple are also deposited a great many holy gifts, craters, rings and precious stones, offered to Achilles in gratitude. One can still read inscriptions in Greek and Latin, in which Achilles is praised and celebrated. Some of these are worded in Patroclusâ€™ honor, because those who wish to be favored by Achilles, honor Patroclus at the same time. There are also in this island countless numbers of sea birds, which look after Achillesâ€™ temple. Every morning they fly out to sea, wet their wings with water, and return quickly to the temple and sprinkle it. And after they finish the sprinkling, they clean the hearth of the temple with their wings. Other people say still more, that some of the men who reach this island, come here intentionally. They bring animals in their ships, destined to be sacrificed. Some of these animals they slaughter, others they set free on the island, in Achillesâ€™ honor. But there are others, who are forced to come to this island by sea storms. As they have no sacrificial animals, but wish to get them from the god of the island himself, they consult Achillesâ€™ oracle. They ask permission to slaughter the victims chosen from among the animals that graze freely on the island, and to deposit in exchange the price which they consider fair. But in case the oracle denies them permission, because there is an oracle here, they add something to the price offered, and if the oracle refuses again, they add something more, until at last, the oracle agrees that the price is sufficient. And then the victim doesnâ€™t run away any more, but waits willingly to be caught. So, there is a great quantity of silver there, consecrated to the hero, as price for the sacrificial victims. To some of the people who come to this island, Achilles appears in dreams, to others he would appear even during their navigation, if they were not too far away, and would instruct them as to which part of the island they would better anchor their ships". (quoted in DensuÅŸianu)

The heroic cult of Achilles on Leuce island was widespread in antiquity, not only along the sea lanes of the Pontic Sea but also in maritime cities whose economic interests were tightly connected to the riches of the Black Sea.

Achilles from Leuce island was venerated as Pontarches the lord and master of the Pontic Sea, the protector of sailors and navigation. Sailors went out of their way to offer sacrifice. To Achilles of Leuce were dedicated a number of important commercial port cities of the Greek waters: Achilleion in Messenia (Stephanus Byzantinus), Achilleios in Laconia (Pausanias, III.25,4) Nicolae DensuÅŸianu (DensuÅŸianu 1913) even though he recognized Achilles in the name of Aquileia and in the north arm of the Danube delta, the arm of Chilia ("Achileii"), though his conclusion, that Leuce had sovereign rights over Pontos, evokes modern rather than archaic sea-law."

Leuce had also a reputation as a place of healing. Pausanias (III.19,13) reports that the Delphic Pythia sent a lord of Croton to be cured of a chest wound. Ammianus Marcellinus (XXII.8) attributes the healing to waters (aquae) on the island.

In the region of Gastouri (Î“Î±ÏƒÏ„Î¿ÏÏÎ¹) to the south of the city of Corfu Greece, Empress of Austria Elisabeth of Bavaria also known as Sissi built in 1890 a summer palace with Achilles as its central theme and it is a monument to platonic romanticism. The palace, naturally, was named after Achilles: Achilleion (Î‘Ï‡Î¯Î»Î»ÎµÎ¹Î¿Î½). This elegant structure abounds with paintings and statues of Achilles both in the main hall and in the lavish gardens depicting the heroic and tragic scenes of the Trojan war.

Some post-Homeric sources claim that in order to keep Achilles safe from the war, Thetis (or, in some versions, Peleus) hides the young man at the court of Lycomedes, king of Skyros. There, Achilles is disguised as a girl and lives among Lycomedes' daughters, perhaps under the name "Pyrrha" (the red-haired girl). With Lycomedes' daughter Deidamia, whom in the account of Statius he rapes, Achilles there fathers a son, Neoptolemus (also called Pyrrhus, after his father's possible alias). According to this story, Odysseus learns from the prophet Calchas that the Achaeans would be unable to capture Troy without Achilles' aid. Odysseus goes to Skyros in the guise of a peddler selling women's clothes and jewelry and places a shield and spear among his goods. When Achilles instantly takes up the spear, Odysseus sees through his disguise and convinces him to join the Greek campaign. In another version of the story, Odysseus arranges for a trumpet alarm to be sounded while he was with Lycomedes' women; while the women flee in panic, Achilles prepares to defend the court, thus giving his identity away.

In book 11 of Homer's Odyssey, Odysseus sails to the underworld and converses with the shades. One of these is Achilles, who when greeted as "blessed in life, blessed in death", responds that he would rather be a slave to the worst of masters than be king of all the dead. But Achilles then asks Odysseus of his son's exploits in the Trojan war, and when Odysseus tells of Neoptolemus' heroic actions, Achilles is filled with satisfaction. This leaves the reader with an ambiguous understanding of how Achilles felt about the heroic life. Achilles was worshipped as a sea-god in many of the Greek colonies on the Black Sea, the location of the mythical "White Island" which he was said to inhabit after his death, together with many other heroes.

The kings of the Epirus claimed to be descended from Achilles through his son, Neoptolemus. Alexander the Great, son of the Epirote princess Olympias, could therefore also claim this descent, and in many ways strove to be like his great ancestor. He is said to have visited the tomb of Achilles at Achilleion while passing Troy. In AD 216 the Roman Emperor Caracalla, while on his way to war against Parthia, emulated Alexander by holding games around Achilles' tumulus.

Achilles fought and killed the Amazon Helene. Some also said he married Medea, and that after both their deaths they were united in the Elysian Fields of Hadesâ€“ as Hera promised Thetis in Apollonius' Argonautica. In some versions of the myth, Achilles has a relationship with his captive Briseis.

The Greek tragedian Aeschylus wrote a trilogy of plays about Achilles, given the title Achilleis by modern scholars. The tragedies relate the deeds of Achilles during the Trojan War, including his defeat of Hector and eventual death when an arrow shot by Paris and guided by Apollo punctures his heel. Extant fragments of the Achilleis and other Aeschylean fragments have been assembled to produce a workable modern play. The first part of the Achilleis trilogy, The Myrmidons, focused on the relationship between Achilles and chorus, who represent the Achaean army and try to convince Achilles to give up his quarrel with Agamemnon; only a few lines survive today. In Plato's Symposium, Phaedrus points out that Aeschylus portrayed Achilles as the lover and Patroclus as the beloved; Phaedrus argues that this is incorrect because Achilles, being the younger and more beautiful of the two, was the beloved, who loved his lover so much that he chose to die to revenge him.

The tragedian Sophocles also wrote The Lovers of Achilles, a play with Achilles as the main character. Only a few fragments survive.

Towards the end of the 5th century BC, a more negative view of Achilles emerges in Greek drama; Euripides refers to Achilles in a bitter or ironic tone in Hecuba, Electra, and Iphigenia in Aulis.

The philosopher Zeno of Elea centered one of his paradoxes on an imaginary footrace between "swift-footed" Achilles and a tortoise, by which he attempted to show that Achilles could not catch up to a tortoise with a head start, and therefore that motion and change were impossible. As a student of the monist Parmenides and a member of the Eleatic school, Zeno believed time and motion to be illusions.

The Romans, who traditionally traced their lineage to Troy, took a highly negative view of Achilles. Vergil refers to Achilles as a savage and a merciless butcher of men, while Horace portrays Achilles ruthlessly slaying women and children. Other writers, such as Catullus, Propertius, and Ovid, represent a second strand of disparagement, with an emphasis on Achilles' erotic career. This strand continues in Latin accounts of the Trojan War by writers such as Dictys Cretensis and Dares Phrygius and in BenoÃ®t de Sainte-Maure's Roman de Troie and Guido delle Colonne's Historia destructionis Troiae, which remained the most widely read and retold versions of the Matter of Troy until the 17th century.

Achilles is portrayed as a former hero who has become lazy and devoted to the love of Patroclus, in William Shakespeare's Troilus and Cressida.

Achilles appears in Dante's Inferno. He is seen in Hell's Circle of Lust.

Achilles is the subject of the poem AchilleÃ¯s, a fragment by Johann Wolfgang von Goethe.

Achilles is a major character in Madeline Miller's debut novel, The Song of Achilles (2011), which won the 2012 Orange Prize for Fiction. The novel explores the relationship between Patroclus and Achilles from boyhood to the fateful events of the Iliad.

Achilles is a central and playable character in KOEI's .

Achilles is mentioned in Tennyson's "Ulysses": "...we shall touch the happy isles and meet there the great Achilles whom we knew."

Achilles (Akhilles) is killed by a poisoned Kentaur arrow shot by Kassandra in Marion Zimmer Bradley's novel The Firebrand (1987). 

In Disney's Hercules, Achilles was mentioned to have been a student of Philoctetes and a reference to his heel being his weakness was made. Later in the film, some people of Thebes mock Philoctetes for his training of Achilles.
Achilles appears in Hercules episode "Achilles and the Living Legend," voiced by Dom Irrera. He is shown as a washed-out has-been ever since he was defeated.

Achilles is one of various 'narrators' in Colleen McCullough's novel The Song of Troy (1998). 

Achilles is the main character in David Malouf's novel Ransom (2009).

The ghost of Achilles appears in Rick Riordan's The Last Olympian (2009). He warns Percy Jackson about the Curse of Achilles and its side effects.

The role of Achilles has been played in film by:

Piero Lulli in Ulysses (1955)

Stanley Baker in Helen of Troy (1956)

Riley Ottenhof in Something about Zeus (1958)

Arturo Dominici in La Guerra di Troia (1962)

Gordon Mitchell in The Fury of Achilles (1962)

Steve Davislim in La Belle HÃ©lÃ¨ne (TV, 1996)

Richard Trewett in the miniseries The Odyssey (TV, 1997)

Joe Montana in Helen of Troy (TV, 2003)

Brad Pitt in Troy (2004)

Achilles has frequently been mentioned in music:

Achilles is a hardcore band.

"Achilles" is an oratorio by German composer Max Bruch (1885).

"Achilles, Agony & Ecstasy In Eight Parts", by Manowar (The Triumph of Steel, 1992).

Achilles Heel is an album by Pedro the Lion.

"Achilles Last Stand" is a song by Led Zeppelin (Presence, 1976).

"Achilles' Revenge" is a song by Warlord.

"Achilles' Wrath" is a concert piece by Sean O'Loughlin.

Achilles is referred to in Bob Dylan's song "Temporary Like Achilles".

"Cry of Achilles" is the lead track off of Alter Bridge's fourth album, Fortress.

The name of Achilles has been used for at least nine Royal Navy warships since 1744 - both as HMS Achilles and with the French spelling HMS Achille. A 60-gun ship of that name served at the Battle of Belleisle in 1761 while a 74-gun ship served at the Battle of Trafalgar. Other battle honours include Walcheren 1809. An armored cruiser of that name served in the Royal Navy during the First World War.
HMNZS Achilles was a Leander-class cruiser which served with the Royal New Zealand Navy in World War II. It became famous for its part in the Battle of the River Plate, alongside  and . In addition to earning the battle honour 'River Plate', HMNZS Achilles also served at Guadalcanal 1942â€“43 and Okinawa in 1945. After returning to the Royal Navy, the ship was sold to the Indian Navy in 1948 but when she was scrapped parts of the ship were saved and preserved in New Zealand.
Capois La Mort, a slave who fought in the Haitian Revolution, was nicknamed the Black Achilles because of his heroic performance during the last battle against the French.
Prince Achileas-Andreas of Greece and Denmark was the grandson of the deposed Greek king, Constantine II.
The character Achilles in Ender's Shadow, by Orson Scott Card, shares his namesake's cunning mind and ruthless attitude.
In the Star Trek universe, the Achilles Class is an advanced type of Federation battleship brought into service at the outbreak of the Dominion War, though not seen in any of the canon Star Trek TV series.
Achilles armor and valor are included in the video games Titan Quest and TQ Immortal Throne.
The 2005 video game Spartan Total Warrior features two campaign missions located in the fictional buried city of Troy, with the story arc for this segment of the game culminating in the discovery of the Tomb of Achilles and the acquisition of the Spear of Achilles.

Homer, Iliad
Homer, Odyssey XI, 467â€“540
Pseudo-Apollodorus, Bibliotheca III, xiii, 5â€“8
Apollodorus, Epitome III, 14-V, 7
Ovid, Metamorphoses XI, 217â€“265; XII, 580-XIII, 398
Ovid, Heroides III
Apollonius Rhodius, Argonautica IV, 783â€“879
Dante Alighieri, The Divine Comedy, Inferno, V.

Ileana Chirassi Colombo, "Heroes Achilleusâ€”Theos Apollon." In Il Mito Greco, ed. Bruno Gentili & Giuseppe Paione, Rome, 1977;
Anthony Edwards:
"Achilles in the Underworld: Iliad, Odyssey, and Ã†thiopis", Greek, Roman, and Byzantine Studies, 26 (1985): pp.215â€“227 ;
"Achilles in the Odyssey: Ideologies of Heroism in the Homeric Epic", BeitrÃ¤ge zur klassischen Philologie, 171, Meisenheim, 1985;
"Kleos Aphthiton and Oral Theory," Classical Quarterly, 38 (1988): pp.25â€“30;
HÃ©lÃ¨ne MonsacrÃ©, Les larmes d'Achille. Le hÃ©ros, la femme et la souffrance dans la poÃ©sie d'HomÃ¨re, Paris, Albin Michel, 1984
Gregory Nagy:
The Best of The Acheans: Concepts of the Hero in Archaic Greek Poetry, Johns Hopkins University, 1999 (rev. edition);
The Name of Achilles: Questions of Etymology and 'Folk Etymology, Illinois Classical Studies, 19, 1994;
Dale S. Sinos, The Entry of Achilles into Greek Epic, Ph.D. thesis, Johns Hopkins University;
Jonathan S. Burgess, The Death and Afterlife of Achilles (Baltimore: Johns Hopkins University Press, 2009).



Abraham Lincoln  (February 12, 1809 â€“ April 15, 1865) was the 16th President of the United States, serving from March 1861 until his assassination in April 1865. Lincoln led the United States through its Civil Warâ€”its bloodiest war and its greatest moral, constitutional and political crisis. In doing so, he preserved the Union, abolished slavery, strengthened the federal government, and modernized the economy.

Born in Hodgenville, Kentucky, Lincoln grew up on the western frontier in Kentucky and Indiana. Largely self-educated, he became a lawyer in Illinois, a Whig Party leader, and a member of the Illinois House of Representatives, where he served from 1834 to 1846. Elected to the United States House of Representatives in 1846, Lincoln promoted rapid modernization of the economy through banks, tariffs, and railroads. Because he had originally agreed not to run for a second term in Congress, and because his opposition to the Mexicanâ€“American War was unpopular among Illinois voters, Lincoln returned to Springfield and resumed his successful law practice. Reentering politics in 1854, he became a leader in building the new Republican Party, which had a statewide majority in Illinois. In 1858, while taking part in a series of highly publicized debates with his opponent and rival, Democrat Stephen A. Douglas, Lincoln spoke out against the expansion of slavery, but lost the U.S. Senate race to Douglas.

In 1860 Lincoln secured the Republican Party presidential nomination as a moderate from a swing state. With very little support in the slaveholding states of the South, he swept the North and was elected president in 1860. His election prompted seven southern slave states to form the Confederate States of America before he was sworn into office. No compromise or reconciliation was found regarding slavery and secession.

After the Confederates attacked Fort Sumter on April 12, 1861, the North enthusiastically rallied behind the Union. Lincoln concentrated on the military and political dimensions of the war. His primary goal was to reunite the nation. He suspended habeas corpus, leading to the controversial ex parte Merryman decision. Lincoln averted potential British intervention in the war by defusing the Trent Affair in late 1861. His complex moves toward ending slavery centered on the Emancipation Proclamation of 1863. Lincoln used the U.S. Army to protect escaped slaves, encouraged the border states to outlaw slavery, and helped push through Congress the Thirteenth Amendment to the United States Constitution, which permanently outlawed slavery. Lincoln closely supervised the war effort, especially the selection of top generals, including his most successful general, Ulysses S. Grant. He also made major decisions on Union war strategy; for example: a naval blockade that shut down the South's normal trade; moves to take control of Kentucky and Tennessee; and using gunboats to gain control of the southern river system. Lincoln tried repeatedly to capture the Confederate capital at Richmond; each time a general failed, Lincoln substituted another, until finally Grant succeeded in 1865.

An exceptionally astute politician deeply involved with power issues in each state, Lincoln reached out to "War Democrats" (those who supported the North against the South), and managed his own re-election campaign in the 1864 presidential election. As the leader of the moderate faction of the Republican Party, Lincoln confronted Radical Republicans, who demanded harsher treatment of the South, War Democrats, who called for more compromise, anti-war Democrats (called Copperheads), who despised him, and irreconcilable secessionists, who plotted his assassination. Politically, Lincoln fought back by pitting his opponents against each other, by appealing to the American people with his powers of oratory, and by carefully planned political patronage. His Gettysburg Address of 1863 became an iconic endorsement of the principles of nationalism, republicanism, equal rights, liberty, and democracy. Lincoln held a moderate view of Reconstruction, seeking to reunite the nation speedily through a policy of generous reconciliation in the face of lingering and bitter divisiveness. Six days after the surrender of Confederate commanding general Robert E. Lee, Lincoln was assassinated by John Wilkes Booth, a Confederate sympathizer.

Lincoln has been consistently ranked both by scholars and the public as one of the three greatest U.S. presidents.



Abraham Lincoln was born February 12, 1809, the second child of Thomas and Nancy Hanks Lincoln, in a one-room log cabin on the Sinking Spring Farm in Hardin County, Kentucky (now LaRue County). He is a descendant of Samuel Lincoln, who migrated from Norfolk, England to Hingham, Massachusetts, in 1638. Samuel's grandson and great-grandson began the family's western migration, which passed through New Jersey, Pennsylvania, and Virginia. Lincoln's paternal grandfather and namesake, Captain Abraham Lincoln, moved the family from Virginia to Jefferson County, Kentucky in the 1780s. Captain Lincoln was killed in an Indian raid in 1786. His children, including six-year-old Thomas, the future president's father, witnessed the attack. After his father's murder, Thomas was left to make his own way on the frontier, working at odd jobs in Kentucky and in Tennessee, before settling with members of his family in Hardin County, Kentucky, in the early 1800s.

Lincoln's mother, Nancy, was the daughter of Lucy Shipley Hanks, and was born in what is now Mineral County, West Virginia, then part of Virginia. The identity of Lincoln's maternal grandfather is unclear. According to William Ensign Lincoln's book The Ancestry of Abraham Lincoln, Nancy was the daughter of Joseph Hanks; however, the debate continues over whether she was born out of wedlock. Lucy Hanks migrated to Kentucky with her daughter, Nancy. The two women resided with relatives in Washington County, Kentucky.

Thomas Lincoln and Nancy Hanks were married on June 12, 1806, in Washington County, and moved to Elizabethtown, Kentucky, following their marriage. They became the parents of three children: Sarah, born on February 10, 1807; Abraham, on February 12, 1809; and another son, Thomas, who died in infancy. Thomas Lincoln bought or leased several farms in Kentucky, including the Sinking Spring farm, where Abraham was born; however, a land title dispute soon forced the Lincolns to move. In 1811 the family relocated eight miles north, to Knob Creek Farm, where Thomas acquired title to  of land. In 1815 a claimant in another land dispute sought to eject the family from the farm. Of the 816.5 acres that Thomas held in Kentucky, he lost all but  of his land in court disputes over property titles. Frustrated over the lack of security provided by Kentucky courts, Thomas sold the remaining land he held in Kentucky in 1814, and began planning a move to Indiana, where the land survey process was more reliable and the ability for an individual to retain land titles was more secure.

In 1816 the family moved north across the Ohio River to Indiana, a free, non-slaveholding territory, where they settled in an "unbroken forest" in Hurricane Township, Perry County. (Their land in southern Indiana became part of Spencer County, Indiana, when the county was established in 1818.) The farm is preserved as part of the Lincoln Boyhood National Memorial. In 1860 Lincoln noted that the family's move to Indiana was "partly on account of slavery"; but mainly due to land title difficulties in Kentucky. During the family's years in Kentucky and Indiana, Thomas Lincoln worked as a farmer, cabinetmaker, and carpenter. He owned farms, several town lots and livestock, paid taxes, sat on juries, appraised estates, served on country slave patrols, and guarded prisoners. Thomas and Nancy Lincoln were also members of a Separate Baptists church, which had restrictive moral standards and opposed alcohol, dancing, and slavery. Within a year of the family's arrival in Indiana, Thomas claimed title to  of Indiana land. Despite some financial challenges he eventually obtained clear title to  of land in what became known as the Little Pigeon Creek community in Spencer County. Prior to the family's move to Illinois in 1830, Thomas had acquired an additional twenty acres of land adjacent to his property.
Several significant family events took place during Lincoln's youth in Indiana. On October 5, 1818, Nancy Lincoln died of milk sickness, leaving eleven-year-old Sarah in charge of a household that included her father, nine-year-old Abraham, and Dennis Hanks, Nancy's nineteen-year-old orphaned cousin. On December 2, 1819, Lincoln's father married  Sarah "Sally" Bush Johnston, a widow from Elizabethtown, Kentucky, with three children of her own. Abraham became very close to his stepmother, whom he referred to as "Mother". Those who knew Lincoln as a teenager later recalled him being very distraught over his sister Sarah's death on January 20, 1828, while giving birth to a stillborn son.

As a youth, Lincoln disliked the hard labor associated with frontier life. Some of his neighbors and family members thought for a time that he was lazy for all his "reading, scribbling, writing, ciphering, writing Poetry, etc.", and must have done it to avoid manual labor. His stepmother also acknowledged he did not enjoy "physical labor", but loved to read. Lincoln was largely self-educated. His formal schooling from several itinerant teachers was intermittent, the aggregate of which may have amounted to less than a year; however, he was an avid reader and retained a lifelong interest in learning. Family, neighbors, and schoolmates of Lincoln's youth recalled that he read and reread the King James Bible, Aesop's Fables, Bunyan's The Pilgrim's Progress, Defoe's Robinson Crusoe, Weems's The Life of Washington, and Franklin's Autobiography, among others.

As he grew into his teens, Lincoln took responsibility for the chores expected of him as one of the boys in the household. He also complied with the customary obligation of a son giving his father all earnings from work done outside the home until the age of twenty-one. Abraham became an adept as using an axe. Tall for his age, Lincoln was also strong and athletic. He attained a reputation for brawn and audacity after a very competitive wrestling match with the renowned leader of a group of ruffians known as "the Clary's Grove boys".

In early March 1830, fearing a milk sickness outbreak along the Ohio River, the Lincoln family moved west to Illinois, a non-slaveholding state. They settled on a site in Macon County, Illinois,  west of Decatur. Historians disagree on who initiated the move. After the family relocated to Illinois, Abraham became increasingly distant from his father, in part because of his father's lack of education, and occasionally lent him money. In 1831, as Thomas and other members of the family prepared to move to a new homestead in Coles County, Illinois, Abraham was old enough to make his own decisions and struck out on his own. Traveling down the Sangamon River, he ended up in the village of New Salem in Sangamon County. Later that spring, Denton Offutt, a New Salem merchant, hired Lincoln and some friends to take goods by flatboat from New Salem to New Orleans via the Sangamon, Illinois, and Mississippi rivers. After arriving in New Orleansâ€”and witnessing slavery firsthandâ€”Lincoln returned to New Salem, where he remained for the next six years.

Lincoln's first romantic interest was Ann Rutledge, whom he met when he first moved to New Salem; by 1835, they were in a relationship but not formally engaged. She died at the age of 22 on August 25, 1835, most likely of typhoid fever. In the early 1830s, he met Mary Owens from Kentucky when she was visiting her sister.

Late in 1836, Lincoln agreed to a match with Mary if she returned to New Salem. Mary did return in November 1836, and Lincoln courted her for a time; however, they both had second thoughts about their relationship. On August 16, 1837, Lincoln wrote Mary a letter suggesting he would not blame her if she ended the relationship. She never replied and the courtship ended.

In 1840, Lincoln became engaged to Mary Todd, who was from a wealthy slave-holding family in Lexington, Kentucky. They met in Springfield, Illinois, in December 1839 and were engaged the following December.
A wedding set for January 1, 1841, was canceled when the two broke off their engagement at Lincoln's initiative. They later met again at a party and married on November 4, 1842, in the Springfield mansion of Mary's married sister. While preparing for the nuptials and feeling anxiety again, Lincoln, when asked where he was going, replied, "To hell, I suppose."

In 1844, the couple bought a house in Springfield near Lincoln's law office. Mary Todd Lincoln kept house, often with the help of a relative or hired servant girl. Robert Todd Lincoln was born in 1843 and Edward Baker Lincoln (Eddie) in 1846. Lincoln "was remarkably fond of children", and the Lincolns were not considered to be strict with their children.

Edward died on February 1, 1850, in Springfield, probably of tuberculosis. "Willie" Lincoln was born on December 21, 1850, and died of a fever on February 20, 1862. The Lincolns' fourth son, Thomas "Tad" Lincoln, was born on April 4, 1853, and died of heart failure at the age of 18 on July 16, 1871. Robert was the only child to live to adulthood and have children. His last descendant, great-grandson Robert Todd Lincoln Beckwith, died in 1985.

The deaths of their sons had profound effects on both parents. Later in life, Mary struggled with the stresses of losing her husband and sons, and Robert Lincoln committed her temporarily to a mental health asylum in 1875. Abraham Lincoln suffered from "melancholy," a condition which now is referred to as clinical depression.

Lincoln's father-in-law and others of the Todd family were either slave owners or slave traders. Lincoln was close to the Todds, and he and his family occasionally visited the Todd estate in Lexington. He was an affectionate, though often absent, husband and father of four children.

In 1832, at age 23, Lincoln and a partner bought a small general store on credit in New Salem, Illinois. Although the economy was booming in the region, the business struggled and Lincoln eventually sold his share. That March he began his political career with his first campaign for the Illinois General Assembly. He had attained local popularity and could draw crowds as a natural raconteur in New Salem, though he lacked an education, powerful friends, and money, which may be why he lost. He advocated navigational improvements on the Sangamon River.

Before the election, Lincoln served as a captain in the Illinois Militia during the Black Hawk War. Following his return, Lincoln continued his campaign for the August 6 election for the Illinois General Assembly. At , he was tall and "strong enough to intimidate any rival". At his first speech, when he saw a supporter in the crowd being attacked, Lincoln grabbed the assailant by his "neck and the seat of his trousers" and threw him. Lincoln finished eighth out of 13 candidates (the top four were elected), though he received 277 of the 300 votes cast in the New Salem precinct.

Lincoln served as New Salem's postmaster and later as county surveyor, all the while reading voraciously. He then decided to become a lawyer and began teaching himself law by reading Blackstone's Commentaries on the Laws of England and other law books. Of his learning method, Lincoln stated: "I studied with nobody". His second campaign in 1834 was successful. He won election to the state legislature; though he ran as a Whig, many Democrats favored him over a more powerful Whig opponent.

Admitted to the bar in 1836, he moved to Springfield, Illinois, and began to practice law under John T. Stuart, Mary Todd's cousin. Lincoln became an able and successful lawyer with a reputation as a formidable adversary during cross-examinations and closing arguments. He partnered with Stephen T. Logan from 1841 until 1844.  Then Lincoln began his practice with William Herndon, whom Lincoln thought "a studious young man".

Successful on his second run for office, Lincoln served four successive terms in the Illinois House of Representatives as a Whig representative from Sangamon County. He supported the construction of the Illinois and Michigan Canal, which he remained involved with later as a Canal Commissioner. In the 1835â€“36 legislative session, he voted to expand suffrage to white males, whether landowners or not. He was known for his "free soil" stance of opposing both slavery and abolitionism. He first articulated this in 1837, saying, " Institution of slavery is founded on both injustice and bad policy, but the promulgation of abolition doctrines tends rather to increase than abate its evils." His stance closely followed Henry Clay in supporting the American Colonization Society program of making the abolition of slavery practical by its advocation and helping the freed slaves to settle in Liberia in Africa.


From the early 1830s, Lincoln was a steadfast Whig and professed to friends in 1861 to be, "an old line Whig, a disciple of Henry Clay". The party, including Lincoln, favored economic modernization in banking, protective tariffs to fund internal improvements including railroads, and espoused urbanization as well.

In 1846, Lincoln was elected to the U.S. House of Representatives, where he served one two-year term. He was the only Whig in the Illinois delegation, but he showed his party loyalty by participating in almost all votes and making speeches that echoed the party line. Lincoln, in collaboration with abolitionist Congressman Joshua R. Giddings, wrote a bill to abolish slavery in the District of Columbia with compensation for the owners, enforcement to capture fugitive slaves, and a popular vote on the matter. He abandoned the bill when it failed to garner sufficient Whig supporters.

On foreign and military policy, Lincoln spoke out against the Mexicanâ€“American War, which he attributed to President Polk's desire for "military gloryâ€”that attractive rainbow, that rises in showers of blood". Lincoln also supported the Wilmot Proviso, which, if it had been adopted, would have banned slavery in any U.S. territory won from Mexico.

Lincoln emphasized his opposition to Polk by drafting and introducing his Spot Resolutions. The war had begun with a Mexican slaughter of American soldiers in territory disputed by Mexico and the U.S. Polk insisted that Mexican soldiers had "invaded our territory and shed the blood of our fellow-citizens on our own soil. Lincoln demanded that Polk show Congress the exact spot on which blood had been shed and prove that the spot was on American soil.

Congress never enacted the resolution or even debated it, the national papers ignored it, and it resulted in a loss of political support for Lincoln in his district. One Illinois newspaper derisively nicknamed him "spotty Lincoln". Lincoln later regretted some of his statements, especially his attack on the presidential war-making powers.

Realizing Clay was unlikely to win the presidency, Lincoln, who had pledged in 1846 to serve only one term in the House, supported General Zachary Taylor for the Whig nomination in the 1848 presidential election. Taylor won and Lincoln hoped to be appointed Commissioner of the General Land Office, but that lucrative patronage job went to an Illinois rival, Justin Butterfield, considered by the administration to be a highly skilled lawyer, but in Lincoln's view, an "old fossil". The administration offered him the consolation prize of secretary or governor of the Oregon Territory. This distant territory was a Democratic stronghold, and acceptance of the post would have effectively ended his legal and political career in Illinois, so he declined and resumed his law practice.

Lincoln returned to practicing law in Springfield, handling "every kind of business that could come before a prairie lawyer". Twice a year for 16 years, 10 weeks at a time, he appeared in county seats in the midstate region when the county courts were in session. Lincoln handled many transportation cases in the midst of the nation's western expansion, particularly the conflicts arising from the operation of river barges under the many new railroad bridges. As a riverboat man, Lincoln initially favored those interests, but ultimately represented whoever hired him. In fact, he later represented a bridge company against a riverboat company in a landmark case involving a canal boat that sank after hitting a bridge. In 1849, he received a patent for a flotation device for the movement of boats in shallow water. The idea was never commercialized, but Lincoln is the only president to hold a patent.

In 1851, he represented the Alton & Sangamon Railroad in a dispute with one of its shareholders, James A. Barret, who had refused to pay the balance on his pledge to buy shares in the railroad on the grounds that the company had changed its original train route. Lincoln successfully argued that the railroad company was not bound by its original charter extant at the time of Barret's pledge; the charter was amended in the public interest to provide a newer, superior, and less expensive route, and the corporation retained the right to demand Barret's payment. The decision by the Illinois Supreme Court has been cited by numerous other courts in the nation. Lincoln appeared before the Illinois Supreme Court in 175 cases, in 51 as sole counsel, of which 31 were decided in his favor. From 1853 to 1860, another of Lincoln's largest clients was the Illinois Central Railroad.

Lincoln's most notable criminal trial occurred in 1858 when he defended William "Duff" Armstrong, who was on trial for the murder of James Preston Metzker. The case is famous for Lincoln's use of a fact established by judicial notice in order to challenge the credibility of an eyewitness. After an opposing witness testified seeing the crime in the moonlight, Lincoln produced a Farmers' Almanac showing the moon was at a low angle, drastically reducing visibility. Based on this evidence, Armstrong was acquitted.

Lincoln rarely raised objections in the courtroom; but in an 1859 case, where he defended a cousin, Peachy Harrison, who was accused of stabbing another to death, Lincoln angrily protested the judge's decision to exclude evidence favorable to his client. Instead of holding Lincoln in contempt of court as was expected, the judge, a Democrat, reversed his ruling, allowing the evidence and acquitting Harrison.



By the 1850s, slavery was still legal in the southern United States, but had been generally outlawed in the northern states, including Illinois, whose original 1818 Constitution forbade slavery, as required by the Northwest Ordinance. Lincoln disapproved of slavery, and the spread of slavery to new U.S. territory in the west. He returned to politics to oppose the pro-slavery Kansasâ€“Nebraska Act (1854); this law repealed the slavery-restricting Missouri Compromise (1820). Senior Senator Stephen A. Douglas of Illinois had incorporated popular sovereignty into the Act. Douglas' provision, which Lincoln opposed, specified settlers had the right to determine locally whether to allow slavery in new U.S. territory, rather than have such a decision restricted by the national Congress.

Eric Foner (2010) contrasts the abolitionists and anti-slavery Radical Republicans of the Northeast who saw slavery as a sin, with the conservative Republicans who thought it was bad because it hurt white people and blocked progress. Foner argues that Lincoln was a moderate in the middle, opposing slavery primarily because it violated the republicanism principles of the Founding Fathers, especially the equality of all men and democratic self-government as expressed in the Declaration of Independence.
On October 16, 1854, in his "Peoria Speech", Lincoln declared his opposition to slavery, which he repeated en route to the presidency. Speaking in his Kentucky accent, with a very powerful voice, he said the Kansas Act had a "declared indifference, but as I must think, a covert real zeal for the spread of slavery. I cannot but hate it. I hate it because of the monstrous injustice of slavery itself. I hate it because it deprives our republican example of its just influence in the world..."

In late 1854, Lincoln ran as a Whig for the U.S. Senate seat from Illinois. At that time, senators were elected by the state legislature. After leading in the first six rounds of voting in the Illinois assembly, his support began to dwindle, and Lincoln instructed his backers to vote for Lyman Trumbull, who defeated opponent Joel Aldrich Matteson. The Whigs had been irreparably split by the Kansasâ€“Nebraska Act. Lincoln wrote, "I think I am a Whig, but others say there are no Whigs, and that I am an abolitionist  I do no more than oppose the extension of slavery."

Drawing on remnants of the old Whig party, and on disenchanted Free Soil, Liberty, and Democratic Party members, he was instrumental in forging the shape of the new Republican Party. At the 1856 Republican National Convention, Lincoln placed second in the contest to become the party's candidate for vice president.

In 1857â€“1858, Douglas broke with President James Buchanan, leading to a fight for control of the Democratic Party. Some eastern Republicans even favored the reelection of Douglas for the Senate in 1858, since he had led the opposition to the Lecompton Constitution, which would have admitted Kansas as a slave state. In March 1857, the Supreme Court issued its decision in Dred Scott v. Sandford; Chief Justice Roger B. Taney opined that blacks were not citizens, and derived no rights from the Constitution. Lincoln denounced the decision, alleging it was the product of a conspiracy of Democrats to support the Slave Power. Lincoln argued, "The authors of the Declaration of Independence never intended 'to say all were equal in color, size, intellect, moral developments, or social capacity', but they 'did consider all men created equalâ€”equal in certain inalienable rights, among which are life, liberty, and the pursuit of happiness'."

After the state Republican party convention nominated him for the U.S. Senate in 1858, Lincoln delivered his House Divided Speech, drawing on , "A house divided against itself cannot stand. I believe this government cannot endure permanently half slave and half free. I do not expect the Union to be dissolvedâ€”I do not expect the house to fallâ€”but I do expect it will cease to be divided. It will become all one thing, or all the other." The speech created an evocative image of the danger of disunion caused by the slavery debate, and rallied Republicans across the North. The stage was then set for the campaign for statewide election of the Illinois legislature which would, in turn, select Lincoln or Douglas as its U.S. senator.

The Senate campaign featured the seven Lincolnâ€“Douglas debates of 1858, the most famous political debates in American history. The principals stood in stark contrast both physically and politically. Lincoln warned that "The Slave Power" was threatening the values of republicanism, and accused Douglas of distorting the values of the Founding Fathers that all men are created equal, while Douglas emphasized his Freeport Doctrine, that local settlers were free to choose whether to allow slavery or not, and accused Lincoln of having joined the abolitionists. The debates had an atmosphere of a prize fight and drew crowds in the thousands. Lincoln stated Douglas' popular sovereignty theory was a threat to the nation's morality and that Douglas represented a conspiracy to extend slavery to free states. Douglas said that Lincoln was defying the authority of the U.S. Supreme Court and the Dred Scott decision.

Though the Republican legislative candidates won more popular votes, the Democrats won more seats, and the legislature re-elected Douglas to the Senate. Despite the bitterness of the defeat for Lincoln, his articulation of the issues gave him a national political reputation. In May 1859, Lincoln purchased the Illinois Staats-Anzeiger, a German-language newspaper which was consistently supportive; most of the state's 130,000 German Americans voted Democratic but there was Republican support that a German-language paper could mobilize.

On February 27, 1860, New York party leaders invited Lincoln to give a speech at Cooper Union to a group of powerful Republicans. Lincoln argued that the Founding Fathers had little use for popular sovereignty and had repeatedly sought to restrict slavery. Lincoln insisted the moral foundation of the Republicans required opposition to slavery, and rejected any "groping for some middle ground between the right and the wrong". Despite his inelegant appearanceâ€”many in the audience thought him awkward and even uglyâ€”Lincoln demonstrated an intellectual leadership that brought him into the front ranks of the party and into contention for the Republican presidential nomination. Journalist Noah Brooks reported, "No man ever before made such an impression on his first appeal to a New York audience."

Historian Donald described the speech as a "superb political move for an unannounced candidate, to appear in one rival's (William H. Seward) own state at an event sponsored by the second rival's (Salmon P. Chase) loyalists, while not mentioning either by name during its delivery". In response to an inquiry about his presidential intentions, Lincoln said, "The taste is in my mouth a little."

On May 9â€“10, 1860, the Illinois Republican State Convention was held in Decatur. Lincoln's followers organized a campaign team led by David Davis, Norman Judd, Leonard Swett, and Jesse DuBois, and Lincoln received his first endorsement to run for the presidency. Exploiting the embellished legend of his frontier days with his father (clearing the land and splitting fence rails with an ax), Lincoln's supporters adopted the label of "The Rail Candidate".

On May 18, at the Republican National Convention in Chicago, Lincoln's friends promised and manipulated and won the nomination on the third ballot, beating candidates such as William H. Seward and Salmon P. Chase. A former Democrat, Hannibal Hamlin of Maine, was nominated for Vice President to balance the ticket. Lincoln's success depended on his reputation as a moderate on the slavery issue, and his strong support for Whiggish programs of internal improvements and the protective tariff.

On the third ballot Pennsylvania put him over the top. Pennsylvania iron interests were reassured by his support for protective tariffs. Lincoln's managers had been adroitly focused on this delegation as well as the others, while following Lincoln's strong dictate to "Make no contracts that bind me".

Most Republicans agreed with Lincoln that the North was the aggrieved party, as the Slave Power tightened its grasp on the national government with the Dred Scott decision and the presidency of James Buchanan. Throughout the 1850s, Lincoln doubted the prospects of civil war, and his supporters rejected claims that his election would incite secession. Meanwhile, Douglas was selected as the candidate of the Northern Democrats. Delegates from 11 slave states walked out of the Democratic convention, disagreeing with Douglas' position on popular sovereignty, and ultimately selected John C. Breckinridge as their candidate.

As Douglas and the other candidates went through with their campaigns, Lincoln was the only one of them who gave no speeches. Instead, he monitored the campaign closely and relied on the enthusiasm of the Republican Party. The party did the leg work that produced majorities across the North, and produced an abundance of campaign posters, leaflets, and newspaper editorials. There were thousands of Republican speakers who focused first on the party platform, and second on Lincoln's life story, emphasizing his childhood poverty. The goal was to demonstrate the superior power of "free labor", whereby a common farm boy could work his way to the top by his own efforts. The Republican Party's production of campaign literature dwarfed the combined opposition; a Chicago Tribune writer produced a pamphlet that detailed Lincoln's life, and sold 100,000 to 200,000 copies.

On November 6, 1860, Lincoln was elected the 16th president of the United States, beating Democrat Stephen A. Douglas, John C. Breckinridge of the Southern Democrats, and John Bell of the new Constitutional Union Party. He was the first president from the Republican Party. His victory was entirely due to the strength of his support in the North and West; no ballots were cast for him in 10 of the 15 Southern slave states, and he won only two of 996 counties in all the Southern states.

Lincoln received 1,866,452 votes, Douglas 1,376,957 votes, Breckinridge 849,781 votes, and Bell 588,789 votes. Turnout was 82.2percent, with Lincoln winning the free Northern states, as well as California and Oregon. Douglas won Missouri, and split New Jersey with Lincoln. Bell won Virginia, Tennessee, and Kentucky, and Breckinridge won the rest of the South.

Although Lincoln won only a plurality of the popular vote, his victory in the electoral college was decisive: Lincoln had 180 and his opponents added together had only 123. There were fusion tickets in which all of Lincoln's opponents combined to support the same slate of Electors in New York, New Jersey, and Rhode Island, but even if the anti-Lincoln vote had been combined in every state, Lincoln still would have won a majority in the Electoral College.
As Lincoln's election became evident, secessionists made clear their intent to leave the Union before he took office the next March. On December 20, 1860, South Carolina took the lead by adopting an ordinance of secession; by February 1, 1861, Florida, Mississippi, Alabama, Georgia, Louisiana, and Texas followed. Six of these states then adopted a constitution and declared themselves to be a sovereign nation, the Confederate States of America. The upper South and border states (Delaware, Maryland, Virginia, North Carolina, Tennessee, Kentucky, Missouri, and Arkansas) listened to, but initially rejected, the secessionist appeal. President Buchanan and President-elect Lincoln refused to recognize the Confederacy, declaring secession illegal. The Confederacy selected Jefferson Davis as its provisional President on February 9, 1861.

There were attempts at compromise. The Crittenden Compromise would have extended the Missouri Compromise line of 1820, dividing the territories into slave and free, contrary to the Republican Party's free-soil platform. Lincoln rejected the idea, saying, "I will suffer death before I consent... to any concession or compromise which looks like buying the privilege to take possession of this government to which we have a constitutional right."

Lincoln, however, did tacitly support the proposed Corwin Amendment to the Constitution, which passed Congress before Lincoln came into office and was then awaiting ratification by the states. That proposed amendment would have protected slavery in states where it already existed and would have guaranteed that Congress would not interfere with slavery without Southern consent. A few weeks before the war, Lincoln sent a letter to every governor informing them Congress had passed a joint resolution to amend the Constitution. Lincoln was open to the possibility of a constitutional convention to make further amendments to the Constitution.

En route to his inauguration by train, Lincoln addressed crowds and legislatures across the North. The president-elect then evaded possible assassins in Baltimore, who were uncovered by Lincoln's head of security, Allan Pinkerton. On February 23, 1861, he arrived in disguise in Washington, D.C., which was placed under substantial military guard. Lincoln directed his inaugural address to the South, proclaiming once again that he had no intention, or inclination, to abolish slavery in the Southern states:

The President ended his address with an appeal to the people of the South: "We are not enemies, but friends. We must not be enemies... The mystic chords of memory, stretching from every battlefield, and patriot grave, to every living heart and hearthstone, all over this broad land, will yet swell the chorus of the Union, when again touched, as surely they will be, by the better angels of our nature." The failure of the Peace Conference of 1861 signaled that legislative compromise was impossible. By March 1861, no leaders of the insurrection had proposed rejoining the Union on any terms. Meanwhile, Lincoln and the Republican leadership agreed that the dismantling of the Union could not be tolerated.  Lincoln said as the war was ending:
Both parties deprecated war, but one of them would make war rather than let the Nation survive, and the other would accept war rather than let it perish, and the war came.

The commander of Fort Sumter, South Carolina, Major Robert Anderson, sent a request for provisions to Washington, and the execution of Lincoln's order to meet that request was seen by the secessionists as an act of war. On April 12, 1861, Confederate forces fired on Union troops at Fort Sumter, forcing them to surrender, and began the war. Historian Allan Nevins argued that the newly inaugurated Lincoln made three miscalculations: underestimating the gravity of the crisis, exaggerating the strength of Unionist sentiment in the South, and not realizing the Southern Unionists were insisting there be no invasion.

William Tecumseh Sherman talked to Lincoln during inauguration week and was "sadly disappointed" at his failure to realize that "the country was sleeping on a volcano" and that the South was preparing for war. Donald concludes that, "His repeated efforts to avoid collision in the months between inauguration and the firing on Ft. Sumter showed he adhered to his vow not to be the first to shed fraternal blood. But he also vowed not to surrender the forts. The only resolution of these contradictory positions was for the confederates to fire the first shot; they did just that."

On April 15, Lincoln called on all the states to send detachments totaling 75,000 troops to recapture forts, protect Washington, and "preserve the Union", which, in his view, still existed intact despite the actions of the seceding states. This call forced the states to choose sides. Virginia declared its secession and was rewarded with the Confederate capital, despite the exposed position of Richmond so close to Union lines. North Carolina, Tennessee, and Arkansas also voted for secession over the next two months. Secession sentiment was strong in Missouri and Maryland, but did not prevail; Kentucky tried to be neutral. The Confederate attack on Fort Sumter rallied Americans north of the Mason-Dixon line to the defense of the American nation. Historian Allan Nevins says: 
The thunderclap of Sumter produced a startling crystallization of Northern sentiment....Anger swept the land. From every side came news of mass meetings, speeches, resolutions, tenders of business support, the muster of companies and regiments, the determined action of governors and legislatures."

States sent Union regiments south in response to Lincoln's call to save the capital and confront the rebellion. On April 19, mobs in Baltimore, which controlled the rail links, attacked Union troops who were changing trains, and local leaders' groups later burned critical rail bridges to the capital. The Army responded by arresting local Maryland officials. Lincoln suspended the writ of habeas corpus in areas the army felt it needed to secure for troops to reach Washington. John Merryman, a Maryland official involved in hindering the U.S. troop movements, petitioned Supreme Court Chief Justice and Marylander, Roger B. Taney, author of the controversial pro-slavery Dred Scott opinion, to issue a writ of habeas corpus, and in June Taney, acting as a circuit judge and not speaking for the Supreme Court, issued the writ, because in his opinion only Congress could suspend the writ. Lincoln continued the army policy that the writ was suspended in limited areas despite the Ex parte Merryman ruling.

After the Battle of Fort Sumter, Lincoln realized the importance of taking immediate executive control of the war and making an overall strategy to put down the rebellion. Lincoln encountered an unprecedented political and military crisis, and he responded as commander-in-chief, using unprecedented powers. He expanded his war powers, and imposed a blockade on all the Confederate shipping ports, disbursed funds before appropriation by Congress, and after suspending habeas corpus, arrested and imprisoned thousands of suspected Confederate sympathizers. Lincoln was supported by Congress and the northern public for these actions. In addition, Lincoln had to contend with reinforcing strong Union sympathies in the border slave states and keeping the war from becoming an international conflict.
The war effort was the source of continued disparagement of Lincoln, and dominated his time and attention. From the start, it was clear that bipartisan support would be essential to success in the war effort, and any manner of compromise alienated factions on both sides of the aisle, such as the appointment of Republicans and Democrats to command positions in the Union Army. Copperheads criticized Lincoln for refusing to compromise on the slavery issue. Conversely, the Radical Republicans criticized him for moving too slowly in abolishing slavery. On August 6, 1861, Lincoln signed the Confiscation Act that authorized judiciary proceedings to confiscate and free slaves who were used to support the Confederate war effort. In practice, the law had little effect, but it did signal political support for abolishing slavery in the Confederacy.

In late August 1861, General John C. FrÃ©mont, the 1856 Republican presidential nominee, issued, without consulting his superiors in Washington, a proclamation of martial law in Missouri. He declared that any citizen found bearing arms could be court-martialed and shot, and that slaves of persons aiding the rebellion would be freed. FrÃ©mont was already under a cloud with charges of negligence in his command of the Department of the West compounded with allegations of fraud and corruption. Lincoln overruled FrÃ©mont's proclamation. Lincoln believed that Fremont's emancipation was political; neither militarily necessary nor legal. After Lincoln acted, Union enlistments from Maryland, Kentucky, and Missouri increased by over 40,000 troops.

The Trent Affair of late 1861 threatened war with Great Britain. The U.S. Navy illegally intercepted a British merchant ship, the Trent, on the high seas and seized two Confederate envoys; Britain protested vehemently while the U.S. cheered. Lincoln resolved the issue by releasing the two men and war was successfully averted with Britain. Lincoln's foreign policy approach had been initially hands off, due to his inexperience; he left most diplomacy appointments and other foreign policy matters to his Secretary of State, William Seward. Seward's initial reaction to the Trent affair, however, was too bellicose, so Lincoln also turned to Senator Charles Sumner, the chairman of the Senate Foreign Relations Committee and an expert in British diplomacy.

To learn technical military terms, Lincoln borrowed and studied Henry Halleck's book, Elements of Military Art and Science from the Library of Congress. Lincoln painstakingly monitored the telegraphic reports coming into the War Department in Washington, D.C. He kept close tabs on all phases of the military effort, consulted with governors, and selected generals based on their past success (as well as their state and party). In January 1862, after many complaints of inefficiency and profiteering in the War Department, Lincoln replaced Simon Cameron with Edwin Stanton as War Secretary. Stanton was a staunchly Unionist pro-business conservative Democrat who moved toward the Radical Republican faction. Nevertheless he worked more often and more closely with Lincoln than any other senior official. "Stanton and Lincoln virtually conducted the war together," say Thomas and Hyman.

In terms of war strategy, Lincoln articulated two priorities: to ensure that Washington was well-defended, and to conduct an aggressive war effort that would satisfy the demand in the North for prompt, decisive victory; major Northern newspaper editors expected victory within 90 days. Twice a week, Lincoln would meet with his cabinet in the afternoon, and occasionally Mary Lincoln would force him to take a carriage ride because she was concerned he was working too hard. Lincoln learned from his chief of staff General Henry Halleck, a student of the European strategist Jomini, of the critical need to control strategic points, such as the Mississippi River; he also knew well the importance of Vicksburg and understood the necessity of defeating the enemy's army, rather than simply capturing territory.

After the Union defeat at the First Battle of Bull Run and the retirement of the aged Winfield Scott in late 1861, Lincoln appointed Major General George B. McClellan general-in-chief of all the Union armies. McClellan, a young West Point graduate, railroad executive, and Pennsylvania Democrat, took several months to plan and attempt his Peninsula Campaign, longer than Lincoln wanted. The campaign's objective was to capture Richmond by moving the Army of the Potomac by boat to the peninsula and then overland to the Confederate capital. McClellan's repeated delays frustrated Lincoln and Congress, as did his position that no troops were needed to defend Washington. Lincoln insisted on holding some of McClellan's troops in defense of the capital; McClellan, who consistently overestimated the strength of Confederate troops, blamed this decision for the ultimate failure of the Peninsula Campaign.
Lincoln removed McClellan as general-in-chief and appointed Henry Wager Halleck in March 1862, after McClellan's "Harrison's Landing Letter", in which he offered unsolicited political advice to Lincoln urging caution in the war effort. McClellan's letter incensed Radical Republicans, who successfully pressured Lincoln to appoint John Pope, a Republican, as head of the new Army of Virginia. Pope complied with Lincoln's strategic desire to move toward Richmond from the north, thus protecting the capital from attack.

However, lacking requested reinforcements from McClellan, now commanding the Army of the Potomac, Pope was soundly defeated at the Second Battle of Bull Run in the summer of 1862, forcing the Army of the Potomac to defend Washington for a second time. The war also expanded with naval operations in 1862 when the CSS Virginia, formerly the USS Merrimack, damaged or destroyed three Union vessels in Norfolk, Virginia, before being engaged and damaged by the USS Monitor. Lincoln closely reviewed the dispatches and interrogated naval officers during their clash in the Battle of Hampton Roads.

Despite his dissatisfaction with McClellan's failure to reinforce Pope, Lincoln was desperate, and restored him to command of all forces around Washington, to the dismay of all in his cabinet but Seward. Two days after McClellan's return to command, General Robert E. Lee's forces crossed the Potomac River into Maryland, leading to the Battle of Antietam in September 1862. The ensuing Union victory was among the bloodiest in American history, but it enabled Lincoln to announce that he would issue an Emancipation Proclamation in January. Having composed the Proclamation some time earlier, Lincoln had waited for a military victory to publish it to avoid it being perceived as the product of desperation.

McClellan then resisted the President's demand that he pursue Lee's retreating and exposed army, while his counterpart General Don Carlos Buell likewise refused orders to move the Army of the Ohio against rebel forces in eastern Tennessee. As a result, Lincoln replaced Buell with William Rosecrans; and, after the 1862 midterm elections, he replaced McClellan with Republican Ambrose Burnside. Both of these replacements were political moderates and prospectively more supportive of the Commander-in-Chief.
Burnside, against the advice of the president, prematurely launched an offensive across the Rappahannock River and was stunningly defeated by Lee at Fredericksburg in December. Not only had Burnside been defeated on the battlefield, but his soldiers were disgruntled and undisciplined. Desertions during 1863 were in the thousands and they increased after Fredericksburg. Lincoln brought in Joseph Hooker, despite his record of loose talk about the need for a military dictatorship.

The mid-term elections in 1862 brought the Republicans severe losses due to sharp disfavor with the administration over its failure to deliver a speedy end to the war, as well as rising inflation, new high taxes, rumors of corruption, the suspension of habeas corpus, the military draft law, and fears that freed slaves would undermine the labor market. The Emancipation Proclamation announced in September gained votes for the Republicans in the rural areas of New England and the upper Midwest, but it lost votes in the cities and the lower Midwest.

While Republicans were discouraged, Democrats were energized and did especially well in Pennsylvania, Ohio, Indiana, and New York. The Republicans did maintain their majorities in Congress and in the major states, except New York. The Cincinnati Gazette contended that the voters were "depressed by the interminable nature of this war, as so far conducted, and by the rapid exhaustion of the national resources without progress".

In the spring of 1863, Lincoln was optimistic about upcoming military campaigns to the point of thinking the end of the war could be near if a string of victories could be put together; these plans included Hooker's attack on Lee north of Richmond, Rosecrans' on Chattanooga, Grant's on Vicksburg, and a naval assault on Charleston.

Hooker was routed by Lee at the Battle of Chancellorsville in May, but continued to command his troops for some weeks. He ignored Lincoln's order to divide his troops, and possibly force Lee to do the same in Harper's Ferry, and tendered his resignation, which Lincoln accepted. He was replaced by George Meade, who followed Lee into Pennsylvania for the Gettysburg Campaign, which was a victory for the Union, though Lee's army avoided capture. At the same time, after initial setbacks, Grant laid siege to Vicksburg and the Union navy attained some success in Charleston harbor.
After the Battle of Gettysburg, Lincoln clearly understood that his military decisions would be more effectively carried out by conveying his orders through his War Secretary or his general-in-chief on to his generals, who resented his civilian interference with their own plans. Even so, he often continued to give detailed directions to his generals as Commander-in-Chief.

Lincoln understood that the Federal government's power to end slavery was limited by the Constitution, which before 1865, committed the issue to individual states. He argued before and during his election that the eventual extinction of slavery would result from preventing its expansion into new U.S. territory. At the beginning of the war, he also sought to persuade the states to accept compensated emancipation in return for their prohibition of slavery. Lincoln believed that curtailing slavery in these ways would economically expunge it, as envisioned by the Founding Fathers, under the constitution. President Lincoln rejected two geographically limited emancipation attempts by Major General John C. FrÃ©mont in August 1861 and by Major General David Hunter in May 1862, on the grounds that it was not within their power, and it would upset the border states loyal to the Union.

On June 19, 1862, endorsed by Lincoln, Congress passed an act banning slavery on all federal territory. In July 1862, the Second Confiscation Act was passed, which set up court procedures that could free the slaves of anyone convicted of aiding the rebellion. Although Lincoln believed it was not within Congress's power to free the slaves within the states, he approved the bill in deference to the legislature. He felt such action could only be taken by the Commander-in-Chief using war powers granted to the president by the Constitution, and Lincoln was planning to take that action. In that month, Lincoln discussed a draft of the Emancipation Proclamation with his cabinet. In it, he stated that "as a fit and necessary military measure, on January 1, 1863, all persons held as slaves in the Confederate states will thenceforward, and forever, be free".

Privately, Lincoln concluded at this point that the slave base of the Confederacy had to be eliminated. However Copperheads argued that emancipation was a stumbling block to peace and reunification. Republican editor Horace Greeley of the highly influential New York Tribune fell for the ploy, and Lincoln refuted it directly in a shrewd letter of August 22, 1862. Although he said he personally wished all men could be free, Lincoln stated that the primary goal of his actions as the U.S. president (he used the first person pronoun and explicitly refers to his "official duty") was that of preserving the Union:
The Emancipation Proclamation, issued on September 22, 1862, and put into effect on January 1, 1863, declared free the slaves in 10 states not then under Union control, with exemptions specified for areas already under Union control in two states. Lincoln spent the next 100 days preparing the army and the nation for emancipation, while Democrats rallied their voters in the 1862 off-year elections by warning of the threat freed slaves posed to northern whites.

Once the abolition of slavery in the rebel states became a military objective, as Union armies advanced south, more slaves were liberated until all three million of them in Confederate territory were freed. Lincoln's comment on the signing of the Proclamation was: "I never, in my life, felt more certain that I was doing right, than I do in signing this paper." For some time, Lincoln continued earlier plans to set up colonies for the newly freed slaves. He commented favorably on colonization in the Emancipation Proclamation, but all attempts at such a massive undertaking failed. A few days after Emancipation was announced, 13 Republican governors met at the War Governors' Conference; they supported the president's Proclamation, but suggested the removal of General George B. McClellan as commander of the Union Army.

Enlisting former slaves in the military was official government policy after the issuance of the Emancipation Proclamation.  By the spring of 1863, Lincoln was ready to recruit black troops in more than token numbers. In a letter to Andrew Johnson, the military governor of Tennessee, encouraging him to lead the way in raising black troops, Lincoln wrote, "The bare sight of 50,000 armed and drilled black soldiers on the banks of the Mississippi would end the rebellion at once". By the end of 1863, at Lincoln's direction, General Lorenzo Thomas had recruited 20 regiments of blacks from the Mississippi Valley. Frederick Douglass once observed of Lincoln: "In his company, I was never reminded of my humble origin, or of my unpopular color".

With the great Union victory at the Battle of Gettysburg in July 1863, and the defeat of the Copperheads in the Ohio election in the fall, Lincoln maintained a strong base of party support and was in a strong position to redefine the war effort, despite the New York City draft riots. The stage was set for his address at the Gettysburg battlefield cemetery on November 19, 1863. Defying Lincoln's prediction that "the world will little note, nor long remember what we say here," the Address became the most quoted speech in American history.

In 272 words, and three minutes, Lincoln asserted the nation was born not in 1789, but in 1776, "conceived in Liberty, and dedicated to the proposition that all men are created equal". He defined the war as an effort dedicated to these principles of liberty and equality for all. The emancipation of slaves was now part of the national war effort. He declared that the deaths of so many brave soldiers would not be in vain, that slavery would end as a result of the losses, and the future of democracy in the world would be assured, that "government of the people, by the people, for the people, shall not perish from the earth". Lincoln concluded that the Civil War had a profound objective: a new birth of freedom in the nation.

Meade's failure to capture Lee's army as it retreated from Gettysburg, and the continued passivity of the Army of the Potomac, persuaded Lincoln that a change in command was needed. General Ulysses S. Grant's victories at the Battle of Shiloh and in the Vicksburg campaign impressed Lincoln and made Grant a strong candidate to head the Union Army. Responding to criticism of Grant after Shiloh, Lincoln had said, "I can't spare this man. He fights." With Grant in command, Lincoln felt the Union Army could relentlessly pursue a series of coordinated offensives in multiple theaters, and have a top commander who agreed on the use of black troops.

Nevertheless, Lincoln was concerned that Grant might be considering a candidacy for President in 1864, as McClellan was. Lincoln arranged for an intermediary to make inquiry into Grant's political intentions, and being assured that he had none, submitted to the Senate Grant's promotion to commander of the Union Army. He obtained Congress's consent to reinstate for Grant the rank of Lieutenant General, which no officer had held since George Washington.

Grant waged his bloody Overland Campaign in 1864. This is often characterized as a war of attrition, given high Union losses at battles such as the Battle of the Wilderness and Cold Harbor. Even though they had the advantage of fighting on the defensive, the Confederate forces had "almost as high a percentage of casualties as the Union forces". The high casualty figures of the Union alarmed the North; Grant had lost a third of his army, and Lincoln asked what Grant's plans were, to which the general replied, "I propose to fight it out on this line if it takes all summer."

The Confederacy lacked reinforcements, so Lee's army shrank with every costly battle. Grant's army moved south, crossed the James River, forcing a siege and trench warfare outside Petersburg, Virginia. Lincoln then made an extended visit to Grant's headquarters at City Point, Virginia. This allowed the president to confer in person with Grant and William Tecumseh Sherman about the hostilities, as Sherman coincidentally managed a hasty visit to Grant from his position in North Carolina. Lincoln and the Republican Party mobilized support for the draft throughout the North, and replaced the Union losses.

Lincoln authorized Grant to target the Confederate infrastructureâ€”such as plantations, railroads, and bridgesâ€”hoping to destroy the South's morale and weaken its economic ability to continue fighting. Grant's move to Petersburg resulted in the obstruction of three railroads between Richmond and the South. This strategy allowed Generals Sherman and Philip Sheridan to destroy plantations and towns in Virginia's Shenandoah Valley. The damage caused by Sherman's March to the Sea through Georgia in 1864 was limited to a  swath, but neither Lincoln nor his commanders saw destruction as the main goal, but rather defeat of the Confederate armies. Mark E. Neely Jr. has argued that there was no effort to engage in "total war" against civilians which he believed did take place during World War II.

Confederate general Jubal Anderson Early began a series of assaults in the North that threatened the Capital. During Early's raid on Washington, D.C. in 1864, Lincoln was watching the combat from an exposed position; Captain Oliver Wendell Holmes shouted at him, "Get down, you damn fool, before you get shot!" After repeated calls on Grant to defend Washington, Sheridan was appointed and the threat from Early was dispatched.

As Grant continued to wear down Lee's forces, efforts to discuss peace began. Confederate Vice President Stephens led a group to meet with Lincoln, Seward, and others at Hampton Roads. Lincoln refused to allow any negotiation with the Confederacy as a coequal; his sole objective was an agreement to end the fighting and the meetings produced no results. On April 1, 1865, Grant successfully outflanked Lee's forces in the Battle of Five Forks and nearly encircled Petersburg, and the Confederate government evacuated Richmond. Days later, when that city fell, Lincoln visited the vanquished Confederate capital; as he walked through the city, white Southerners were stone-faced, but freedmen greeted him as a hero. On April 9, Lee surrendered to Grant at Appomattox and the war was effectively over.

While the war was still being waged, Lincoln faced reelection in 1864. Lincoln was a master politician, bringing togetherâ€”and holding togetherâ€”all the main factions of the Republican Party, and bringing in War Democrats such as Edwin M. Stanton and Andrew Johnson as well. Lincoln spent many hours a week talking to politicians from across the land and using his patronage powersâ€”greatly expanded over peacetimeâ€”to hold the factions of his party together, build support for his own policies, and fend off efforts by Radicals to drop him from the 1864 ticket. At its 1864 convention, the Republican Party selected Johnson, a War Democrat from the Southern state of Tennessee, as his running mate. To broaden his coalition to include War Democrats as well as Republicans, Lincoln ran under the label of the new Union Party.

When Grant's 1864 spring campaigns turned into bloody stalemates and Union casualties mounted, the lack of military success wore heavily on the President's re-election prospects, and many Republicans across the country feared that Lincoln would be defeated. Sharing this fear, Lincoln wrote and signed a pledge that, if he should lose the election, he would still defeat the Confederacy before turning over the White House:
 Lincoln did not show the pledge to his cabinet, but asked them to sign the sealed envelope.
While the Democratic platform followed the "Peace wing" of the party and called the war a "failure", their candidate, General George B. McClellan, supported the war and repudiated the platform. Lincoln provided Grant with more troops and mobilized his party to renew its support of Grant in the war effort. Sherman's capture of Atlanta in September and David Farragut's capture of Mobile ended defeatist jitters; the Democratic Party was deeply split, with some leaders and most soldiers openly for Lincoln. By contrast, the National Union Party was united and energized as Lincoln made emancipation the central issue, and state Republican parties stressed the perfidy of the Copperheads. On November 8, Lincoln was re-elected in a landslide, carrying all but three states, and receiving 78 percent of the Union soldiers' vote.

On March 4, 1865, Lincoln delivered his second inaugural address. In it, he deemed the high casualties on both sides to be God's will. Historian Mark Noll concludes it ranks "among the small handful of semi-sacred texts by which Americans conceive their place in the world". Lincoln said:

Reconstruction began during the war, as Lincoln and his associates anticipated questions of how to reintegrate the conquered southern states, and how to determine the fates of Confederate leaders and freed slaves. Shortly after Lee's surrender, a general had asked Lincoln how the defeated Confederates should be treated, and Lincoln replied, "Let 'em up easy." In keeping with that sentiment, Lincoln led the moderates regarding Reconstruction policy, and was opposed by the Radical Republicans, under Rep. Thaddeus Stevens, Sen. Charles Sumner and Sen. Benjamin Wade, political allies of the president on other issues. Determined to find a course that would reunite the nation and not alienate the South, Lincoln urged that speedy elections under generous terms be held throughout the war. His Amnesty Proclamation of December 8, 1863, offered pardons to those who had not held a Confederate civil office, had not mistreated Union prisoners, and would sign an oath of allegiance.
As Southern states were subdued, critical decisions had to be made as to their leadership while their administrations were re-formed. Of special importance were Tennessee and Arkansas, where Lincoln appointed Generals Andrew Johnson and Frederick Steele as military governors, respectively. In Louisiana, Lincoln ordered General Nathaniel P. Banks to promote a plan that would restore statehood when 10 percent of the voters agreed to it. Lincoln's Democratic opponents seized on these appointments to accuse him of using the military to ensure his and the Republicans' political aspirations. On the other hand, the Radicals denounced his policy as too lenient, and passed their own plan, the Wade-Davis Bill, in 1864. When Lincoln vetoed the bill, the Radicals retaliated by refusing to seat representatives elected from Louisiana, Arkansas, and Tennessee.

Lincoln's appointments were designed to keep both the moderate and Radical factions in harness. To fill Chief Justice Taney's seat on the Supreme Court, he named the choice of the Radicals, Salmon P. Chase, who Lincoln believed would uphold the emancipation and paper money policies.

After implementing the Emancipation Proclamation, which did not apply to every state, Lincoln increased pressure on Congress to outlaw slavery throughout the entire nation with a constitutional amendment. Lincoln declared that such an amendment would "clinch the whole matter". By December 1863, a proposed constitutional amendment that would outlaw slavery was brought to Congress for passage. This first attempt at an amendment failed to pass, falling short of the required two-thirds majority on June 15, 1864, in the House of Representatives. Passage of the proposed amendment became part of the Republican/Unionist platform in the election of 1864. After a long debate in the House, a second attempt passed Congress on January 31, 1865, and was sent to the state legislatures for ratification. Upon ratification, it became the Thirteenth Amendment to the United States Constitution on December 6, 1865.

As the war drew to a close, Lincoln's presidential Reconstruction for the South was in flux; having believed the federal government had limited responsibility to the millions of freedmen. He signed into law Senator Charles Sumner's Freedmen's Bureau bill that set up a temporary federal agency designed to meet the immediate material needs of former slaves. The law assigned land for a lease of three years with the ability to purchase title for the freedmen. Lincoln stated that his Louisiana plan did not apply to all states under Reconstruction. Shortly before his assassination, Lincoln announced he had a new plan for southern Reconstruction. Discussions with his cabinet revealed Lincoln planned short-term military control over southern states, until readmission under the control of southern Unionists.

Historians agree that it is impossible to predict exactly what Lincoln would have done about Reconstruction if he had lived, but they make projections based on his known policy positions and political acumen. Lincoln biographers James G. Randall and Richard Current, according to David Lincove, argue that:
It is likely that had he lived, Lincoln would have followed a policy similar to Johnson's, that he would have clashed with congressional Radicals, that he would have produced a better result for the freedmen than occurred, and that his political skills would have helped him avoid Johnson's mistakes.

Eric Foner argues that:
Unlike Sumner and other Radicals, Lincoln did not see Reconstruction as an opportunity for a sweeping political and social revolution beyond emancipation. He had long made clear his opposition to the confiscation and redistribution of land. He believed, as most Republicans did in April 1865, that the voting requirements should be determined by the states. He assumed that political control in the South would pass to white Unionists, reluctant secessionists, and forward-looking former Confederates. But time and again during the war, Lincoln, after initial opposition, had come to embrace positions first advanced by abolitionists and Radical Republicans..... Lincoln undoubtedly would have listened carefully to the outcry for further protection for the former slaves.... It is entirely plausible to imagine Lincoln and Congress agreeing on a Reconstruction policy that encompassed federal protection for basic civil rights plus limited black suffrage, along the lines Lincoln proposed just before his death."

The successful reunification of the states had consequences for the name of the country. The term "the United States" has historically been used, sometimes in the plural ("these United States"), and other times in the singular, without any particular grammatical consistency. The Civil War was a significant force in the eventual dominance of the singular usage by the end of the 19th century.

In recent years, historians such as Harry Jaffa, Herman Belz, John Diggins, Vernon Burton and Eric Foner have stressed Lincoln's redefinition of republican values. As early as the 1850s, a time when most political rhetoric focused on the sanctity of the Constitution, Lincoln redirected emphasis to the Declaration of Independence as the foundation of American political valuesâ€”what he called the "sheet anchor" of republicanism. The Declaration's emphasis on freedom and equality for all, in contrast to the Constitution's tolerance of slavery, shifted the debate. As Diggins concludes regarding the highly influential Cooper Union speech of early 1860, "Lincoln presented Americans a theory of history that offers a profound contribution to the theory and destiny of republicanism itself." His position gained strength because he highlighted the moral basis of republicanism, rather than its legalisms. Nevertheless, in 1861, Lincoln justified the war in terms of legalisms (the Constitution was a contract, and for one party to get out of a contract all the other parties had to agree), and then in terms of the national duty to guarantee a republican form of government in every state. Burton (2008) argues that Lincoln's republicanism was taken up by the Freedmen as they were emancipated.

In March 1861, in Lincoln's first inaugural address, he explored the nature of democracy. He denounced secession as anarchy, and explained that majority rule had to be balanced by constitutional restraints in the American system. He said "A majority held in restraint by constitutional checks and limitations, and always changing easily with deliberate changes of popular opinions and sentiments, is the only true sovereign of a free people."
Lincoln adhered to the Whig theory of the presidency, which gave Congress primary responsibility for writing the laws while the Executive enforced them. Lincoln only vetoed four bills passed by Congress; the only important one was the Wade-Davis Bill with its harsh program of Reconstruction. He signed the Homestead Act in 1862, making millions of acres of government-held land in the West available for purchase at very low cost. The Morrill Land-Grant Colleges Act, also signed in 1862, provided government grants for agricultural colleges in each state. The Pacific Railway Acts of 1862 and 1864 granted federal support for the construction of the United States' First Transcontinental Railroad, which was completed in 1869. The passage of the Homestead Act and the Pacific Railway Acts was made possible by the absence of Southern congressmen and senators who had opposed the measures in the 1850s.
Other important legislation involved two measures to raise revenues for the Federal government: tariffs (a policy with long precedent), and a new Federal income tax. In 1861, Lincoln signed the second and third Morrill Tariff, the first having become law under James Buchanan. Also in 1861, Lincoln signed the Revenue Act of 1861, creating the first U.S. income tax. This created a flat tax of 3 percent on incomes above $800 ($ in current dollar terms), which was later changed by the Revenue Act of 1862 to a progressive rate structure.

Lincoln also presided over the expansion of the federal government's economic influence in several other areas. The creation of the system of national banks by the National Banking Act provided a strong financial network in the country. It also established a national currency. In 1862, Congress created, with Lincoln's approval, the Department of Agriculture. In 1862, Lincoln sent a senior general, John Pope, to put down the "Sioux Uprising" in Minnesota. Presented with 303 execution warrants for convicted Santee Dakota who were accused of killing innocent farmers, Lincoln conducted his own personal review of each of these warrants, eventually approving 39 for execution (one was later reprieved). President Lincoln had planned to reform federal Indian policy.

In the wake of Grant's casualties in his campaign against Lee, Lincoln had considered yet another executive call for a military draft, but it was never issued. In response to rumors of one, however, the editors of the New York World and the Journal of Commerce published a false draft proclamation which created an opportunity for the editors and others employed at the publications to corner the gold market. Lincoln's reaction was to send the strongest of messages to the media about such behavior; he ordered the military to seize the two papers. The seizure lasted for two days.

Lincoln is largely responsible for the institution of the Thanksgiving holiday in the United States. Before Lincoln's presidency, Thanksgiving, while a regional holiday in New England since the 17th century, had been proclaimed by the federal government only sporadically and on irregular dates. The last such proclamation had been during James Madison's presidency 50 years before. In 1863, Lincoln declared the final Thursday in November of that year to be a day of Thanksgiving. In June 1864, Lincoln approved the Yosemite Grant enacted by Congress, which provided unprecedented federal protection for the area now known as Yosemite National Park.

Noah Haynes Swayne â€“ 1862
Samuel Freeman Miller â€“ 1862
David Davis â€“ 1862
Stephen Johnson Field â€“ 1863
Salmon Portland Chase â€“ 1864 (Chief Justice)
Lincoln's declared philosophy on court nominations was that "we cannot ask a man what he will do, and if we should, and he should answer us, we should despise him for it. Therefore we must take a man whose opinions are known." Lincoln made five appointments to the United States Supreme Court. Noah Haynes Swayne, nominated January 21, 1862 and appointed January 24, 1862, was chosen as an anti-slavery lawyer who was committed to the Union. Samuel Freeman Miller, nominated and appointed on July 16, 1862, supported Lincoln in the 1860 election and was an avowed abolitionist. David Davis, Lincoln's campaign manager in 1860, nominated December 1, 1862 and appointed December 8, 1862, had also served as a judge in Lincoln's Illinois court circuit. Stephen Johnson Field, a previous California Supreme Court justice, was nominated March 6, 1863 and appointed March 10, 1863, and provided geographic balance, as well as political balance to the court as a Democrat. Finally, Lincoln's Treasury Secretary, Salmon P. Chase, was nominated as Chief Justice, and appointed the same day, on December 6, 1864. Lincoln believed Chase was an able jurist, would support Reconstruction legislation, and that his appointment united the Republican Party.


Lincoln appointed 32 federal judges, including four Associate Justices and one Chief Justice to the Supreme Court of the United States, and 27 judges to the United States district courts. Lincoln appointed no judges to the United States circuit courts during his time in office.

West Virginia, admitted to the Union June 20, 1863, contained the former north-westernmost counties of Virginia that seceded from Virginia after that commonwealth declared its secession from the Union. As a condition for its admission, West Virginia's constitution was required to provide for the gradual abolition of slavery. Nevada, which became the third State in the far-west of the continent, was admitted as a free state on October 31, 1864.
John Wilkes Booth was a well-known actor and a Confederate spy from Maryland; though he never joined the Confederate army, he had contacts with the Confederate secret service. In 1864, Booth formulated a plan (very similar to one of Thomas N. Conrad previously authorized by the Confederacy) to kidnap Lincoln in exchange for the release of Confederate prisoners. After attending an April 11, 1865, speech in which Lincoln promoted voting rights for blacks, an incensed Booth changed his plans and became determined to assassinate the president. Learning that the President and Grant would be attending Ford's Theatre, Booth formulated a plan with co-conspirators to assassinate Lincoln and Grant at the theater, as well as Vice President Johnson and Secretary of State Seward at their homes. Without his main bodyguard, Ward Hill Lamon, Lincoln left to attend the play Our American Cousin on April 14. At the last minute, Grant decided to go to New Jersey to visit his children instead of attending the play.

Lincoln's bodyguard, John Parker, left Ford's Theater during intermission to drink at the saloon next door. The now unguarded President sat in his state box in the balcony. Seizing the opportunity, Booth crept up from behind and at about 10:13pm, aimed at the back of Lincoln's head and fired at point-blank range, mortally wounding the President. Major Henry Rathbone momentarily grappled with Booth, but Booth stabbed him and escaped.

After being on the run for 12 days, Booth was tracked down and found on a farm in Virginia, some  south of Washington. After refusing to surrender to Union troops, Booth was killed by Sergeant Boston Corbett on April 26.

Doctor Charles Leale, an Army surgeon, found the President unresponsive, barely breathing and with no detectable pulse. Having determined that the President had been shot in the head, and not stabbed in the shoulder as originally thought, he made an attempt to clear the blood clot, after which the President began to breathe more naturally. The dying President was taken across the street to Petersen House. After remaining in a coma for nine hours, Lincoln died at 7:22am on April 15. Secretary of War Stanton saluted and said, "Now he belongs to the ages."

Lincoln's flag-enfolded body was then escorted in the rain to the White House by bareheaded Union officers, while the city's church bells rang. President Johnson was sworn in at 10:00am, less than 3 hours after Lincoln's death. The late President lay in state in the East Room, and then in the Capitol Rotunda from April 19 through April 21. For his final journey with his son Willie, both caskets were transported in the executive coach "United States" and for three weeks the Lincoln Special funeral train decorated in black bunting bore Lincoln's remains on a slow circuitous waypoint journey from Washington D.C. to Springfield, Illinois, stopping at many cities across the North for large-scale memorials attended by hundreds of thousands, as well as many people who gathered in informal trackside tributes with bands, bonfires, and hymn singing or silent reverence with hat in hand as the railway procession slowly passed by. Poet Walt Whitman composed When Lilacs Last in the Dooryard Bloom'd to eulogize Lincoln, one of four poems he wrote about the assassinated president. Historians have emphasized the widespread shock and sorrow, but also noted that some Lincoln haters cheered when they heard the news.  African-Americans were especially moved; they had lost 'their Moses'.  In a larger sense, the outpouring of grief and anguish was in response to the deaths of so many men in the war that had just ended.

As a young man, Lincoln was a religious skeptic, or, in the words of a biographer, an iconoclast. Later in life, Lincoln's frequent use of religious imagery and language might have reflected his own personal beliefs or might have been a device to appeal to his audiences, who were mostly evangelical Protestants. He never joined a church, although he frequently attended with his wife. However, he was deeply familiar with the Bible, and he both quoted and praised it. He was private about his beliefs and respected the beliefs of others.  Lincoln never made a clear profession of Christian beliefs. However he did believe in an all-powerful God that shaped events and, by 1865, was expressing those beliefs in major speeches.

In the 1840s, Lincoln subscribed to the Doctrine of Necessity, a belief that asserted the human mind was controlled by some higher power. In the 1850s, Lincoln believed in "providence" in a general way, and rarely used the language or imagery of the evangelicals; he regarded the republicanism of the Founding Fathers with an almost religious reverence. When he suffered the death of his son Edward, Lincoln more frequently expressed a need to depend on God. The death of his son Willie in February 1862 may have caused Lincoln to look toward religion for answers and solace. After Willie's death, Lincoln considered why, from a divine standpoint, the severity of the war was necessary. He wrote at this time that God "could have either saved or destroyed the Union without a human contest. Yet the contest began. And having begun He could give the final victory to either side any day. Yet the contest proceeds." On the day Lincoln was assassinated, he reportedly told his wife he desired to visit the Holy Land.


Several claims abound that Lincoln's health was declining before the assassination. These are often based on photographs appearing to show weight loss and muscle wasting. One such claim is that he suffered from a rare genetic disorder MEN2b, which manifests with a medullary thyroid carcinoma, mucosal neuromas and a Marfinoid appearance. Others simply claim he had Marfan's syndrome, based on his tall appearance with spindly fingers, and the association of possible aortic regurgitation, which can cause bobbing of the head (DeMusset's sign) based on blurring of Lincoln's head in photographs, which back then had a long exposure time.
DNA analysis is so far being refused by the Grand Army of the Republic museum in Philadelphia.


In surveys of U.S. scholars ranking presidents conducted since the 1940s, Lincoln is consistently ranked in the top three, often as number one. A 2004 study found that scholars in the fields of history and politics ranked Lincoln number one, while legal scholars placed him second after Washington. In presidential ranking polls conducted in the United States since 1948, Lincoln has been rated at the very top in the majority of polls: Schlesinger 1948, Schlesinger 1962, 1982 Murray Blessing Survey, Chicago Tribune 1982 poll, Schlesinger 1996, CSPAN 1996, Ridings-McIver 1996, Time 2008, and CSPAN 2009. Generally, the top three presidents are rated as 1. Lincoln; 2. George Washington; and 3. Franklin D. Roosevelt, although Lincoln and Washington, and Washington and Roosevelt, occasionally are reversed.

President Lincoln's assassination increased his status to the point of making him a national martyr. Lincoln was viewed by abolitionists as a champion for human liberty. Republicans linked Lincoln's name to their party. Many, though not all, in the South considered Lincoln as a man of outstanding ability.

Schwartz argues that Lincoln's reputation grew slowly in the late 19th century until the Progressive Era (1900â€“1920s) when he emerged as one of the most venerated heroes in American history, with even white Southerners in agreement. The high point came in 1922 with the dedication of the Lincoln Memorial on the Mall in Washington. In the New Deal era liberals honored Lincoln not so much as the self-made man or the great war president, but as the advocate of the common man who doubtless would have supported the welfare state. In the Cold War years, Lincoln's image shifted to emphasize the symbol of freedom who brought hope to those oppressed by communist regimes.

By the 1970s Lincoln had become a hero to political conservatives for his intense nationalism, support for business, his insistence on stopping the spread of human bondage, his acting in terms of Lockean and Burkean principles on behalf of both liberty and tradition, and his devotion to the principles of the Founding Fathers. As a Whig activist, Lincoln was a spokesman for business interests, favoring high tariffs, banks, internal improvements, and railroads in opposition to the agrarian Democrats. William C. Harris found that Lincoln's "reverence for the Founding Fathers, the Constitution, the laws under it, and the preservation of the Republic and its institutions undergirded and strengthened his conservatism". James G. Randall emphasizes his tolerance and especially his moderation "in his preference for orderly progress, his distrust of dangerous agitation, and his reluctance toward ill digested schemes of reform". Randall concludes that, "he was conservative in his complete avoidance of that type of so-called 'radicalism' which involved abuse of the South, hatred for the slaveholder, thirst for vengeance, partisan plotting, and ungenerous demands that Southern institutions be transformed overnight by outsiders."

By the late 1960s, liberals, such as historian Lerone Bennett, were having second thoughts, especially regarding Lincoln's views on racial issues. Bennett won wide attention when he called Lincoln a white supremacist in 1968. He noted that Lincoln used ethnic slurs, told jokes that ridiculed blacks, insisted he opposed social equality, and proposed sending freed slaves to another country. Defenders, such as authors Dirck and Cashin, retorted that he was not as bad as most politicians of his day; and that he was a "moral visionary" who deftly advanced the abolitionist cause, as fast as politically possible. The emphasis shifted away from Lincoln-the-emancipator to an argument that blacks had freed themselves from slavery, or at least were responsible for pressuring the government on emancipation. Historian Barry Schwartz wrote in 2009 that Lincoln's image suffered "erosion, fading prestige, benign ridicule" in the late 20th century. On the other hand, Donald opined in his 1996 biography that Lincoln was distinctly endowed with the personality trait of negative capability, defined by the poet John Keats and attributed to extraordinary leaders who were "content in the midst of uncertainties and doubts, and not compelled toward fact or reason".

Lincoln has often been portrayed by Hollywood, almost always in a flattering light.

Lincoln's portrait appears on two denominations of United States currency, the penny and the $5 bill. His likeness also appears on many postage stamps and has been memorialized in many town, city, and county names, including the capital of Nebraska. 

The most famous and most visited memorials are the Lincoln Memorial in Washington, D.C.; Lincoln's sculpture on Mount Rushmore; Ford's Theatre and Petersen House (where he died) in Washington and the Abraham Lincoln Presidential Library and Museum, located in Springfield, Illinois, not far from Lincoln's home and his tomb.

There was also the Great Moments with Mr. Lincoln exhibit in Disneyland, and the Hall of Presidents at Walt Disney World, which had to do with Walt Disney admiring Lincoln ever since he was a little boy.

Barry Schwartz, a sociologist who has examined America's cultural memory, argues that in the 1930s and 1940s, the memory of Abraham Lincoln was practically sacred and provided the nation with "a moral symbol inspiring and guiding American life". During the Great Depression, he argues,  Lincoln served "as a means for seeing the world's disappointments, for making its sufferings not so much explicable as meaningful". Franklin D. Roosevelt, preparing America for war, used the words of the Civil War president to clarify the threat posed by Germany and Japan. Americans asked, "What would Lincoln do?" However, Schwartz also finds that since World War II, Lincoln's symbolic power has lost relevance, and this "fading hero is symptomatic of fading confidence in national greatness".  He suggested that postmodernism and multiculturalism have diluted greatness as a concept.


List of Presidents of the United States
List of Presidents of the United States, sortable by previous experience
Blab school
Dakota War of 1862
Lincoln Tower
List of photographs of Abraham Lincoln
List of civil rights leaders
McClintock, Russell. Lincoln and the Decision for War: The Northern Response to Secession (2008) 
, also published as vol 3â€“4 of Ordeal of the Union
; also published as vol 5â€“8 of Ordeal of the Union
Holzer, Harold and Craig L. Symonds, eds. Exploring Lincoln: Great Historians Reappraise Our Greatest President (2015), essays by 16 scholars
Manning, Chandra, "The Shifting Terrain of Attitudes toward Abraham Lincoln and Emancipation," Journal of the Abraham Lincoln Association, 34 (Winter 2013), 18â€“39.
Smith, Adam I.P. "The 'Cult' of Abraham Lincoln and the Strange Survival of Liberal England in the Era of the World Wars," Twentieth Century British History, (Dec 2010) 21#4 pp 486â€“509
Spielberg, Steven; Goodwin, Doris Kearns; Kushner, Tony. "Mr. Lincoln Goes to Hollywood," Smithsonian  (2012) 43#7 pp 46â€“53.

Green, Michael S. Lincoln and the Election of 1860 (Concise Lincoln Library) 
, vol 3. of detailed biography
Official
Organizations
Media coverage
Other

 at C-SPAN's 
 - Shapell Manuscript Foundation
 - Northern Illinois University Libraries
 - National Endowment for the Humanities


Aristotle (;  , AristotÃ©lÄ“s; 384322BC) was a Greek philosopher and scientist born in the Macedonian city of Stagira, Chalkidice, on the northern periphery of Classical Greece. His father, Nicomachus, died when Aristotle was a child, whereafter Proxenus of Atarneus became his guardian. At eighteen, he joined Plato's Academy in Athens and remained there until the age of thirty-seven (c.347BC). His writings cover many subjects â€“including physics, biology, zoology, metaphysics, logic, ethics, aesthetics, poetry, theater, music, rhetoric, linguistics, politics and governmentâ€“ and constitute the first comprehensive system of Western philosophy. Shortly after Plato died, Aristotle left Athens and, at the request of Philip of Macedon, tutored Alexander the Great starting from 343BC. According to the EncyclopÃ¦dia Britannica, "Aristotle was the first genuine scientist in history ...  every scientist is in his debt."

Teaching Alexander the Great gave Aristotle many opportunities and an abundance of supplies. He established a library in the Lyceum which aided in the production of many of his hundreds of books. The fact that Aristotle was a pupil of Plato contributed to his former views of Platonism, but, following Plato's death, Aristotle immersed himself in empirical studies and shifted from Platonism to empiricism. He believed all peoples' concepts and all of their knowledge was ultimately based on perception. Aristotle's views on natural sciences represent the groundwork underlying many of his works.

Aristotle's views on physical science profoundly shaped medieval scholarship. Their influence extended into the Renaissance and were not replaced systematically until the Enlightenment and theories such as classical mechanics. Some of Aristotle's zoological observations, such as on the hectocotyl (reproductive) arm of the octopus, were not confirmed or refuted until the 19th century. His works contain the earliest known formal study of logic, which was incorporated in the late 19th century into modern formal logic.

In metaphysics, Aristotelianism profoundly influenced Judeo-Islamic philosophical and theological thought during the Middle Ages and continues to influence Christian theology, especially the scholastic tradition of the Catholic Church. Aristotle was well known among medieval Muslim intellectuals and revered as "The First Teacher" ().

His ethics, though always influential, gained renewed interest with the modern advent of virtue ethics. All aspects of Aristotle's philosophy continue to be the object of active academic study today. Though Aristotle wrote many elegant treatises and dialogues â€“Cicero described his literary style as "a river of gold"â€“ it is thought that only around a third of his original output has survived.

The sum of his work's influence often ranks him among the world's top personalities of all time with the greatest influence, along with his teacher Plato, and his pupil Alexander the Great.

Aristotle, whose name means "the best purpose", was born in 384BC in Stagira, Chalcidice, about 55km (34 miles) east of modern-day Thessaloniki. His father Nicomachus was the personal physician to King Amyntas of Macedon.  Although there is little information on Aristotle's childhood, he probably spent some time within the Macedonian palace, making his first connections with the Macedonian monarchy.

At about the age of eighteen, Aristotle moved to Athens to continue his education at Plato's Academy. He remained there for nearly twenty years before leaving Athens in 348/47BC. The traditional story about his departure records that he was disappointed with the Academy's direction after control passed to Plato's nephew Speusippus, although it is possible that he feared anti-Macedonian sentiments and left before Plato had died.

Aristotle then accompanied Xenocrates to the court of his friend Hermias of Atarneus in Asia Minor. There, he traveled with Theophrastus to the island of Lesbos, where together they researched the botany and zoology of the island. Aristotle married Pythias, either Hermias's adoptive daughter or niece. She bore him a daughter, whom they also named Pythias. Soon after Hermias' death, Aristotle was invited by Philip II of Macedon to become the tutor to his son Alexander in 343BC.

Aristotle was appointed as the head of the royal academy of Macedon. During that time he gave lessons not only to Alexander, but also to two other future kings: Ptolemy and Cassander. Aristotle encouraged Alexander toward eastern conquest and his attitude towards Persia was unabashedly ethnocentric. In one famous example, he counsels Alexander to be "a leader to the Greeks and a despot to the barbarians, to look after the former as after friends and relatives, and to deal with the latter as with beasts or plants".

By 335BC, Artistotle had returned to Athens, establishing his own school there known as the Lyceum. Aristotle conducted courses at the school for the next twelve years. While in Athens, his wife Pythias died and Aristotle became involved with Herpyllis of Stagira, who bore him a son whom he named after his father, Nicomachus. According to the Suda, he also had an eromenos, Palaephatus of Abydus.

This period in Athens, between 335 and 323BC, is when Aristotle is believed to have composed many of his works. He wrote many dialogues of which only fragments have survived. Those works that have survived are in treatise form and were not, for the most part, intended for widespread publication; they are generally thought to be lecture aids for his students. His most important treatises include Physics, Metaphysics, Nicomachean Ethics, Politics, De Anima (On the Soul) and Poetics.

Aristotle not only studied almost every subject possible at the time, but made significant contributions to most of them. In physical science, Aristotle studied anatomy, astronomy, embryology, geography, geology, meteorology, physics and zoology. In philosophy, he wrote on aesthetics, ethics, government, metaphysics, politics, economics, psychology, rhetoric and theology. He also studied education, foreign customs, literature and poetry. His combined works constitute a virtual encyclopedia of Greek knowledge.

Near the end of his life, Alexander and Aristotle became estranged over Alexander's relationship with Persia and Persians. A widespread tradition in antiquity suspected Aristotle of playing a role in Alexander's death, but there is little evidence.

Following Alexander's death, anti-Macedonian sentiment in Athens was rekindled. In 322BC, Eurymedon the Hierophant denounced Aristotle for not holding the gods in honor, prompting him to flee to his mother's family estate in Chalcis, explaining: "I will not allow the Athenians to sin twice against philosophy"a reference to Athens's prior trial and execution of Socrates. He died in Euboea of natural causes later that same year, having named his student Antipater as his chief executor and leaving a will in which he asked to be buried next to his wife.

Charles Walston argues that the tomb of Aristotle is located on the sacred way between Chalcis and Eretria and to have contained two styluses, a pen, a signet-ring and some terra-cottas as well as what is supposed to be the earthly remains of Aristotle in the form of some skull fragments.

In general, the details of the life of Aristotle are not well-established.  The biographies of Aristotle written in ancient times are often speculative and historians only agree on a few salient points.


With the Prior Analytics, Aristotle is credited with the earliest study of formal logic, and his conception of it was the dominant form of Western logic until 19th century advances in mathematical logic. Kant stated in the Critique of Pure Reason that Aristotle's theory of logic completely accounted for the core of deductive inference.

Aristotle "says that 'on the subject of reasoning' he 'had nothing else on an earlier date to speak of'". However, Plato reports that syntax was devised before him, by Prodicus of Ceos, who was concerned by the correct use of words. Logic seems to have emerged from dialectics; the earlier philosophers made frequent use of concepts like reductio ad absurdum in their discussions, but never truly understood the logical implications. Even Plato had difficulties with logic; although he had a reasonable conception of a deductive system, he could never actually construct one, thus he relied instead on his dialectic.

Plato believed that deduction would simply follow from premises, hence he focused on maintaining solid premises so that the conclusion would logically follow. Consequently, Plato realized that a method for obtaining conclusions would be most beneficial. He never succeeded in devising such a method, but his best attempt was published in his book Sophist, where he introduced his division method.


What we today call Aristotelian logic, Aristotle himself would have labeled "analytics". The term "logic" he reserved to mean dialectics. Most of Aristotle's work is probably not in its original form, because it was most likely edited by students and later lecturers. The logical works of Aristotle were compiled into six books in about the early 1st century CE:
Categories
On Interpretation
Prior Analytics
Posterior Analytics
Topics
On Sophistical Refutations

The order of the books (or the teachings from which they are composed) is not certain, but this list was derived from analysis of Aristotle's writings. It goes from the basics, the analysis of simple terms in the Categories, the analysis of propositions and their elementary relations in On Interpretation, to the study of more complex forms, namely, syllogisms (in the Analytics) and dialectics (in the Topics and Sophistical Refutations). The first three treatises form the core of the logical theory stricto sensu: the grammar of the language of logic and the correct rules of reasoning. There is one volume of Aristotle's concerning logic not found in the Organon, namely the fourth book of Metaphysics.

Like his teacher Plato, Aristotle's philosophy aims at the universal. Aristotle's ontology, however, finds the universal in particular things, which he calls the essence of things, while in Plato's ontology, the universal exists apart from particular things, and is related to them as their prototype or . For Aristotle, therefore, epistemology is based on the study of particular phenomena and rises to the knowledge of essences, while for Plato epistemology begins with knowledge of universal Forms (or ideas) and descends to knowledge of particular imitations of these. For Aristotle, "form" still refers to the unconditional basis of phenomena but is "instantiated" in a particular substance (see Universals and particulars, below). In a certain sense, Aristotle's method is both inductive and deductive, while Plato's is essentially deductive from a priori principles.

In Aristotle's terminology, "natural philosophy" is a branch of philosophy examining the phenomena of the natural world, and includes fields that would be regarded today as physics, biology and other natural sciences. In modern times, the scope of philosophy has become limited to more generic or abstract inquiries, such as ethics and metaphysics, in which logic plays a major role. Today's philosophy tends to exclude empirical study of the natural world by means of the scientific method. In contrast, Aristotle's philosophical endeavors encompassed virtually all facets of intellectual inquiry.

In the larger sense of the word, Aristotle makes philosophy coextensive with reasoning, which he also would describe as "science". Note, however, that his use of the term science carries a different meaning than that covered by the term "scientific method". For Aristotle, "all science (dianoia) is either practical, poetical or theoretical" (Metaphysics 1025b25). By practical science, he means ethics and politics; by poetical science, he means the study of poetry and the other fine arts; by theoretical science, he means physics, mathematics and metaphysics.

If logic (or "analytics") is regarded as a study preliminary to philosophy, the divisions of Aristotelian philosophy would consist of: (1) Logic; (2) Theoretical Philosophy, including Metaphysics, Physics and Mathematics; (3) Practical Philosophy and (4) Poetical Philosophy.

In the period between his two stays in Athens, between his times at the Academy and the Lyceum, Aristotle conducted most of the scientific thinking and research for which he is renowned today. In fact, most of Aristotle's life was devoted to the study of the objects of natural science. Aristotle's metaphysics contains observations on the nature of numbers but he made no original contributions to mathematics. He did, however, perform original research in the natural sciences, e.g., botany, zoology, physics, astronomy, chemistry, meteorology, and several other sciences.

Aristotle's writings on science are largely qualitative, as opposed to quantitative. Beginning in the 16th century, scientists began applying mathematics to the physical sciences, and Aristotle's work in this area was deemed hopelessly inadequate. His failings were largely due to the absence of concepts like mass, velocity, force and temperature. He had a conception of speed and temperature, but no quantitative understanding of them, which was partly due to the absence of basic experimental devices, like clocks and thermometers.

His writings provide an account of many scientific observations, a mixture of precocious accuracy and curious errors. For example, in his History of Animals he claimed that human males have more teeth than females. In a similar vein, John Philoponus, and later Galileo, showed by simple experiments that Aristotle's theory that a heavier object falls faster than a lighter object is incorrect. On the other hand, Aristotle refuted Democritus's claim that the Milky Way was made up of "those stars which are shaded by the earth from the sun's rays," pointing out (correctly, even if such reasoning was bound to be dismissed for a long time) that, given "current astronomical demonstrations" that "the size of the sun is greater than that of the earth and the distance of the stars from the earth many times greater than that of the sun, then... the sun shines on all the stars and the earth screens none of them."

In places, Aristotle goes too far in deriving 'laws of the Universe' from simple observation and over-stretched reason. Today's scientific method assumes that such thinking without sufficient facts is ineffective, and that discerning the validity of one's hypothesis requires far more rigorous experimentation than that which Aristotle used to support his laws.

Aristotle also had some scientific blind spots. He posited a geocentric cosmology that we may discern in selections of the Metaphysics, which was widely accepted up until the 16th century. From the 3rd century to the 16th century, the dominant view held that the Earth was the rotational center of the universe.

Because he was perhaps the philosopher most respected by European thinkers during and after the Renaissance, these thinkers often took Aristotle's erroneous positions as given, which held back science in this epoch. However, Aristotle's scientific shortcomings should not mislead one into forgetting his great advances in the many scientific fields. For instance, he founded logic as a formal science and created foundations to biology that were not superseded for two millennia. Moreover, he introduced the fundamental notion that nature is composed of things that change and that studying such changes can provide useful knowledge of underlying constants.

As quoted from  Principles of Geology:
He  refers to many examples of changes now constantly going on, and insists emphatically on the great results which they must produce in the lapse of ages. He instances particular cases of lakes that had dried up, and deserts that had at length become watered by rivers and fertilized. He points to the growth of the Nilotic delta since the time of Homer, to the shallowing of the Palus Maeotis within sixty years from his own time... He alludes... to the upheaving of one of the Eolian islands, previous to a volcanic eruption. The changes of the earth, he says, are so slow in comparison to the duration of our lives, that they are overlooked; and the migrations of people after great catastrophes, and their removal to other regions, cause the event to be forgotten.

He says  'the distribution of land and sea in particular regions does not endure throughout all time, but it becomes sea in those parts where it was land, and again it becomes land where it was sea, and there is reason for thinking that these changes take place according to a certain system, and within a certain period.' The concluding observation is as follows: 'As time never fails, and the universe is eternal, neither the Tanais, nor the Nile, can have flowed for ever. The places where they rise were once dry, and there is a limit to their operations, but there is none to time. So also of all other rivers; they spring up and they perish; and the sea also continually deserts some lands and invades others The same tracts, therefore, of the earth are not some always sea, and others always continents, but every thing changes in the course of time.'


Aristotle proposed a fifth element, aether, in addition to the four proposed earlier by Empedocles.
Earth, which is cold and dry; this corresponds to the modern idea of a solid.
Water, which is cold and wet; this corresponds to the modern idea of a liquid.
Air, which is hot and wet; this corresponds to the modern idea of a gas.
Fire, which is hot and dry; this corresponds to the modern ideas of plasma and heat.
Aether, which is the divine substance that makes up the heavenly spheres and heavenly bodies (stars and planets).

Each of the four earthly elements has its natural place. All that is earthly tends toward the center of the Universe, i.e., the center of the Earth. Water tends toward a sphere surrounding the center. Air tends toward a sphere surrounding the water sphere. Fire tends toward the lunar sphere (in which the Moon orbits). When elements are moved out of their natural place, they naturally move back towards it. This is "natural motion"â€”motion requiring no extrinsic cause. So, for example, in water, earthy bodies sink while air bubbles rise up; in air, rain falls and flame rises.  Outside all the other spheres, the heavenly, fifth element, manifested in the stars and planets, moves in the perfection of circles.


Aristotle defined motion as the actuality of a potentiality as such. Aquinas suggested that the passage be understood literally; that motion can indeed be understood as the active fulfillment of a potential, as a transition toward a potentially possible state. Because actuality and potentiality are normally opposites in Aristotle, other commentators either suggest that the wording which has come down to us is erroneous, or that the addition of the "as such" to the definition is critical to understanding it.


Aristotle suggested that the reason for anything coming about can be attributed to four different types of simultaneously active causal factors:
Material cause describes the material out of which something is composed. Thus the material cause of a table is wood, and the material cause of a car is rubber and steel. It is not about action. It does not mean one domino knocks over another domino.
The formal cause is its form, i.e., the arrangement of that matter.  It tells us what a thing is, that any thing is determined by the definition, form, pattern, essence, whole, synthesis or archetype. It embraces the account of causes in terms of fundamental principles or general laws, as the whole (i.e., macrostructure) is the cause of its parts, a relationship known as the whole-part causation. Plainly put, the formal cause is the idea existing in the first place as exemplar in the mind of the sculptor, and in the second place as intrinsic, determining cause, embodied in the matter. Formal cause could only refer to the essential quality of causation. A simple example of the formal cause is the mental image or idea that allows an artist, architect, or engineer to create his drawings.
The efficient cause is "the primary source", or that from which the change under consideration proceeds. It identifies 'what makes of what is made and what causes change of what is changed' and so suggests all sorts of agents, nonliving or living, acting as the sources of change or movement or rest. Representing the current understanding of causality as the relation of cause and effect, this covers the modern definitions of "cause" as either the agent or agency or particular events or states of affairs. So, take the two dominoes, this time of equal weighting, the first is knocked over causing the second also to fall over.
The final cause is its purpose, or that for the sake of which a thing exists or is done, including both purposeful and instrumental actions and activities. The final cause or teleos is the purpose or function that something is supposed to serve. This covers modern ideas of motivating causes, such as volition, need, desire, ethics, or spiritual beliefs.

Additionally, things can be causes of one another, causing each other reciprocally, as hard work causes fitness and vice versa, although not in the same way or function, the one is as the beginning of change, the other as the goal. (Thus Aristotle first suggested a reciprocal or circular causality as a relation of mutual dependence or influence of cause upon effect). Moreover, Aristotle indicated that the same thing can be the cause of contrary effects; its presence and absence may result in different outcomes. Simply it is the goal or purpose that brings about an event. Our two dominoes require someone or something to intentionally knock over the first domino, because it cannot fall of its own accord.

Aristotle marked two modes of causation: proper (prior) causation and accidental (chance) causation. All causes, proper and incidental, can be spoken as potential or as actual, particular or generic. The same language refers to the effects of causes, so that generic effects assigned to generic causes, particular effects to particular causes, operating causes to actual effects. Essentially, causality does not suggest a temporal relation between the cause and the effect.

Aristotle held more accurate theories on some optical concepts than other philosophers of his day. The second oldest written evidence of a camera obscura (after Mozi c. 400 BC) can be found in Aristotle's documentation of such a device in 350BC in Problemata. Aristotle's apparatus contained a dark chamber that had a single small hole, or aperture, to allow for sunlight to enter. Aristotle used the device to make observations of the sun and noted that no matter what shape the hole was, the sun would still be correctly displayed as a round object. In modern cameras, this is analogous to the diaphragm. Aristotle also made the observation that when the distance between the aperture and the surface with the image increased, the image was magnified.

According to Aristotle, spontaneity and chance are causes of some things, distinguishable from other types of cause. Chance as an incidental cause lies in the realm of accidental things. It is "from what is spontaneous" (but note that what is spontaneous does not come from chance). For a better understanding of Aristotle's conception of "chance" it might be better to think of "coincidence": Something takes place by chance if a person sets out with the intent of having one thing take place, but with the result of another thing (not intended) taking place.

For example: A person seeks donations. That person may find another person willing to donate a substantial sum. However, if the person seeking the donations met the person donating, not for the purpose of collecting donations, but for some other purpose, Aristotle would call the collecting of the donation by that particular donator a result of chance. It must be unusual that something happens by chance. In other words, if something happens all or most of the time, we cannot say that it is by chance.

There is also more specific kind of chance, which Aristotle names "luck", that can only apply to human beings, because it is in the sphere of moral actions. According to Aristotle, luck must involve choice (and thus deliberation), and only humans are capable of deliberation and choice. "What is not capable of action cannot do anything by chance".

Aristotle defines metaphysics as "the knowledge of immaterial being," or of "being in the highest degree of abstraction." He refers to metaphysics as "first philosophy", as well as "the theologic science."


Aristotle examines the concepts of substance and essence (ousia) in his Metaphysics (Book VII), and he concludes that a particular substance is a combination of both matter and form. In book VIII, he distinguishes the matter of the substance as the substratum, or the stuff of which it is composed. For example, the matter of a house is the bricks, stones, timbers etc., or whatever constitutes the potential house, while the form of the substance is the actual house, namely 'covering for bodies and chattels' or any other differentia (see also predicables) that let us define something as a house. The formula that gives the components is the account of the matter, and the formula that gives the differentia is the account of the form.

With regard to the change (kinesis) and its causes now, as he defines in his Physics and On Generation and Corruption 319bâ€“320a, he distinguishes the coming to be from:
growth and diminution, which is change in quantity;
locomotion, which is change in space; and
alteration, which is change in quality.

The coming to be is a change where nothing persists of which the resultant is a property. In that particular change he introduces the concept of potentiality (dynamis) and actuality (entelecheia) in association with the matter and the form.

Referring to potentiality, this is what a thing is capable of doing, or being acted upon, if the conditions are right and it is not prevented by something else. For example, the seed of a plant in the soil is potentially (dynamei) plant, and if is not prevented by something, it will become a plant. Potentially beings can either 'act' (poiein) or 'be acted upon' (paschein), which can be either innate or learned. For example, the eyes possess the potentiality of sight (innateâ€“ being acted upon), while the capability of playing the flute can be possessed by learning (exerciseâ€“ acting).

Actuality is the fulfillment of the end of the potentiality. Because the end (telos) is the principle of every change, and for the sake of the end exists potentiality, therefore actuality is the end. Referring then to our previous example, we could say that an actuality is when a plant does one of the activities that plants do.

"For that for the sake of which a thing is, is its principle, and the becoming is for the sake of the end; and the actuality is the end, and it is for the sake of this that the potentiality is acquired. For animals do not see in order that they may have sight, but they have sight that they may see."

In summary, the matter used to make a house has potentiality to be a house and both the activity of building and the form of the final house are actualities, which is also a final cause or end. Then Aristotle proceeds and concludes that the actuality is prior to potentiality in formula, in time and in substantiality.

With this definition of the particular substance (i.e., matter and form), Aristotle tries to solve the problem of the unity of the beings, for example, "what is it that makes a man one"? Since, according to Plato there are two Ideas: animal and biped, how then is man a unity? However, according to Aristotle, the potential being (matter) and the actual one (form) are one and the same thing.


Aristotle's predecessor, Plato, argued that all things have a universal form, which could be either a property, or a relation to other things. When we look at an apple, for example, we see an apple, and we can also analyze a form of an apple. In this distinction, there is a particular apple and a universal form of an apple. Moreover, we can place an apple next to a book, so that we can speak of both the book and apple as being next to each other.

Plato argued that there are some universal forms that are not a part of particular things. For example, it is possible that there is no particular good in existence, but "good" is still a proper universal form. Bertrand Russell is a 20th-century philosopher who agreed with Plato on the existence of "uninstantiated universals".

Aristotle disagreed with Plato on this point, arguing that all universals are instantiated. Aristotle argued that there are no universals that are unattached to existing things. According to Aristotle, if a universal exists, either as a particular or a relation, then there must have been, must be currently, or must be in the future, something on which the universal can be predicated. Consequently, according to Aristotle, if it is not the case that some universal can be predicated to an object that exists at some period of time, then it does not exist.

In addition, Aristotle disagreed with Plato about the location of universals. As Plato spoke of the world of the forms, a location where all universal forms subsist, Aristotle maintained that universals exist within each thing on which each universal is predicated. So, according to Aristotle, the form of apple exists within each apple, rather than in the world of the forms.

In Aristotelian science, especially in biology, things he saw himself have stood the test of time better than his retelling of the reports of others, which contain error and superstition. He dissected animals but not humans; his ideas on how the human body works have been almost entirely superseded.

Aristotle is the earliest natural historian whose work has survived in some detail. Aristotle certainly did research on the natural history of Lesbos, and the surrounding seas and neighbouring areas. The works that reflect this research, such as History of Animals, Generation of Animals, and Parts of Animals, contain some observations and interpretations, along with sundry myths and mistakes. The most striking passages are about the sea-life visible from observation on Lesbos and available from the catches of fishermen. His observations on catfish, electric fish (Torpedo) and angler-fish are detailed, as is his writing on cephalopods, namely, Octopus, Sepia (cuttlefish) and the paper nautilus (Argonauta argo). His description of the hectocotyl arm, used in sexual reproduction, was widely disbelieved until its rediscovery in the 19th century. He separated the aquatic mammals from fish, and knew that sharks and rays were part of the group he called SelachÄ“ (selachians).

Another good example of his methods comes from the Generation of Animals in which Aristotle describes breaking open fertilized chicken eggs at intervals to observe when visible organs were generated.

He gave accurate descriptions of ruminants' four-chambered fore-stomachs, and of the ovoviviparous embryological development of the hound shark Mustelus mustelus.

Aristotle distinguished about 500 species of birds, mammals and fishes. His classification of living things contains some elements which still existed in the 19th century. What the modern zoologist would call vertebrates and invertebrates, Aristotle called 'animals with blood' and 'animals without blood' (he did not know that complex invertebrates do make use of hemoglobin, but of a different kind from vertebrates). Animals with blood were divided into live-bearing (mammals), and egg-bearing (birds and fish). Invertebrates ('animals without blood') are insects, crustacea (divided into non-shelled â€“ cephalopods â€“ and shelled) and testacea (molluscs). In some respects, this incomplete classification is better than that of Linnaeus, who crowded the invertebrata together into two groups, Insecta and Vermes (worms).

For Charles Singer, "Nothing is more remarkable than  efforts to  the relationships of living things as a scala naturae" Aristotle's History of Animals classified organisms in relation to a hierarchical "Ladder of Life" (scala naturae  or Great Chain of Being), placing them according to complexity of structure and function so that higher organisms showed greater vitality and ability to move.

Aristotle believed that intellectual purposes, i.e., final causes, guided all natural processes. Such a teleological view gave Aristotle cause to justify his observed data as an expression of formal design. Noting that "no animal has, at the same time, both tusks and horns," and "a single-hooved animal with two horns I have never seen," Aristotle suggested that Nature, giving no animal both horns and tusks, was staving off vanity, and giving creatures faculties only to such a degree as they are necessary. Noting that ruminants had multiple stomachs and weak teeth, he supposed the first was to compensate for the latter, with Nature trying to preserve a type of balance.

In a similar fashion, Aristotle believed that creatures were arranged in a graded scale of perfection rising from plants on up to man, the scala naturae. His system had eleven grades, arranged according "to the degree to which they are infected with potentiality", expressed in their form at birth. The highest animals laid warm and wet creatures alive, the lowest bore theirs cold, dry, and in thick eggs.

Aristotle also held that the level of a creature's perfection was reflected in its form, but not preordained by that form. Ideas like this, and his ideas about souls, are not regarded as science at all in modern times.

He placed emphasis on the type(s) of soul an organism possessed, asserting that plants possess a vegetative soul, responsible for reproduction and growth, animals a vegetative and a sensitive soul, responsible for mobility and sensation, and humans a vegetative, a sensitive, and a rational soul, capable of thought and reflection.

Aristotle, in contrast to earlier philosophers, but in accordance with the Egyptians, placed the rational soul in the heart, rather than the brain. Notable is Aristotle's division of sensation and thought, which generally went against previous philosophers, with the exception of Alcmaeon.

Aristotle's successor at the Lyceum, Theophrastus, wrote a series of books on botanyâ€”the History of Plantsâ€”which survived as the most important contribution of antiquity to botany, even into the Middle Ages. Many of Theophrastus' names survive into modern times, such as carpos for fruit, and pericarpion for seed vessel.

Rather than focus on formal causes, as Aristotle did, Theophrastus suggested a mechanistic scheme, drawing analogies between natural and artificial processes, and relying on Aristotle's concept of the efficient cause. Theophrastus also recognized the role of sex in the reproduction of some higher plants, though this last discovery was lost in later ages.


After Theophrastus, the Lyceum failed to produce any original work. Though interest in Aristotle's ideas survived, they were generally taken unquestioningly. It is not until the age of Alexandria under the Ptolemies that advances in biology can be again found.

The first medical teacher at Alexandria, Herophilus of Chalcedon, corrected Aristotle, placing intelligence in the brain, and connected the nervous system to motion and sensation. Herophilus also distinguished between veins and arteries, noting that the latter pulse while the former do not. Though a few ancient atomists such as Lucretius challenged the teleological viewpoint of Aristotelian ideas about life, teleology (and after the rise of Christianity, natural theology) would remain central to biological thought essentially until the 18th and 19th centuries. Ernst Mayr claimed that there was "nothing of any real consequence in biology after Lucretius and Galen until the Renaissance." Aristotle's ideas of natural history and medicine survived, but they were generally taken unquestioningly.

Aristotle's psychology, given in his treatise On the Soul (peri psyche, often known by its Latin title De Anima), posits three kinds of soul ("psyches"): the vegetative soul, the sensitive soul, and the rational soul. Humans have a rational soul. This kind of soul is capable of the same powers as the other kinds: Like the vegetative soul it can grow and nourish itself; like the sensitive soul it can experience sensations and move locally. The unique part of the human, rational soul is its ability to receive forms of other things and compare them.

For Aristotle, the soul (psyche) was a simpler concept than it is for us today. By soul he simply meant the form of a living being. Because all beings are composites of form and matter, the form of living beings is that which endows them with what is specific to living beings, e.g. the ability to initiate movement (or in the case of plants, growth and chemical transformations, which Aristotle considers types of movement).


According to Aristotle, memory is the ability to hold a perceived experience in your mind and to have the ability to distinguish between the internal "appearance" and an occurrence in the past. In other words, a memory is a mental picture () in which Aristotle defines in De Anima, as an appearance which is imprinted on the part of the body that forms a memory. Aristotle believed an "imprint" becomes impressed on a semi-fluid bodily organ that undergoes several changes in order to make a memory. A memory occurs when a stimuli is too complex that the nervous system (semi-fluid bodily organ) cannot receive all the impressions at once. These changes are the same as those involved in the operations of sensation, common sense, and thinking . The mental picture imprinted on the bodily organ is the final product of the entire process of sense perception. It does not matter if the experience was seen or heard, every experience ends up as a mental image in memory
Aristotle uses the word "memory" for two basic abilities. First, the actual retaining of the experience in the mnemonic "imprint" that can develop from sensation. Second, the intellectual anxiety that comes with the "imprint" due to being impressed at a particular time and processing specific contents. These abilities can be explained as memory is neither sensation nor thinking because is arises only after a lapse of time. Therefore, memory is of the past,  prediction is of the future, and sensation is of the present. The retrieval of our "imprints" cannot be performed suddenly. A transitional channel is needed and located in our past experiences, both for our previous experience and present experience.

Aristotle proposed that slow-witted people have good memory because the fluids in their brain do not wash away their memory organ used to imprint experiences and so the "imprint" can easily continue. However, they cannot be too slow or the hardened surface of the organ will not receive new "imprints". He believed the young and the old do not properly develop an "imprint". Young people undergo rapid changes as they develop, while the elderly's organs are beginning to decay, thus stunting new "imprints". Likewise, people who are too quick-witted are similar to the young and the image cannot be fixed because of the rapid changes of their organ. Because intellectual functions are not involved in memory, memories belong to some animals too, but only those in which have perception of time.

Because Aristotle believes people receive all kinds of sense perceptions and people perceive them as images or "imprints", people are continually weaving together new "imprints" of things they experience. In order to search for these "imprints", people search the memory itself. Within the memory, if one experience is offered instead of a specific memory, that person will reject this experience until they find what they are looking for. Recollection occurs when one experience naturally follows another. If the chain of "images" is needed, one memory will stimulate the other. If the chain of "images" is not needed, but expected, then it will only stimulate the other memory in most instances. When people recall experiences, they stimulate certain previous experiences until they have stimulated the one that was needed.

Recollection is the self-directed activity of retrieving the information stored in a memory "imprint" after some time has passed. Retrieval of stored information is dependent on the scope of mnemonic capabilities of a being (human or animal) and the abilities the human or animal possesses . Only humans will remember "imprints" of intellectual activity, such as numbers and words. Animals that have perception of time will be able to retrieve memories of their past observations. Remembering involves only perception of the things remembered and of the time passed. Recollection of an "imprint" is when the present experiences a person remembers are similar with elements corresponding in character and arrangement of past sensory experiences. When an "imprint" is recalled, it may bring forth a large group of related "imprints".

Aristotle believed the chain of thought, which ends in recollection of certain "imprints", was connected systematically in three sorts of relationships: similarity, contrast, and contiguity. These three laws make up his Laws of Association. Aristotle believed that past experiences are hidden within our mind. A force operates to awaken the hidden material to bring up the actual experience. According to Aristotle, association is the power innate in a mental state, which operates upon the unexpressed remains of former experiences, allowing them to rise and be recalled.


Before understanding Aristotleâ€™s take on dreams, first his idea of sleep must be examined. Aristotle gives an account of his explanation of sleep in On Sleep and Wakefulness.  Sleep takes place as a result of overuse of the senses  or of digestion,  so it is vital to the body, including the senses, so it can be revitalized. While a person is asleep, the critical activities, which include thinking, sensing, recalling and remembering, do not function  as they do during wakefulness.  Since a person cannot sense during sleep they can also not have a desire, which is the result of a sensation. However, the senses are able to work during sleep, albeit differently than when a person is awake because during sleep a person can still have sensory experiences. Also, all of the senses are not inactive during sleep, only the ones that are weary.

Dreams do not involve actually sensing a stimulus because, as discussed, the senses do not work as they normally do during sleep. In dreams, sensation is still involved, but in an altered manner than when awake. Aristotle explains the phenomenon that occurs when a person stares at a moving stimulus such as the waves in a body of water. When they look away from that stimulus, the next thing they look at appears to be moving in a wave like motion.  When a person perceives a stimulus and the stimulus is no longer the focus of their attention, it leaves an impression.  When the body is awake and the senses are functioning properly, a person constantly encounters new stimuli to sense and so the impressions left from previously perceived stimuli become irrelevant. However, during sleep the impressions stimuli made throughout the day become noticed because there are not new sensory experiences to distract from these impressions that were made.  So, dreams result from these lasting impressions. Since impressions are all that are left and not the exact stimuli, dreams will not resemble the actual experience that occurred when awake.

During sleep, a person is in an altered state of mind. Aristotle compares a sleeping person to a person who is overtaken by strong feelings toward a stimulus. For example, a person who has a strong infatuation with someone may begin to think they see that person everywhere because they are so overtaken by their feelings. When a person is asleep, their senses are not acting as they do when they are awake and this results in them thinking like a person who is influenced by strong feelings. Since a person sleeping is in this suggestible state, they become easily deceived by what appears in their dreams.

When asleep, a person is unable to make judgments as they do when they are awake  Due to the senses not functioning normally during sleep, they are unable to help a person judge what is happening in their dream. This in turn leads the person to believe the dream is real. Dreams may be absurd in nature but the senses are not able to discern whether they are real or not. So, the dreamer is left to accept the dream because they lack the choice to judge it.

One component of Aristotleâ€™s theory of dreams introduces ideas that are contradictory to previously held beliefs. He claimed that dreams are not foretelling and that they are not sent by a divine being. Aristotle reasoned that instances in which dreams do resemble future events are happenstances not divinations. These ideas were contradictory to what had been believed about dreams, but at the time in which he introduced these ideas more thinkers were beginning to give naturalistic as opposed to supernatural explanations to phenomena.

Aristotle also includes in his theory of dreams what constitutes a dream and what does not. He claimed that a dream is first established by the fact that the person is asleep when they experience it. If a person had an image appear for a moment after waking up or if they see something in the dark it is not considered a dream because they were awake when it occurred. Secondly, any sensory experience that actually occurs while a person is asleep and is perceived by the person while asleep does not qualify as part of a dream. For example, if, while a person is sleeping, a door shuts and in their dream they hear a door is shut, Aristotle argues that this sensory experience is not part of the dream. The actual sensory experience is perceived by the senses, the fact that it occurred while the person was asleep does not make it part of the dream. Lastly, the images of dreams must be a result of lasting impressions of sensory experiences had when awake.



Aristotle considered ethics to be a practical rather than theoretical study, i.e., one aimed at becoming good and doing good rather than knowing for its own sake. He wrote several treatises on ethics, including most notably, the Nicomachean Ethics.

Aristotle taught that virtue has to do with the proper function (ergon) of a thing. An eye is only a good eye in so much as it can see, because the proper function of an eye is sight. Aristotle reasoned that humans must have a function specific to humans, and that this function must be an activity of the psuchÄ“ (normally translated as soul) in accordance with reason (logos). Aristotle identified such an optimum activity of the soul as the aim of all human deliberate action, eudaimonia, generally translated as "happiness" or sometimes "well being". To have the potential of ever being happy in this way necessarily requires a good character (Ä“thikÄ“ aretÄ“), often translated as moral (or ethical) virtue (or excellence).

Aristotle taught that to achieve a virtuous and potentially happy character requires a first stage of having the fortune to be habituated not deliberately, but by teachers, and experience, leading to a later stage in which one consciously chooses to do the best things. When the best people come to live life this way their practical wisdom (phronesis) and their intellect (nous) can develop with each other towards the highest possible human virtue, the wisdom of an accomplished theoretical or speculative thinker, or in other words, a philosopher.

In addition to his works on ethics, which address the individual, Aristotle addressed the city in his work titled Politics. Aristotle considered the city to be a natural community. Moreover, he considered the city to be prior in importance to the family which in turn is prior to the individual, "for the whole must of necessity be prior to the part". He also famously stated that "man is by nature a political animal". Aristotle conceived of politics as being like an organism rather than like a machine, and as a collection of parts none of which can exist without the others. Aristotle's conception of the city is organic, and he is considered one of the first to conceive of the city in this manner.

The common modern understanding of a political community as a modern state is quite different from Aristotle's understanding. Although he was aware of the existence and potential of larger empires, the natural community according to Aristotle was the city (polis) which functions as a political "community" or "partnership" (koinÅnia). The aim of the city is not just to avoid injustice or for economic stability, but rather to allow at least some citizens the possibility to live a good life, and to perform beautiful acts: "The political partnership must be regarded, therefore, as being for the sake of noble actions, not for the sake of living together." This is distinguished from modern approaches, beginning with social contract theory, according to which individuals leave the state of nature because of "fear of violent death" or its "inconveniences."

Excerpt from a speech by the character â€˜Aristotleâ€™ in the book Protrepticus (Hutchinson and Johnson, 2015 p.22)
For we all agree that the most excellent man should rule, i.e., the supreme by nature, and that the law rules and alone is authoritative; but the law is a kind of intelligence, i.e. a discourse based on intelligence. And again, what standard do we have, what criterion of good things, that is more precise than the intelligent man? For all that this man will choose, if the choice is based on his knowledge, are good things and their contraries are bad. And since everybody chooses most of all what conforms to their own proper dispositions (a just man choosing to live justly, a man with bravery to live bravely, likewise a self-controlled man to live with self-control), it is clear that the intelligent man will choose most of all to be intelligent; for this is the function of that capacity. Hence itâ€™s evident that, according to the most authoritative judgment, intelligence is supreme among goods.


Aristotle considered epic poetry, tragedy, comedy, dithyrambic poetry and music to be imitative, each varying in imitation by medium, object, and manner. For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language. The forms also differ in their object of imitation. Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average. Lastly, the forms differ in their manner of imitation â€“ through narrative or character, through change or no change, and through drama or no drama. Aristotle believed that imitation is natural to mankind and constitutes one of mankind's advantages over animals.

While it is believed that Aristotle's Poetics comprised two books â€“ one on comedy and one on tragedy â€“ only the portion that focuses on tragedy has survived. Aristotle taught that tragedy is composed of six elements: plot-structure, character, style, thought, spectacle, and lyric poetry. The characters in a tragedy are merely a means of driving the story; and the plot, not the characters, is the chief focus of tragedy. Tragedy is the imitation of action arousing pity and fear, and is meant to effect the catharsis of those same emotions. Aristotle concludes Poetics with a discussion on which, if either, is superior: epic or tragic mimesis. He suggests that because tragedy possesses all the attributes of an epic, possibly possesses additional attributes such as spectacle and music, is more unified, and achieves the aim of its mimesis in shorter scope, it can be considered superior to epic.

Aristotle was a keen systematic collector of riddles, folklore, and proverbs; he and his school had a special interest in the riddles of the Delphic Oracle and studied the fables of Aesop.


Aristotle's analysis of procreation describes an active, ensouling masculine element bringing life to an inert, passive female element. On this ground, feminist metaphysics have accused Aristotle of misogyny and sexism. However, Aristotle gave equal weight to women's happiness as he did to men's, and commented in his Rhetoric that the things that lead to happiness need to be in women as well as men.

Modern scholarship reveals that Aristotle's "lost" works stray considerably in characterization from the surviving Aristotelian corpus. Whereas the lost works appear to have been originally written with an intent for subsequent publication, the surviving works do not appear to have been so. Rather the surviving works mostly resemble lecture notes unintended for publication. The authenticity of a portion of the surviving works as originally Aristotelian is also today held suspect, with some books duplicating or summarizing each other, the authorship of one book questioned and another book considered to be unlikely Aristotle's at all.

Some of the individual works within the corpus, including the Constitution of Athens, are regarded by most scholars as products of Aristotle's "school," perhaps compiled under his direction or supervision. Others, such as On Colors, may have been produced by Aristotle's successors at the Lyceum, e.g., Theophrastus and Straton. Still others acquired Aristotle's name through similarities in doctrine or content, such as the De Plantis, possibly by Nicolaus of Damascus. Other works in the corpus include medieval palmistries and astrological and magical texts whose connections to Aristotle are purely fanciful and self-promotional.

According to a distinction that originates with Aristotle himself, his writings are divisible into two groups: the "exoteric" and the "esoteric". Most scholars have understood this as a distinction between works Aristotle intended for the public (exoteric), and the more technical works intended for use within the Lyceum course / school (esoteric). Modern scholars commonly assume these latter to be Aristotle's own (unpolished) lecture notes (or in some cases possible notes by his students). However, one classic scholar offers an alternative interpretation.  The 5th century neoplatonist Ammonius Hermiae writes that Aristotle's writing style is deliberately obscurantist so that "good people may for that reason stretch their mind even more, whereas empty minds that are lost through carelessness will be put to flight by the obscurity when they encounter sentences like these."

Another common assumption is that none of the exoteric works is extant â€“ that all of Aristotle's extant writings are of the esoteric kind. Current knowledge of what exactly the exoteric writings were like is scant and dubious, though many of them may have been in dialogue form. (Fragments of some of Aristotle's dialogues have survived.) Perhaps it is to these that Cicero refers when he characterized Aristotle's writing style as "a river of gold"; it is hard for many modern readers to accept that one could seriously so admire the style of those works currently available to us. However, some modern scholars have warned that we cannot know for certain that Cicero's praise was reserved specifically for the exoteric works; a few modern scholars have actually admired the concise writing style found in Aristotle's extant works.

One major question in the history of Aristotle's works, then, is how were the exoteric writings all lost, and how did the ones we now possess come to us The story of the original manuscripts of the esoteric treatises is described by Strabo in his Geography and Plutarch in his Parallel Lives. The manuscripts were left from Aristotle to his successor Theophrastus, who in turn willed them to Neleus of Scepsis. Neleus supposedly took the writings from Athens to Scepsis, where his heirs let them languish in a cellar until the 1st century BC, when Apellicon of Teos discovered and purchased the manuscripts, bringing them back to Athens. According to the story, Apellicon tried to repair some of the damage that was done during the manuscripts' stay in the basement, introducing a number of errors into the text. When Lucius Cornelius Sulla occupied Athens in 86 BC, he carried off the library of Apellicon to Rome, where they were first published in 60 BC by the grammarian Tyrannion of Amisus and then by the philosopher Andronicus of Rhodes.

Carnes Lord attributes the popular belief in this story to the fact that it provides "the most plausible explanation for the rapid eclipse of the Peripatetic school after the middle of the third century, and for the absence of widespread knowledge of the specialized treatises of Aristotle throughout the Hellenistic period, as well as for the sudden reappearance of a flourishing Aristotelianism during the first century B.C." Lord voices a number of reservations concerning this story, however. First, the condition of the texts is far too good for them to have suffered considerable damage followed by Apellicon's inexpert attempt at repair.

Second, there is "incontrovertible evidence," Lord says, that the treatises were in circulation during the time in which Strabo and Plutarch suggest they were confined within the cellar in Scepsis. Third, the definitive edition of Aristotle's texts seems to have been made in Athens some fifty years before Andronicus supposedly compiled his. And fourth, ancient library catalogues predating Andronicus' intervention list an Aristotelian corpus quite similar to the one we currently possess. Lord sees a number of post-Aristotelian interpolations in the Politics, for example, but is generally confident that the work has come down to us relatively intact.

On the one hand, the surviving texts of Aristotle do not derive from finished literary texts, but rather from working drafts used within Aristotle's school, as opposed, on the other hand, to the dialogues and other "exoteric" texts which Aristotle published more widely during his lifetime. The consensus is that Andronicus of Rhodes collected the esoteric works of Aristotle's school which existed in the form of smaller, separate works, distinguished them from those of Theophrastus and other Peripatetics, edited them, and finally compiled them into the more cohesive, larger works as they are known today.

More than 2300 years after his death, Aristotle remains one of the most influential people who ever lived. He contributed to almost every field of human knowledge then in existence, and he was the founder of many new fields. According to the philosopher Bryan Magee, "it is doubtful whether any human being has ever known as much as he did". Among countless other achievements, Aristotle was the founder of formal logic, pioneered the study of zoology, and left every future scientist and philosopher in his debt through his contributions to the scientific method.

Despite these achievements, the influence of Aristotle's errors is considered by some to have held back science considerably. Bertrand Russell notes that "almost every serious intellectual advance has had to begin with an attack on some Aristotelian doctrine". Russell also refers to Aristotle's ethics as "repulsive", and calls his logic "as definitely antiquated as Ptolemaic astronomy". Russell notes that these errors make it difficult to do historical justice to Aristotle, until one remembers how large of an advance he made upon all of his predecessors.

The immediate influence of Aristotle's work was felt as the Lyceum grew into the Peripatetic school. Aristotle's notable students included Aristoxenus, Dicaearchus, Demetrius of Phalerum, Eudemos of Rhodes, Harpalus, Hephaestion, Meno, Mnason of Phocis, Nicomachus, and Theophrastus. Aristotle's influence over Alexander the Great is seen in the latter's bringing with him on his expedition a host of zoologists, botanists, and researchers. He had also learned a great deal about Persian customs and traditions from his teacher. Although his respect for Aristotle was diminished as his travels made it clear that much of Aristotle's geography was clearly wrong, when the old philosopher released his works to the public, Alexander complained "Thou hast not done well to publish thy acroamatic doctrines; for in what shall I surpass other men if those doctrines wherein I have been trained are to be all men's common property?"

Greek Christian scribes played a crucial role in the preservation of Aristotle by copying all the extant Greek language manuscripts of the corpus. The first Greek Christians to comment extensively on Aristotle were John Philoponus, Elias, and David in the sixth century, and Stephen of Alexandria in the early seventh century. John Philoponus stands out for having attempted a fundamental critique of Aristotle's views on the eternity of the world, movement, and other elements of Aristotelian thought. After a hiatus of several centuries, formal commentary by Eustratius and Michael of Ephesus reappears in the late eleventh and early twelfth centuries, apparently sponsored by Anna Comnena.

Aristotle was one of the most revered Western thinkers in early Islamic theology. Most of the still extant works of Aristotle, as well as a number of the original Greek commentaries, were translated into Arabic and studied by Muslim philosophers, scientists and scholars.  Averroes, Avicenna and Alpharabius, who wrote on Aristotle in great depth, also influenced Thomas Aquinas and other Western Christian scholastic philosophers. Alkindus considered Aristotle as the outstanding and unique representative of philosophy and Averroes spoke of Aristotle as the "exemplar" for all future philosophers. Medieval Muslim scholars regularly described Aristotle as the "First Teacher". The title "teacher" was first given to Aristotle by Muslim scholars, and was later used by Western philosophers (as in the famous poem of Dante) who were influenced by the tradition of Islamic philosophy.

In accordance with the Greek theorists, the Muslims considered Aristotle to be a dogmatic philosopher, the author of a closed system, and believed that Aristotle shared with Plato essential tenets of thought. Some went so far as to credit Aristotle himself with neo-Platonic metaphysical ideas.

With the loss of the study of ancient Greek in the early medieval Latin West, Aristotle was practically unknown there from c. AD 600 to c. 1100 except through the Latin translation of the Organon made by Boethius. In the twelfth and thirteenth centuries, interest in Aristotle revived and Latin Christians had translations made, both from Arabic translations, such as those by Gerard of Cremona, and from the original Greek, such as those by James of Venice and William of Moerbeke.

After Thomas Aquinas wrote his theology, working from Moerbeke's translations, the demand for Aristotle's writings grew and the Greek manuscripts returned to the West, stimulating a revival of Aristotelianism in Europe that continued into the Renaissance.
Aristotle is referred to as "The Philosopher" by Scholastic thinkers such as Thomas Aquinas. See Summa Theologica, Part I, Question 3, etc. These thinkers blended Aristotelian philosophy with Christianity, bringing the thought of Ancient Greece into the Middle Ages. It required a repudiation of some Aristotelian principles for the sciences and the arts to free themselves for the discovery of modern scientific laws and empirical methods. The medieval English poet Chaucer describes his student as being happy by having
 at his beddes heed
Twenty bookes, clad in blak or reed,
Of aristotle and his philosophie,
The Italian poet Dante says of Aristotle in the first circles of hell,
I saw the Master there of those who know,
Amid the philosophic family,
By all admired, and by all reverenced;
There Plato too I saw, and Socrates,
Who stood beside him closer than the rest.

The German philosopher Friedrich Nietzsche has been said to have taken nearly all of his political philosophy from Aristotle. However implausible this is, it is certainly the case that Aristotle's rigid separation of action from production, and his justification of the subservience of slaves and others to the virtue â€“ or arete â€“ of a few justified the ideal of aristocracy. It is Martin Heidegger, not Nietzsche, who elaborated a new interpretation of Aristotle, intended to warrant his deconstruction of scholastic and philosophical tradition. Ayn Rand accredited Aristotle as "the greatest philosopher in history" and cited him as a major influence on her thinking. More recently, Alasdair MacIntyre has attempted to reform what he calls the Aristotelian tradition in a way that is anti-elitist and capable of disputing the claims of both liberals and Nietzscheans.

The works of Aristotle that have survived from antiquity through medieval manuscript transmission are collected in the Corpus Aristotelicum. These texts, as opposed to Aristotle's lost works, are technical philosophical treatises from within Aristotle's school. Reference to them is made according to the organization of Immanuel Bekker's Royal Prussian Academy edition (Aristotelis Opera edidit Academia Regia Borussica, Berlin, 1831â€“1870), which in turn is based on ancient classifications of these works.


The Aristotle Mountains along the Oscar II Coast of Graham Land, Antarctica, are named after Aristotle. He was the first person known to conjecture the existence of a landmass in the southern high-latitude region and call it "Antarctica".

Aristoteles (crater) is a crater on the Moon bearing the classical form of Aristotle's name.

Aristotelian physics
Aristotelian society
Aristotelian theology
Conimbricenses
List of writers influenced by Aristotle
Otium
Philia
Pseudo-Aristotle

The secondary literature on Aristotle is vast. The following references are only a small selection.

Ackrill J. L. (1997). Essays on Plato and Aristotle, Oxford University Press, USA.

 A popular exposition for the general reader.

 These translations are available in several places online; see External links.
Bakalis Nikolaos. (2005). Handbook of Greek Philosophy: From Thales to the Stoics Analysis and Fragments, Trafford Publishing ISBN 1-4120-4843-5
Barnes J. (1995). The Cambridge Companion to Aristotle, Cambridge University Press.

Bolotin, David (1998). An Approach to Aristotle's Physics: With Particular Attention to the Role of His Manner of Writing. Albany: SUNY Press. A contribution to our understanding of how to read Aristotle's scientific works.
Burnyeat, M. F. et al. (1979). Notes on Book Zeta of Aristotle's Metaphysics. Oxford: Sub-faculty of Philosophy.

Chappell, V. (1973). Aristotle's Conception of Matter, Journal of Philosophy 70: 679â€“696.
Code, Alan. (1995). Potentiality in Aristotle's Science and Metaphysics, Pacific Philosophical Quarterly 76.

De Groot, Jean (2014). Aristotle's Empiricism: Experience and Mechanics in the 4th Century BC, Parmenides Publishing, ISBN 978-1-930972-83-4 
Frede, Michael. (1987). Essays in Ancient Philosophy. Minneapolis: University of Minnesota Press.

Gendlin, Eugene T. (2012). Line by Line Commentary on Aristotle's De Anima, Volume 1: Books I & II; Volume 2: Book III. Spring Valley, New York: The Focusing Institute. 
Gill, Mary Louise. (1989). Aristotle on Substance: The Paradox of Unity. Princeton: Princeton University Press.

Halper, Edward C. (2009). One and Many in Aristotle's Metaphysics, Volume 1: Books Alpha â€“ Delta, Parmenides Publishing, ISBN 978-1-930972-21-6.
Halper, Edward C. (2005). One and Many in Aristotle's Metaphysics, Volume 2: The Central Books, Parmenides Publishing, ISBN 978-1-930972-05-6.
Irwin, T. H. (1988). . Oxford: Clarendon Press, ISBN 0-19-824290-5.

Jori, Alberto. (2003). Aristotele, Milano: Bruno Mondadori Editore (Prize 2003 of the "International Academy of the History of Science") ISBN 88-424-9737-1.

Knight, Kelvin. (2007). Aristotelian Philosophy: Ethics and Politics from Aristotle to MacIntyre, Polity Press.
Lewis, Frank A. (1991). Substance and Predication in Aristotle. Cambridge: Cambridge University Press.
Lloyd, G. E. R. (1968). Aristotle: The Growth and Structure of his Thought. Cambridge: Cambridge Univ. Pr., ISBN 0-521-09456-9.
Lord, Carnes. (1984). Introduction to The Politics, by Aristotle. Chicago: Chicago University Press.
Loux, Michael J. (1991). Primary Ousia: An Essay on Aristotle's Metaphysics Î– and Î—. Ithaca, NY: Cornell University Press.
Maso, Stefano (Ed.), Natali, Carlo (Ed.), Seel, Gerhard (Ed.). (2012) Reading Aristotle: Physics VII.3: What is Alteration? Proceedings of the International ESAP-HYELE Conference, Parmenides Publishing. ISBN 978-1-930972-73-5      
Pangle, Lorraine Smith (2003). Aristotle and the Philosophy of Friendship. Cambridge: Cambridge University Press. Aristotle's conception of the deepest human relationship viewed in the light of the history of philosophic thought on friendship.

Reeve, C. D. C. (2000). Substantial Knowledge: Aristotle's Metaphysics. Indianapolis: Hackett.

 A classic overview by one of Aristotle's most prominent English translators, in print since 1923.
Scaltsas, T. (1994). Substances and Universals in Aristotle's Metaphysics. Ithaca: Cornell University Press.
Strauss, Leo (1964). "On Aristotle's Politics", in The City and Man, Chicago; Rand McNally.
 For the general reader.
.
.
At the Internet Encyclopedia of Philosophy:

From the Stanford Encyclopedia of Philosophy:
.

.

Collections of works
At the  (primarily in English).
   at Tufts University.
At the  (primarily in English).
  
The 11-volume 1837 Bekker edition of Aristotle's Works in Greek ()
Bekker's Prussian Academy of Sciences edition of the complete works of Aristotle at Archive.org: 
  (translation).


An American in Paris is a jazz-influenced symphonic poem by the American composer George Gershwin, written in 1928. Inspired by the time Gershwin had spent in Paris, it evokes the sights and energy of the French capital in the 1920s and is one of his best-known compositions.

Gershwin composed An American in Paris on commission from the conductor Walter Damrosch. He scored the piece for the standard instruments of the symphony orchestra plus celesta, saxophones, and automobile horns. He brought back some Parisian taxi horns for the New York premiere of the composition, which took place on December13, 1928 in Carnegie Hall, with Damrosch conducting the New York Philharmonic. Gershwin completed the orchestration on November 18, less than four weeks before the work's premiere.

Gershwin collaborated on the original program notes with the critic and composer Deems Taylor, noting that: "My purpose here is to portray the impression of an American visitor in Paris as he strolls about the city and listens to various street noises and absorbs the French atmosphere." When the tone poem moves into the blues, "our American friend ... has succumbed to a spasm of homesickness." But, "nostalgia is not a fatal disease." The American visitor "once again is an alert spectator of Parisian life" and "the street noises and French atmosphere are triumphant."

Maurice Ravel met Gershwin in New York during Ravel's tour of the United States. In that meeting, Gershwin asked Ravel to be his teacher, to which Ravel responded that it was better to be a first-rate Gershwin than a second-rate Ravel. Instead, Ravel recommended that Gershwin see Nadia Boulanger in Paris. Ravel's high praise of Gershwin in an introductory letter to Boulanger caused Gershwin to seriously consider taking time to study abroad in Paris.

Gershwin arrived in Paris in March 1928. Paris at this time hosted many expatriate writers: among them Ezra Pound, W. B. Yeats, Ernest Hemingway; and artist Pablo Picasso. Gershwin met with Boulanger and at her request he played ten minutes of his music. Boulanger replied that she had nothing to teach him. This did not set Gershwin back, as his real intent abroad was to complete a new work based on Paris and perhaps a second rhapsody for piano and orchestra to follow his Rhapsody in Blue.

Gershwin based An American in Paris on a melodic fragment called "Very Parisienne", written in 1926 on his first visit to Paris as a gift to his hosts, Robert and Mabel Schirmer. He described the piece as a "rhapsodic ballet" because it was written freely and is more modern than his previous works. Gershwin explained in Musical America, "My purpose here is to portray the impressions of an American visitor in Paris as he strolls about the city, listens to the various street noises, and absorbs the French atmosphere."

The piece is structured into five sections, which culminate in a loose ABA format. Gershwin's first A episode introduces the two main "walking" themes in the "Allegretto grazioso" and develops a third theme in the "Subito con brio". The style of this A section is written in the typical French style of composers Claude Debussy and Les Six. This A section featured duple meter, singsong rhythms, and diatonic melodies with the sounds of oboe, English horn, and taxi horns. The B section's "Andante ma con ritmo deciso" introduces the American Blues and spasms of homesickness. The "Allegro" that follows continues to express homesickness in a faster twelve-bar blues. In the B section, Gershwin uses common time, syncopated rhythms, and bluesy melodies with the sounds of trumpet, saxophone, and snare drum. "Moderato con grazia" is the last A section that returns to the themes set in A. After recapitulating the "walking" themes, Gershwin overlays the slow blues theme from section B in the final â€œGrandioso.â€

An American in Paris is scored for 3 flutes (3rd doubling on piccolo), 2 oboes, English horn, 2 clarinets in B flat, bass clarinet in B flat, 2 bassoons, 4 horns in F, 3 trumpets in B flat, 3 trombones, tuba, timpani, snare drum, bass drum, triangle, wood block, cymbals, low and high tom-toms, xylophone, glockenspiel, celesta, 4 taxi horns labeled as A, B, C and D, alto saxophone/soprano saxophone, tenor saxophone/soprano saxophone/alto saxophone, baritone saxophone/soprano saxophone/alto saxophone, and strings.

The revised edition by F. Campbell-Watson calls for three saxophones, alto, tenor and baritone. In this arrangement the soprano and alto doublings have been rewritten to avoid changing instruments.

William Daly arranged the score for piano solo which was published by New World Music in 1929.

Gershwin did not particularly like Walter Damrosch's interpretation at the world premiere of An American in Paris. He stated that Damrosch's sluggish, dragging tempo caused him to walk out of the hall during a matinee performance of this work. The audience, according to Edward Cushing, responded with "a demonstration of enthusiasm impressively genuine in contrast to the conventional applause which new music, good and bad, ordinarily arouses." Critics believed that An American in Paris was better crafted than his lukewarm Concerto in F. Some did not think it belonged in a program with classical composers CÃ©sar Franck, Richard Wagner, or Guillaume Lekeu on its premiere. Gershwin responded to the critics, "It's not a Beethoven Symphony, you know... It's a humorous piece, nothing solemn about it. It's not intended to draw tears. If it pleases symphony audiences as a light, jolly piece, a series of impressions musically expressed, it succeeds."


On September 22, 2013, it was announced that a musicological critical edition of the full orchestral score will be eventually released. The Gershwin family, working in conjunction with the Library of Congress and the University of Michigan, are working to make scores available to the public that represent Gershwin's true intent. It is unknown if the critical score will include the four minutes of material Gershwin later deleted from the work (such as the restatement of the blues theme after the faster 12 bar blues section), or if the score will document changes in the orchestration during Gershwin's composition process. 

The score to An American in Paris is currently scheduled to be issued first in a series of scores to be released. The project may take 30 to 40 years to complete. The first edition will probably be published by 2023.  


An American in Paris has been frequently recorded. The first recording was made for RCA Victor in 1929 with Nathaniel Shilkret conducting the Victor Symphony Orchestra, drawn from members of the Philadelphia Orchestra. Gershwin was on hand to "supervise" the recording; however, Shilkret was reported to be in charge and eventually asked the composer to leave the recording studio. Then, a little later, Shilkret discovered there was no one to play the brief celesta solo during the slow section, so he hastily asked Gershwin if he might play the solo; Gershwin said he could and so he briefly participated in the actual recording. The radio broadcast of the September8, 1937 Hollywood Bowl George Gershwin Memorial Concert, in which An American in Paris, also conducted by Shilkret, was second on the program, was recorded and was released in 1998 in a two-CD set. Arthur Fiedler and the Boston Pops Orchestra recorded the work for RCA Victor, including one of the first stereo recordings of the music. In 1945, Arturo Toscanini and the NBC Symphony Orchestra recorded the music in Carnegie Hall, one of the few commercial recordings Toscanini made of music by an American composer. The Seattle Symphony also recorded a version in 1990 of Gershwin's original score, before he made numerous edits resulting in the score as we hear it today. 

In 1951, MGM released the musical An American in Paris, featuring Gene Kelly and Leslie Caron. Winning the 1951 Best Picture Oscar and numerous other awards, the film was directed by Vincente Minnelli, featured many tunes of Gershwin, and concluded with an extensive, elaborate dance sequence built around the An American in Paris symphonic poem (arranged for the film by Johnny Green), costing $500,000.

A part of the symphonic composition is also featured in As Good as It Gets, released in 1997.


Rimler, Walter. George Gershwinâ€“ An Intimate Portrait. Urbana, University of Illinois Press, 2009. 29â€“33.
Pollack, Howard. George Gershwinâ€“ His Life and Work.  Berkeley, University of California Press, 2006. 431â€“42.


 by The New York Philharmonic conducted by Artur RodziÅ„ski
, New York Philharmonic, Leonard Bernstein, 1959


The Academy Awards are the oldest awards ceremony for achievements in motion pictures. The Academy Award for Best Production Design recognizes achievement in art direction on a film. The category's original name was Best Art Direction, but was changed to its current name in 2012 for the 85th Academy Awards.  This change resulted from the Art Director's branch of the Academy being renamed the Designer's branch. Since 1947, the award is shared with the Set decorator(s).

The films below are listed with their production year (for example, the 2000 Academy Award for Best Art Direction is given to a film from 1999). In the lists below, the winner of the award for each year is shown first, followed by the other nominees.



The Academy Awards or The Oscars is an annual American awards ceremony honoring cinematic achievements in the film industry. The various category winners are awarded a copy of a statuette, officially the Academy Award of Merit, which is better known by its nickname Oscar. The awards, first presented in 1929 at the Hollywood Roosevelt Hotel, are overseen by the Academy of Motion Picture Arts and Sciences (AMPAS).

The awards ceremony was first televised in 1953 and is now seen live in more than 200 countries. The Oscars is the oldest entertainment awards ceremony; its equivalents, the Emmy Awards for television, the Tony Awards for theatre, and the Grammy Awards for music and recording, are modeled after the Academy Awards. The Academy Awards are widely considered to be the most prestigious cinema awards ceremony in the world.

The 86th Academy Awards ceremony was held on March 2, 2014, at the Dolby Theatre in Los Angeles, later than usual as to not clash with the Winter Olympics in Sochi, Russia. The 87th Academy Awards ceremony was held on February 22, 2015. Historically given during the first quarter of the new year, the awards honor achievements for cinematic accomplishments for the preceding year. For example, 12 Years a Slave was awarded Best Picture for 2013, although the Oscar ceremony was conducted in 2014.

As of the 87th Academy Awards ceremony held on February 22, 2015, a total of 2,947 Oscars have been awarded.
The first Academy Awards presentation was held on May 16, 1929, at a private dinner at the Hollywood Roosevelt Hotel with an audience of about 270 people. The post-awards party was held at the Mayfair Hotel. The cost of guest tickets for that night's ceremony was $5 ($ as of ),. Fifteen statuettes were awarded, honoring artists, directors and other personalities of the film-making industry of the time for their works during the 1927â€“28 period; the ceremony ran for 15minutes.

Winners had been announced to media three months earlier; however, that was changed in the second ceremony of the Academy Awards in 1930. Since then and during the first decade, the results were given to newspapers for publication at 11:00pm on the night of the awards. This method was used until the Los Angeles Times announced the winners before the ceremony began; as a result, the Academy has since 1941 used a sealed envelope to reveal the name of the winners.

The first Best Actor awarded was Emil Jannings, for his performances in The Last Command and The Way of All Flesh. He had to return to Europe before the ceremony, so the Academy agreed to give him the prize earlier; this made him the first Academy Award winner in history. The winners were recognized for all the work done in a certain category during the qualifying period; for example, Jannings received the award for two movies in which he starred during that period, and Janet Gaynor later won a single Oscar for performances in three films. With the fourth ceremony the system changed, and professionals were honored for a specific performance in a single film. For the first six ceremonies, the eligibility period spanned two calendar years.

At the 29th ceremony, held on March 27, 1957, the Best Foreign Language Film category was introduced. Until then, foreign-language films were honored with the Special Achievement Award.


Although there are seven other types of annual awards presented by the Academy (the Irving G. Thalberg Memorial Award, the Jean Hersholt Humanitarian Award, the Gordon E. Sawyer Award, the Academy Scientific and Technical Award, the Academy Award for Technical Achievement, the John A. Bonner Medal of Commendation, and the Student Academy Award) plus two awards that are not presented annually (the Special Achievement Award in the form of an Oscar statuette and the Honorary Award that may or may not be in the form of an Oscar statuette), the best known one is the Academy Award of Merit more popularly known as the Oscar statuette. Made of gold-plated britannium on a black metal base, it is 13.5in (34cm) tall, weighs 8.5lb (3.85kg) and depicts a knight rendered in Art Deco style holding a crusader's sword standing on a reel of film with five spokes. The five spokes represent the original branches of the Academy: Actors, Writers, Directors, Producers, and Technicians.

No model was used during the design process of the statuette. Sculptor George Stanley (who also did the Muse Fountain at the Hollywood Bowl) sculpted Cedric Gibbons's design in clay and Sachin Smith cast the statuette in 92.5 percent tin and 7.5 percent copper and then gold-plated it. The only addition to the Oscar since it was created is a minor streamlining of the base. The original Oscar mold was cast in 1928 at the C.W. Shumway & Sons Foundry in Batavia, Illinois, which also contributed to casting the molds for the Vince Lombardi Trophy and Emmy Awards statuettes. Since 1983, approximately 50 Oscars are made each year in Chicago by Illinois manufacturer R.S. Owens & Company. It takes between three to four weeks to manufacture 50 statuettes.

In support of the American effort in World War II, the statuettes were made of plaster and were traded in for gold ones after the war had ended.


The origin of the name Oscar is disputed. One biography of Bette Davis claims that she named the Oscar after her first husband, band leader Harmon Oscar Nelson; one of the earliest mentions in print of the term Oscar dates back to a Time magazine article about the 1934 6th Academy Awards.  Walt Disney is also quoted as thanking the Academy for his Oscar as early as 1932. Another claimed origin is that the Academy's Executive Secretary, Margaret Herrick, first saw the award in 1931 and made reference to the statuette's reminding her of her "Uncle Oscar" (a nickname for her cousin Oscar Pierce). Columnist Sidney Skolsky was present during Herrick's naming and seized the name in his byline, "Employees have affectionately dubbed their famous statuette 'Oscar'". The trophy was officially dubbed the "Oscar" in 1939 by the Academy of Motion Picture Arts and Sciences.

To prevent information identifying the Oscar winners from leaking ahead of the ceremony, Oscar statuettes presented at the ceremony have blank baseplates.  Until 2010, winners were expected to return the statuettes to the Academy after the ceremony and wait several weeks to have inscriptions applied.  Since 2010, winners have had the option of having engraved nameplates applied to their statuettes at an inscription-processing station at the Governor's Ball, a swanky party held immediately after the Oscar ceremony.  In 2010, the R. S. Owens company made 197 engraved nameplates ahead of the ceremony, bearing the names of every potential winner.  The 175 or so nameplates for non-winning nominees were recycled afterwards.

Since 1950, the statuettes have been legally encumbered by the requirement that neither winners nor their heirs may sell the statuettes without first offering to sell them back to the Academy for US$1. If a winner refuses to agree to this stipulation, then the Academy keeps the statuette. Academy Awards not protected by this agreement have been sold in public auctions and private deals for six-figure sums.  In December 2011, Orson Welles' 1941 Oscar for Citizen Kane (Best Original Screenplay) was put up for auction, after his heirs won a 2004 court decision contending that Welles did not sign any agreement to return the statue to the Academy. On December 20, 2011, it sold in an online auction for US$861,542.

In 1992, Harold Russell needed money for his wife's medical expenses. In a controversial decision, he consigned his 1946 Oscar for Best Supporting Actor for "The Best Years of Our Lives" to Herman Darvick Autograph Auctions, and on August 6, 1992, in New York City, the Oscar sold to a private collector for $60,500. Russell defended his action, saying, "I don't know why anybody would be critical. My wife's health is much more important than sentimental reasons. The movie will be here, even if Oscar isn't." Harold Russell is the only Academy Award winning actor to ever sell an Oscar.

While the Oscar is owned by the recipient, it is essentially not on the open market. Michael Todd's grandson tried to sell Todd's Oscar statuette to a movie prop collector in 1989, but the Academy won the legal battle by getting a permanent injunction. Although some Oscar sales transactions have been successful, some buyers have subsequently returned the statuettes to the Academy, which keeps them in its treasury.

Since 2004, Academy Award nomination results have been announced to the public in late January. Prior to that, the results were announced in early February.

The Academy of Motion Picture Arts and Sciences (AMPAS), a professional honorary organization, maintains a voting membership of 5,783 .

Academy membership is divided into different branches, with each representing a different discipline in film production. Actors constitute the largest voting bloc, numbering 1,311 members (22 percent) of the Academy's composition. Votes have been certified by the auditing firm PricewaterhouseCoopers (and its predecessor Price Waterhouse) for the past 73 annual awards ceremonies.

All AMPAS members must be invited to join by the Board of Governors, on behalf of Academy Branch Executive Committees. Membership eligibility may be achieved by a competitive nomination or a member may submit a name based on other significant contribution to the field of motion pictures.

New membership proposals are considered annually. The Academy does not publicly disclose its membership, although as recently as 2007 press releases have announced the names of those who have been invited to join. The 2007 release also stated that it has just under 6,000 voting members. While the membership had been growing, stricter policies have kept its size steady since then.

In 2012, the results of a study conducted by The Los Angeles Times was published which revealed the demographic breakdown of approximately 88% of AMPAS' voting membership. Of the 5,100+ active voters confirmed, 94% were Caucasian, 77% were male, and 54% were found to be over the age of 60. 33% of voting members are former nominees (14%) and winners (19%).

In May 2011, the Academy sent a letter advising its 6,000 or so voting members that an online system for Oscar voting will be implemented in 2013.

According to Rules 2 and 3 of the official Academy Awards Rules, a film must open in the previous calendar year, from midnight at the start of 1 January to midnight at the end of 31 December, in Los Angeles County, California and play for seven consecutive days, to qualify (except for the Best Foreign Language Film). For example, the 2009 Best Picture winner, The Hurt Locker, was actually first released in 2008, but did not qualify for the 2008 awards as it did not play its Oscar-qualifying run in Los Angeles until mid-2009, thus qualifying for the 2009 awards.

Rule 2 states that a film must be feature-length, defined as a minimum of 40minutes, except for short subject awards, and it must exist either on a 35 mm or 70 mm film print or in 24frame/s or 48frame/s progressive scan digital cinema format with a minimum projector resolution of 2048 by 1080 pixels.

Producers must submit an Official Screen Credits online form before the deadline; in case it is not submitted by the defined deadline, the film will be ineligible for Academy Awards in any year. The form includes the production credits for all related categories. Then, each form is checked and put in a Reminder List of Eligible Releases.

In late December ballots and copies of the Reminder List of Eligible Releases are mailed to around 6000 active members. For most categories, members from each of the branches vote to determine the nominees only in their respective categories (i.e. only directors vote for directors, writers for writers, actors for actors, etc.). In all major categories, voters use an instant runoff voting ballot, with potential nominees rewarded in the single transferable vote tally for having strong supporters who rank them first. There are some exceptions in the case of certain categories, like Foreign Film, Documentary and Animated Feature Film, in which movies are selected by special screening committees made up of members from all branches. In the special case of Best Picture, all voting members are eligible to select the nominees for that category. Foreign films must include English subtitles, and each country can submit only one film per year.

The winners are then determined by a second round of voting in which all members are then allowed to vote in most categories, including Best Picture.

Film companies will spend as much as several million dollars on marketing to awards voters for a movie in the running for Best Picture, in attempts to improve chances of receiving Oscars and other movie awards conferred in Oscar season. The Academy enforces rules to limit overt campaigning by its members so as to try to eliminate excesses and prevent the process from becoming undignified. It has an awards czar on staff who advises members on allowed practices and levies penalties on offenders. For example, a producer of the 2010 Best Picture nominee, The Hurt Locker, was disqualified as a producer in the category when he contacted associates urging them to vote for his film and not another that was seen as front-runner (The Hurt Locker eventually won).


The major awards are presented at a live televised ceremony, most commonly in late February or early March following the relevant calendar year, and six weeks after the announcement of the nominees. It is the culmination of the film awards season, which usually begins during November or December of the previous year. This is an elaborate extravaganza, with the invited guests walking up the red carpet in the creations of the most prominent fashion designers of the day. Black tie dress is the most common outfit for men, although fashion may dictate not wearing a bow-tie, and musical performers sometimes do not adhere to this. (The artists who recorded the nominees for Best Original Song quite often perform those songs live at the awards ceremony, and the fact that they are performing is often used to promote the television broadcast).

The Academy Awards is the only awards show that is televised live in all United States time zones (excluding Hawaii; they aired live in Alaska starting in 2011 for the first time since 1996), Canada, the United Kingdom, and gathers millions of viewers elsewhere throughout the world. The 2007 ceremony was watched by more than 40 million Americans. Other awards ceremonies (such as the Emmys, Golden Globes, and Grammys) are broadcast live in the Eastern & Central time zones, but are on tape delay on the West Coast and might not air the same day outside North America (if the awards are even televised). The Academy has for several years claimed that the award show has up to a billion viewers internationally, but this has so far not been confirmed by any independent sources.
The Awards show was first televised in 1953, on NBC, which continued to broadcast the event until 1960 when the ABC Network took over, televising the festivities through 1970, after which NBC resumed the broadcasts. ABC once again took over broadcast duties in 1976 and it is under contract to do so through the year 2020. The first countries to broadcast the Academy Awards on television, aside from the United States, were Canada, the United Kingdom and Mexico; the latter two countries did not broadcast the live show, and Mexico did not carry the live event until 1964. By 1954, seven other countries, namely Brazil, Cuba, Venezuela, West Germany, Belgium, the Netherlands, and France, were already broadcasting the Awards show, although previously filmed into a condensed 60-minute version, called the "International Edition" of the Academy Awards. The Awards show was broadcast for the first time via satellite in 1970, but only two South American countries, Chile and Brazil purchased the rights to air the live event. By that time, the television rights to the Awards show were sold in 50 countries. A decade later, the TV rights to the Oscars were already being sold to 60 countries, and by 1984, the TV rights to the Awards were licensed in 76 countries. Until the arrival of subscription television in Europe, Africa and the Middle East, which enabled the Oscars to be broadcast live through these territories, there was already available a previously filmed or taped 90-minute version of the Awards show which was aired on broadcast television in these territories. However, several Asian and Australasian countries, such as Japan, Hong Kong, South Korea, Singapore, Malaysia, Australia and New Zealand were already carrying the live broadcast of the Awards show since the late 1970s. AMPAS still produces a condensed 90-minute version of the Academy Awards for those territories in which the rights to the live broadcast are expensive.

After more than 60 years of being held in late March or early April, the ceremonies were moved up to late February or early March starting in 2004 to help disrupt and shorten the intense lobbying and ad campaigns associated with Oscar season in the film industry. Another reason was because of the growing TV ratings success of the NCAA Men's Division I Basketball Championship, which would cut into the Academy Awards audience. The earlier date is also to the advantage of ABC, as it now usually occurs during the highly profitable and important February sweeps period. Some years, the ceremony is moved into early March in deference to the Winter Olympics. Another reason for the move to late February and early March is to avoid the awards ceremony occurring so close to the religious holidays of Passover and Easter, which for decades had been a grievance from members and the general public. Advertising is somewhat restricted, however, as traditionally no movie studios or competitors of official Academy Award sponsors may advertise during the telecast. The Awards show holds the distinction of having won the most Emmys in history, with 47 wins and 195 nominations.

After many years of being held on Mondays at 9:00pm Eastern/6:00 p.m Pacific, in 1999 the ceremonies were moved to Sundays at 8:30pm Eastern/5:30pm Pacific. The reasons given for the move were that more viewers would tune in on Sundays, that Los Angeles rush-hour traffic jams could be avoided, and that an earlier start time would allow viewers on the East Coast to go to bed earlier. For many years the film industry had opposed a Sunday broadcast because it would cut into the weekend box office.

Originally scheduled for April 8, 1968, the 40th Academy Awards ceremony was postponed for two days, because of the assassination of Dr. Martin Luther King, Jr.. On March 30, 1981, the 53rd Academy Awards was postponed for one day, after the shooting of President Ronald Reagan and others in Washington, D.C.

In 1993, an In Memoriam segment was introduced, honoring those who had made a significant contribution to cinema who had died in the preceding 12 months, a selection compiled by a small committee of Academy members. This segment has drawn criticism over the years for the omission of some names. Criticism was also levied for many years regarding another aspect, with the segment having a "popularity contest" feel as the audience varied their applause to those who had died by the subject's cultural impact; the applause has since been muted during the telecast, and the audience is discouraged from clapping during the segment and giving silent reflection instead.

In terms of broadcast length, the ceremony generally averages three and a half hours. The first Oscars, in 1929, lasted 15minutes. At the other end of the spectrum, the 2000 ceremony lasted four hours and four minutes. In 2010, the organizers of the Academy Awards announced that winners' acceptance speeches must not run past 45seconds. This, according to organizer Bill Mechanic, was to ensure the elimination of what he termed "the single most hated thing on the show" â€“ overly long and embarrassing displays of emotion.

The Academy has contemplated moving the ceremony even further back into January, citing TV viewers' fatigue with the film industry's long awards season. However, such an accelerated schedule would dramatically decrease the voting period for its members, to the point where some voters would only have time to view the contending films streamed on their computers (as opposed to traditionally receiving the films and ballots in the mail). Also, a January ceremony would have to compete with National Football League playoff games.

Historically, the "Oscarcast" has pulled in a bigger haul when box-office hits are favored to win the Best Picture trophy. More than 57.25 million viewers tuned to the telecast for the 70th Academy Awards in 1998, the year of Titanic, which generated close to US$600 million at the North American box office pre-Oscars. The 76th Academy Awards ceremony in which  (pre-telecast box office earnings of US$368 million) received 11 Awards including Best Picture drew 43.56 million viewers. The most watched ceremony based on Nielsen ratings to date, however, was the 42nd Academy Awards (Best Picture Midnight Cowboy) which drew a 43.4% household rating on 7 April 1970.

By contrast, ceremonies honoring films that have not performed well at the box office tend to show weaker ratings. The 78th Academy Awards which awarded low-budgeted, independent film Crash (with a pre-Oscar gross of US$53.4 million) generated an audience of 38.64 million with a household rating of 22.91%.  In 2008, the 80th Academy Awards telecast was watched by 31.76 million viewers on average with an 18.66% household rating, the lowest rated and least watched ceremony to date, in spite of celebrating 80 years of the Academy Awards.  The Best Picture winner of that particular ceremony was another independently financed film (No Country for Old Men).


In 1929, the first Academy Awards were presented at a banquet dinner at the Hollywood Roosevelt Hotel.  From 1930 to 1943, the ceremony alternated between two venues: the Ambassador Hotel on Wilshire Boulevard and the Biltmore Hotel in downtown Los Angeles.

Grauman's Chinese Theater in Hollywood then hosted the awards from 1944 to 1946, followed by the Shrine Auditorium in Los Angeles from 1947 to 1948. The 21st Academy Awards in 1949 were held at the Academy Award Theater at what was the Academy's headquarters on Melrose Avenue in Hollywood.

From 1950 to 1960, the awards were presented at Hollywood's Pantages Theatre.  With the advent of television, the awards from 1953 to 1957 took place simultaneously in Hollywood and New York, first at the NBC International Theatre (1953) and then at the NBC Century Theatre, after which the ceremony took place solely in Los Angeles.  The Oscars moved to the Santa Monica Civic Auditorium in Santa Monica, California in 1961. By 1969, the Academy decided to move the ceremonies back to Los Angeles, this time to the Dorothy Chandler Pavilion at the Los Angeles County Music Center.

In 2002, the Dolby Theatre (formerly known as the Kodak Theatre) became the current venue of the presentation.

In the first year of the awards, the Best Directing award was split into two separate categories (Drama and Comedy).  At times, the Best Original Score award has also been split into separate categories (Drama and Comedy/Musical).  From the 1930s through the 1960s, the Art Direction (now Production Design), Cinematography, and Costume Design awards were likewise split into two separate categories (black-and-white films and color films).  Prior to 2012, the Production Design award was called Art Direction, while the Makeup and Hairstyling award was called Makeup.

Another award, entitled the Academy Award for Best Original Musical, is still in the Academy rulebooks and has yet to be discontinued.  However, due to continuous insufficient eligibility each year, it has not been awarded since 1984 (when Purple Rain won).

The Board of Governors meets each year and considers new award categories. To date, the following proposed categories have been rejected:
Best Casting: rejected in 1999
Best Stunt Coordination: rejected every year from 1991 to 2012
Best Title Design: rejected in 1999

The Special Academy Awards are voted on by special committees, rather than by the Academy membership as a whole. They are not always presented on a consistent annual basis.

Academy Honorary Award: since 1929
Academy Scientific and Technical Award: since 1931
Gordon E. Sawyer Award: since 1981
Jean Hersholt Humanitarian Award: since 1956
Irving G. Thalberg Memorial Award: since 1938

Academy Juvenile Award: 1934 to 1960
Academy Special Achievement Award: 1972 to 1995

Due to the positive exposure and prestige of the Academy Awards, studios spend millions of dollars and hire publicists specifically to promote their films during what is typically called the "Oscar season". This has generated accusations of the Academy Awards being influenced more by marketing than quality. William Friedkin, an Academy Award-winning film director and former producer of the ceremony, expressed this sentiment at a conference in New York in 2009, describing it as "the greatest promotion scheme that any industry ever devised for itself".

In addition, some winners critical of the Academy Awards have boycotted the ceremonies and refused to accept their Oscars. The first to do so was Dudley Nichols (Best Writing in 1935 for The Informer). Nichols boycotted the 8th Academy Awards ceremony because of conflicts between the Academy and the Writers' Guild. George C. Scott became the second person to refuse his award (Best Actor in 1970 for Patton) at the 43rd Academy Awards ceremony. Scott described it as a 'meat parade', saying 'I don't want any part of it." The third was Marlon Brando, who refused his award (Best Actor for 1972's The Godfather), citing the film industry's discrimination and mistreatment of Native Americans. At the 45th Academy Awards ceremony, Brando sent Sacheen Littlefeather to read a 15-page speech detailing his criticisms.

Tim Dirks, editor of AMC's filmsite.org, has written of the Academy Awards,
Typical criticism of the Academy Awards for Best Picture is that among the winners and nominees there is an over-representation of romantic historical epics, biographical dramas, romantic dramedies, and family melodramas, most of which are released in the U.S. the last three months of the calendar year. The Oscars have been infamously known for selecting specific genres of movies to be awarded. From 1927 to 2001 around 49% of Best Picture nominated films had been categorized as a drama and out of the 432 films to be analyzed within that time 47% of the winning films were in fact dramas.  This has led to the coining of the term 'Oscar bait', describing such movies.  This has led at times to more specific criticisms that the Academy is disconnected from the audience, e.g. by favoring 'Oscar bait' over audience favorites, or favoring historical melodramas over critically acclaimed movies that depict current life issues.

Acting prizes in certain years have been criticized for not recognizing superior performances so much as being awarded for sentimental reasons, personal popularity, atonement for past mistakes, or presented as a "career honor" to recognize a distinguished nominee's entire body of work.

The following events are closely associated with the annual Academy Awards:
CÃ©sar Award
Nominees luncheon
Governors Awards
The 25th Independent Spirit Awards (in 2010), usually held in Santa Monica the Saturday before the Oscars, marked the first time it was moved to a Friday and a change of venue to L.A. Live.
The annual "Night Before," traditionally held at the Beverly Hills Hotel, begun in 2002 and generally known as the party of the season, benefits the  Motion Picture and Television Fund, which operates a retirement home for SAG actors in the San Fernando Valley.
Elton John AIDS Foundation Academy Award Party airs the awards live at the nearby Pacific Design Center.
The Governors' Ball is the Academy's official after-party, including dinner (until 2011), and is adjacent to the awards-presentation venue.  In 2012, the three-course meal was replaced by appetizers.
The Vanity Fair after-party, historically at the former Morton's restaurant, since 2009 has been at the Sunset Tower.

It has become a tradition to give out gift bags to the presenters and performers at the Oscars. In recent years these gifts have also been extended to award nominees and winners. The total value of these gifts can reach into the tens of thousands of dollars. In 2014 the value was reported to be as high as US$80,000. The value has risen to the point where the U.S. Internal Revenue Service issued a statement regarding the gifts and their taxable status.
Oscar gift bags have included vacation packages to Hawaii and Mexico and Japan, a private dinner party for the recipient and friends at a restaurant, videophones, a four-night stay at a hotel. Watches, bracelets, vacation packages, spa treatments, bottles of vodka, maple salad dressing, and weight-loss gummie candy. Some of the gifts have even had a "risque" element to them; in 2014 the adult products retailer Adam & Eve had a "Secret Room Gifting Suite". Celebrities visiting the gifting suite included Judith Hoag, Carolyn Hennesy, Kate Linder, Chris Mulkey, Jim O'Heir, and NBA star John Salley.


From 2006 onwards, results are Live+SD, all previous years are Live viewing

Academy Awards pre-show
Oscar speech
Oscar season
Film awards seasons
Lists:
List of Academy Award records
List of Academy Award-winning families
List of Academy Award-winning films
List of Academy Awards ceremonies
List of Academy Award trophies on public display
List of actors nominated for two Academy Awards in the same year
List of actors who have appeared in multiple Best Picture Academy Award winners
List of actors with two or more Academy Award nominations in acting categories
List of actors with two or more Academy Awards in acting categories
List of Best Picture milestones
List of Big Five Academy Award winners and nominees
List of fictitious Academy Award winners and nominees
List of films with the most Oscars per ceremony
List of films with all four Academy Award acting nominations
List of foreign-language films nominated for Academy Awards
List of oldest and youngest Academy Award winners and nominees
List of people who have won multiple Academy Awards in a single year
List of posthumous Academy Award winners and nominees
List of presenters of Best Picture Academy Award
List of superlative Academy Award winners and nominees
List of years in film

Nationality:
List of Australian Academy Award winners and nominees
List of Brazilian Academy Award winners and nominees
List of English Academy Award nominees and winners
List of French Academy Award winners and nominees
List of Indian Academy Award winners and nominees
List of Italian Academy Award winners and nominees
List of Latin American Academy Award winners and nominees
List of New Zealand Academy Award winners and nominees
List of Nordic Academy Award winners and nominees
List of Polish Academy Award winners and nominees
List of Spanish Academy Award winners and nominees
Race/Ethnicity:
List of African-American Academy Award winners and nominees
List of Asian Academy Award winners and nominees
Lists of Hispanic Academy Award winners and nominees by country

Global Equivalents:
AACTA Awards (Australian equivalent)
AMAA (Nigerian equivalent)
Amanda Award (Norwegian equivalent)
Argentine Academy of Cinematography Arts and Sciences
Argentine Film Critics Association (Argentine equivalent)  
Ariel Awards (Mexican equivalent)
BAFTA (British equivalent)
Canadian Screen Awards (Canadian equivalent)
CÃ©sar Awards (French equivalent)
David di Donatello Awards (Italian equivalent)
Golden Horse Film Festival and Awards (Chinese/Taiwanese equivalent)
Goya Awards (Spanish equivalent)
Grand Bell Awards (South Korean equivalent)
Guldbagge Awards (Swedish equivalent)
Japanese Academy Awards (Japanese equivalent)
Lola Awards (German equivalent)
Magritte Awards (Belgian equivalent)
National Film Awards (Indian equivalent)
Brokaw, Lauren (2010). . Los Angeles: The Daily Truffle.
Wright, Jon (2007). The Lunacy of Oscar: The Problems with Hollywood's Biggest Night. Thomas Publishing, Inc.

â€”official Academy Award ceremony site.
 (searchable)
.
 at Time magazine.


Actrius (Catalan: Actresses) is a 1996 film directed by Ventura Pons. In the film, there are no male actors and the four leading actresses dubbed themselves in the Castilian version.

In order to prepare the role of an important old actress, a theatre student interviews three actresses who were her pupils: an international diva (GlÃ²ria Marc, played by NÃºria Espert), a television star (Assumpta Roca, played by Rosa Maria SardÃ ) and a dubbing director (Maria Caminal, played by Anna Lizaran).



Animalia is an illustrated children's book by Graeme Base. It was originally published in 1986, followed by a tenth anniversary edition in 1996, and a 25th anniversary edition in 2012. Over three million copies have been sold.   A special numbered and signed anniversary edition was also published in 1996, with an embossed gold jacket.

Animalia is an alliterative alphabet book and contains twenty-six illustrations, one for each letter of the alphabet. Each illustration features an animal from the animal kingdom (A is for alligator, B is for butterfly, etc.) along with a short poem utilizing the letter of the page for many of the words. The illustrations contain many other objects beginning with that letter that the reader can try to identify. As an additional challenge, the author has hidden a picture of himself as a child in every picture.

Penguin Books published an Animalia colouring book in 2008.   H. N. Abrams also published a wall calendar colouring book version for children the same year.

H. N. Abrams published The Animalia Wall Frieze, a fold-out over 26 feet in length, in which the author created new riddles for each letter.

The Great American Puzzle Factory created a 300-piece jigsaw puzzle based on the book's cover.

A television series was also created, based on the book, which airs in the United States, Australia, Canada, the United Kingdom, Norway and Venezuela. It also airs on Minimax for the Czech Republic and Slovakia. And recently in Greece on the channel ET1. The Australian Children's Television Foundation released a teaching resource DVD-ROM in 2011 to accompany the TV series with teaching aids for classroom use.

In 2010, The Base Factory and AppBooks released Animalia as an application for iPad and iPhone/iPod Touch.

Animalia won the Young Australian's Best Book Award in 1987 for Best Picture Story Book.

The Children's Book Council of Australia designated Animalia a 1987 : Honour Book.

Kid's Own Australian Literature Awards named Animalia the 1988 Picture Book Winner.

 for Animalia created by The Little Big Book Club

International Atomic Time (TAI, from the French name ) is a high-precision atomic coordinate time standard based on the notional passage of proper time on Earth's geoid. in over 50 national laboratories worldwide. The clocks are compared using GPS signals and two-way satellite time and frequency transfer.  Due to the averaging it is far more stable than any clock would be alone (see signal averaging for a discussion). The majority of the clocks are caesium clocks; the definition of the SI second is written in terms of caesium.

The participating institutions each broadcast, in real time, a frequency signal with timecodes, which is their estimate of TAI.  Time codes are usually published in the form of UTC, which differs from TAI by a well-known integer number of seconds.  These time scales are denoted in the form UTC(NPL) in the UTC form, where NPL in this case identifies the National Physical Laboratory, UK.  The TAI form may be denoted TAI(NPL).  The latter is not to be confused with TA(NPL), which denotes an independent atomic time scale, not synchronised to TAI or to anything else.

The clocks at different institutions are regularly compared against each other.  The International Bureau of Weights and Measures (BIPM, France), combines these measurements to retrospectively calculate the weighted average that forms the most stable time scale possible.  This combined time scale is published monthly in , and is the canonical TAI.  This time scale is expressed in the form of tables of differences UTC-UTC(k) (equivalent to TAI-TAI(k)) for each participating institution k.  (The same circular also gives tables of TAI-TA(k), for the various unsynchronised atomic time scales.)

Errors in publication may be corrected by issuing a revision of the faulty Circular T or by errata in a subsequent Circular T.  Aside from this, once published in Circular T the TAI scale is not revised.  In hindsight it is possible to discover errors in TAI, and to make better estimates of the true proper time scale.  Doing so does not create another version of TAI; it is instead considered to be creating a better realisation of Terrestrial Time (TT).

Early atomic time scales consisted of quartz clocks with frequencies calibrated by a single atomic clock; the atomic clocks were not operated continuously. Atomic timekeeping services started experimentally in 1955, using the first caesium atomic clock at the National Physical Laboratory, UK (NPL). The "Greenwich Atomic" (GA) scale began in 1955 at the Royal Greenwich Observatory. The International Time Bureau (BIH) began a time scale, Tm or AM, in July 1955, using both local caesium clocks and comparisons to distant clocks using the phase of VLF radio signals. The United States Naval Observatory began the A.1 scale  13 September 1956, using an Atomichron commercial atomic clock, followed by the NBS-A scale at the National Bureau of Standards, Boulder, Colorado.   Both the BIH scale and A.1 was defined by an epoch at the beginning of 1958: it was set to read Julian Date 2436204.5 (1 January 1958 00:00:00) at the corresponding UT2 instant. The procedures used by the BIH evolved, and the name for the time scale changed: "A3" in 1963 and "TA(BIH)" in 1969.  This synchronisation was inevitably imperfect, depending as it did on the astronomical realisation of UT2.  At the time, UT2 as published by various observatories differed by several hundredths of a second.

In 1961, UTC began.  UTC is a discontinuous time scale composed from segments that are linear transformations of atomic time, the discontinuities being arranged so that UTC approximated UT2 until the end of 1971, and UT1 thereafter.  This was a compromise arrangement for a broadcast time scale: a linear transformation of the BIH's atomic time meant that the time scale was stable and internationally synchronised, while approximating UT1 means that tasks such as navigation which require a source of Universal Time continue to be well served by public time broadcasts.

The SI second was defined in terms of the caesium atom in 1967, and in 1971 the name International Atomic Time (TAI) was assigned to a time scale based on SI seconds with no leap seconds. During this time, irregularities in the atomic time were detected and corrected. In 1967 it was suggested that nearby masses caused clocks to operate at different rates, but this was disproven in 1968.

In the 1970s, it became clear that the clocks participating in TAI were ticking at different rates due to gravitational time dilation, and the combined TAI scale therefore corresponded to an average of the altitudes of the various clocks.  Starting from Julian Date 2443144.5 (1 January 1977 00:00:00), corrections were applied to the output of all participating clocks, so that TAI would correspond to proper time at mean sea level (the geoid).  Because the clocks had been on average well above sea level, this meant that TAI slowed down, by about 10âˆ’12.  The former uncorrected time scale continues to be published, under the name EAL (Echelle Atomique Libre, meaning Free Atomic Scale).

The instant that the gravitational correction started to be applied serves as the epoch for Barycentric Coordinate Time (TCB), Geocentric Coordinate Time (TCG), and Terrestrial Time (TT).  All three of these time scales were defined to read JD 2443144.5003725 (1 January 1977 00:00:32.184) exactly at that instant.  (The offset is to provide continuity with the older Ephemeris Time.)  TAI was henceforth a realisation of TT, with the equation TT(TAI) = TAI + 32.184s.

The continued existence of TAI was questioned in a 2007 letter from the BIPM to the ITU-R which stated "In the case of a redefinition of UTC without leap seconds, the CCTF would consider discussing the possibility of suppressing TAI, as it would remain parallel to the continuous UTC."

Time and frequency transfer
Clock synchronization
Network Time Protocol
Precision Time Protocol, a related though separate technology
Magneto-optical trap



Altruism or selflessness is the principle or practice of concern for the welfare of others. It is a traditional virtue in many cultures and a core aspect of various religious traditions and secular worldviews, though the concept of "others" toward whom concern should be directed can vary among cultures and religions. Altruism or selflessness is the opposite of selfishness. The word was coined by the French philosopher Auguste Comte in French, as altruisme, for an antonym of egoism. He derived it from an Italian altrui, which in turn was derived from Latin alteri, meaning "other people" or "somebody else".

Altruism in biological organisms can be defined as an individual performing an action which is at a cost to themselves (e.g., pleasure and quality of life, time, probability of survival or reproduction), but benefits, either directly or indirectly, another third-party individual, without the expectation of reciprocity or compensation for that action.

Altruism can be distinguished from feelings of loyalty, in that whilst the latter is predicated upon social relationships, altruism does not consider relationships. Much debate exists as to whether "true" altruism is possible in human psychology. The theory of psychological egoism suggests that no act of sharing, helping or sacrificing can be described as truly altruistic, as the actor may receive an intrinsic reward in the form of personal gratification. The validity of this argument depends on whether intrinsic rewards qualify as "benefits."

The term altruism may also refer to an ethical doctrine that claims that individuals are morally obliged to benefit others. Used in this sense, it is usually contrasted with egoism, which is defined as acting to the benefit of one's self.

The concept has a long history in philosophical and ethical thought. The term was originally coined in the 19th century by the founding sociologist and philosopher of science, Auguste Comte, and has become a major topic for psychologists (especially evolutionary psychology researchers), evolutionary biologists, and ethologists. Whilst ideas about altruism from one field can have an impact on the other fields, the different methods and focuses of these fields always lead to different perspectives on altruism. In simple terms, altruism is caring about the welfare of other people and acting to help them.

A certain individual may behave altruistically in one case and egotistically in another situation. However, some individuals tend to behave more altruistically, while others tend to behave more egotistically. Altruism may be considered a general attitude to the point where altruism has been considered as a trait.

A 1986 study estimated that altruism was half-inherited. Another study, by the Massachusetts Institute of Technology, was published in 2009. Also based on the twin design, the new study estimated that genetic differences accounted for approximately 20% of individual variations.


Marcel Mauss's book The Gift contains a passage: "Note on alms." This note describes the evolution of the notion of alms (and by extension of altruism) from the notion of sacrifice. In it, he writes:

Alms are the fruits of a moral notion of the gift and of fortune on the one hand,
and of a notion of sacrifice, on the other. Generosity is an obligation, because Nemesis
avenges the poor and the gods for the superabundance of happiness and wealth
of certain people who should rid themselves of it. This is the ancient morality of the
gift, which has become a principle of justice. The gods and the spirits accept that
the share of wealth and happiness that has been offered to them and had been
hitherto destroyed in useless sacrifices should serve the poor and children.

Compare Altruism (ethics) â€“ perception of altruism as self-sacrifice.
Compare explanation of alms in various scriptures.

In the science of ethology (the study of animal behaviour), and more generally in the study of social evolution, altruism refers to behaviour by an individual that increases the fitness of another individual while decreasing the fitness of the actor. In evolutionary psychology this may be applied to a wide range of human behaviors such as charity, emergency aid, help to coalition partners, tipping, courtship gifts, production of public goods, and environmentalism.

Theories of apparently altruistic behavior were accelerated by the need to produce theories compatible with evolutionary origins. Two related strands of research on altruism have emerged from traditional evolutionary analyses and from evolutionary game theory a mathematical model and analysis of behavioural strategies.

Some of the proposed mechanisms are:
Kin selection. That animals and humans are more altruistic towards close kin than to distant kin and non-kin has been confirmed in numerous studies across many different cultures. Even subtle cues indicating kinship may unconsciously increase altruistic behavior. One kinship cue is facial resemblance. One study found that slightly altering photographs so that they more closely resembled the faces of study participants increased the trust the participants expressed regarding depicted persons. Another cue is having the same family name, especially if rare, and this has been found to increase helping behavior. Another study found more cooperative behavior the greater the number of perceived kin in a group. Using kinship terms in political speeches increased audience agreement with the speaker in one study. This effect was especially strong for firstborns, who are typically close to their families.
Vested interests. People are likely to suffer if their friends, allies, and similar social ingroups suffer or even disappear. Helping such group members may therefore eventually benefit the altruist. Making ingroup membership more noticeable increases cooperativeness. Extreme self-sacrifice towards the ingroup may be adaptive if a hostile outgroup threatens to kill the entire ingroup.
Reciprocal altruism. See also Reciprocity (evolution).
Direct reciprocity. Research shows that it can be beneficial to help others if there is a chance that they can and will reciprocate the help. The effective tit for tat strategy is one game theoretic example. Many people seem to be following a similar strategy by cooperating if and only if others cooperate in return.

One consequence is that people are more cooperative if it is more likely that individuals will interact again in the future. People tend to be less cooperative if they perceive that the frequency of helpers in the population is lower. They tend to help less if they see non-cooperativeness by others and this effect tend to be stronger than the opposite effect of seeing cooperative behaviors. Simply changing the cooperative framing of a proposal may increase cooperativeness such as calling it a "Community Game" instead of a "Wall Street Game."

A tendency towards reciprocity implies that people will feel obligated to respond if someone helps them. This has been used by charities that give small gifts to potential donors hoping thereby to induce reciprocity. Another method is to announce publicly that someone has given a large donation. The tendency to reciprocate can even generalize so people become more helpful toward others in general after being helped. On the other hand, people will avoid or even retaliate against those perceived not to be cooperating. People sometimes mistakenly fail to help when they intended to, or their helping may not be noticed, which may cause unintended conflicts. As such, it may be an optimal strategy to be slightly forgiving of and have a slightly generous interpretation of non-cooperation.

People are more likely to cooperate on a task if they can communicate with one another first. This may be due to better assessments of cooperativeness or due to exchange of promises. They are more cooperative if they can gradually build trust, instead of being asked to give extensive help immediately. Direct reciprocity and cooperation in a group can be increased by changing the focus and incentives from intra-group competition to larger scale competitions such as between groups or against the general population. Thus, giving grades and promotions based only on an individual's performance relative to a small local group, as is common, may reduce cooperative behaviors in the group.

Indirect reciprocity. The avoidance of poor reciprocators and cheaters causes a person's reputation to become very important. A person with a good reputation for reciprocity have a higher chance of receiving help even from persons they have had no direct interactions with previously.
Strong reciprocity. A form of reciprocity where some individuals seem to spend more resources on cooperating and punishing than would be most beneficial as predicted by several established theories of altruism. A number of theories have been proposed as explanations as well as criticisms regarding its existence.
Pseudo-reciprocity. An organism behaves altruistically and the recipient does not reciprocate but has an increased chance of acting in a way that is selfish but also as a byproduct benefits the altruist.
Costly signaling and the handicap principle. Since altruism takes away resources from the altruist it can be an "honest signal" of resource availability and the abilities needed to gather resources. This may signal to others that the altruist is a valuable potential partner. It may also be a signal of interactive and cooperative intentions since those not interacting further in the future gain nothing from the costly signaling. It is unclear if costly signaling can indicate a long-term cooperative personality but people have increased trust for those who help. Costly signaling is pointless if everyone has the same traits, resources, and cooperative intentions but become a potentially more important signal if the population increasingly varies on these characteristics.

Hunters widely sharing the meat has been seen as a costly signal of ability and research has found that good hunters have higher reproductive success and more adulterous relations even if they themselves receive no more of the hunted meat than anyone else. Similarly, holding large feasts and giving large donations has been seen as ways of demonstrating one's resources. Heroic risk-taking has also been interpreted as a costly signal of ability.
Both indirect reciprocity and costly signaling depend on the value of reputation and tend to make similar predictions. One is that people will be more helping when they know that their helping behavior will be communicated to people they will interact with later, is publicly announced, is discussed, or is simply being observed by someone else. This have been documented in many studies. The effect is sensitive to subtle cues such as people being more helpful when there were stylized eyespots instead of a logo on a computer screen. Weak reputational cues such as eyespots may become unimportant if there are stronger cues present and may lose their effect with continued exposure unless reinforced with real reputational effects. Public displays such as public weeping for dead celebrities and participation in demonstrations may be influenced by a desire to be seen as altruistic. People who know that they are publicly monitored sometimes even wastefully donate money they know are not needed by recipient which may be because of reputational concerns.

Women have been found to find altruistic men to be attractive partners. When looking for a long-term partner more conventional altruism may be preferred which may indicate that he is also willing to share resources with her and her children while when looking for a short-term partner heroic risk-taking, which may be costly signal showing good genes, may be more preferable. Men also perform more altruistic acts in the early stages of a romantic relationship or simply when in the presence of an attractive woman. While both sexes state that kindness is the most preferable trait in a partner there is some evidence that men place less value on this than women and that women may not be more altruistic in presence of an attractive man. Men may even avoid altruistic women in short-term relationships which may be because they expect less success.

People may compete over getting the benefits of a high reputation which may cause competitive altruism. On the other hand, in some experiments a proportion of people do not seem to care about reputation and they do not help more even if this is conspicuous. This may possibly be due to reasons such as psychopathy or that they are so attractive that they need not be seen to be altruistic. The reputational benefits of altruism occur in the future as compared to the immediate costs of altruism in the present. While humans and other organisms generally place less value on future costs/benefits as compared to those in the present, some have shorter time horizons than others and these people tend to be less cooperative.

Explicit extrinsic rewards and punishments have been found to sometimes actually have the opposite effect on behaviors compared to intrinsic rewards. This may be because such extrinsic, top-down incentives may replace (partially or in whole) intrinsic and reputational incentives, motivating the person to focus on obtaining the extrinsic rewards, which overall may make the behaviors less desirable. Another effect is that people would like altruism to be due to a personality characteristic rather than due to overt reputational concerns and simply pointing out that there are reputational benefits of an action may actually reduce them. This may possibly be used as derogatory tactic against altruists, especially by those who are non-cooperators. A counterargument is that doing good due to reputational concerns is better than doing no good at all.
Group selection. It has controversially been argued by some evolutionary scientists such as E. O. Wilson that natural selection can act at the level of non-kin groups to produce adaptations that benefit a non-kin group even if these adaptions are detrimental at the individual level. Thus, while altruistic persons may under some circumstances be outcompeted by less altruistic persons at the individual level, according to group selection theory the opposite may occur at the group level where groups consisting of the more altruistic persons may outcompete groups consisting of the less altruistic persons. Such altruism may only extend to ingroup members while there may instead prejudice and antagonism against outgroup members (See also in-group favoritism). Group selection theory has been criticized by many other evolutionary scientists.
Such explanations do not imply that humans are always consciously calculating how to increase their inclusive fitness when they are doing altruistic acts. Instead, evolution has shaped psychological mechanisms, such as emotions, that promote altruistic behaviors.

Every single instance of altruistic behavior need not always increase inclusive fitness; altruistic behaviors would have been selected for if such behaviors on average increased inclusive fitness in the ancestral environment. This need not imply that on average 50% or more of altruistic acts were beneficial for the altruist in the ancestral environment; if the benefits from helping the right person were very high it would be beneficial to err on the side of caution and usually be altruistic even if in most cases there were no benefits.

The benefits for the altruist may be increased and the costs reduced by being more altruistic towards certain groups. Research has found that people are more altruistic to kin than to no-kin, to friends than to strangers, to those attractive than to those unattractive, to non-competitors than to competitors, and to members ingroups than to members of outgroup.

The study of altruism was the initial impetus behind George R. Price's development of the Price equation, which is a mathematical equation used to study genetic evolution. An interesting example of altruism is found in the cellular slime moulds, such as Dictyostelium mucoroides. These protists live as individual amoebae until starved, at which point they aggregate and form a multicellular fruiting body in which some cells sacrifice themselves to promote the survival of other cells in the fruiting body.

Selective investment theory proposes that close social bonds, and associated emotional, cognitive, and neurohormonal mechanisms, evolved in order to facilitate long-term, high-cost altruism between those closely depending on one another for survival and reproductive success.

Such cooperative behaviors have sometimes been seen as arguments for left-wing politics such by the Russian zoologist and anarchist Peter Kropotkin in his 1902 book  and Peter Singer in his book A Darwinian Left.

Jorge Moll and Jordan Grafman, neuroscientists at the National Institutes of Health and LABS-D'Or Hospital Network (J.M.) provided the first evidence for the neural bases of altruistic giving in normal healthy volunteers, using functional magnetic resonance imaging. In their research, published in the Proceedings of the National Academy of Sciences USA in October 2006, they showed that both pure monetary rewards and charitable donations activated the mesolimbic reward pathway, a primitive part of the brain that usually lights up in response to food and sex. However, when volunteers generously placed the interests of others before their own by making charitable donations, another brain circuit was selectively activated: the subgenual cortex/septal region. These structures are intimately related to social attachment and bonding in other species. Altruism, the experiment suggested, was not a superior moral faculty that suppresses basic selfish urges but rather was basic to the brain, hard-wired and pleasurable.

The International Encyclopedia of the Social Sciences defines psychological altruism as "a motivational state with the goal of increasing anotherâ€™s welfare." Psychological altruism is contrasted with psychological egoism, which refers to the motivation to increase oneâ€™s own welfare.

There has been some debate on whether or not humans are truly capable of psychological altruism. Some definitions specify a self-sacrificial nature to altruism and a lack of external rewards for altruistic behaviors. However, because altruism ultimately benefits the self in many cases, the selflessness of altruistic acts is brought to question. The social exchange theory postulates that altruism only exists when benefits outweigh costs. Daniel Batson is a psychologist who examined this question and argues against the social exchange theory. He identified four major motives for altruism: altruism to ultimately benefit the self (egoism), to ultimately benefit the other person (altruism), to benefit a group (collectivism), or to uphold a moral principle (principlism). Altruism that ultimately serves selfish gains is thus differentiated from selfless altruism, but the general conclusion has been that empathy-induced altruism can be genuinely selfless. The empathy-altruism hypothesis basically states that psychological altruism does exist and is evoked by the empathic desire to help someone who is suffering. Feelings of empathic concern are contrasted with feelings of personal distress, which compel people to reduce their own unpleasant emotions. People with empathic concern help others in distress even when exposure to the situation could be easily avoided, whereas those lacking in empathic concern avoid helping unless it is difficult or impossible to avoid exposure to another's suffering. Helping behavior is seen in humans at about two years old, when a toddler is capable of understanding subtle emotional cues.
In psychological research on altruism, studies often observe altruism as demonstrated through prosocial behaviors such as helping, comforting, sharing, cooperation, philanthropy, and community service. Research has found that people are most likely to help if they recognize that a person is in need and feel personal responsibility for reducing the person's distress. Research also suggests that the number of bystanders witnessing distress or suffering affects the likelihood of helping (the Bystander effect). Greater numbers of bystanders decrease individual feelings of responsibility. However, a witness with a high level of empathic concern is likely to assume personal responsibility entirely regardless of the number of bystanders. A feeling of personal responsibility or - moral norm - has also strongly been associated with other pro-social behaviors such as charitable giving.

Many studies have observed the effects of volunteerism (as a form of altruism) on happiness and health and have consistently found a strong connection between volunteerism and current and future health and well-being. In a study of older adults, those who volunteered were significantly higher on life satisfaction and will to live, and significantly lower in depression, anxiety, and somatization. Volunteerism and helping behavior have not only been shown to improve mental health, but physical health and longevity as well. One study examined the physical health of mothers who volunteered over a 30-year period and found that 52% of those who did not belong to a volunteer organization experienced a major illness while only 36% of those who did volunteer experienced one. A study on adults ages 55+ found that during the four-year study period, people who volunteered for two or more organizations had a 63% lower likelihood of dying. After controlling for prior health status, it was determined that volunteerism accounted for a 44% reduction in mortality. Merely being aware of kindness in oneself and others is also associated with greater well-being. A study that asked participants to count each act of kindness they performed for one week significantly enhanced their subjective happiness.
It is important to note that, while research supports the idea that altruistic acts bring about happiness, it has also been found to work in the opposite directionâ€”that happier people are also kinder. The relationship between altruistic behavior and happiness is bidirectional. Studies have found that generosity increases linearly from sad to happy affective states. Studies have also been careful to note that feeling over-taxed by the needs of others has conversely negative effects on health and happiness. For example, one study on volunteerism found that feeling overwhelmed by others' demands had an even stronger negative effect on mental health than helping had a positive one (although positive effects were still significant). Additionally, while generous acts make people feel good about themselves, it is also important for people to appreciate the kindness they receive from others. Studies suggest that gratitude goes hand-in-hand with kindness and is also very important for our well-being. A study on the relationship happiness to various character strengths showed that "a conscious focus on gratitude led to reductions in negative affect and increases in optimistic appraisals, positive affect, offering emotional support, sleep quality, and well-being.". Psychologists generally refer to this virtuous cycle of helping others, doing good and subsequently feeling good as "the helper's high".

"Sociologists have long been concerned with how to build the good society" ("Altruism, Morality, and Social Solidarity". American Sociological Association.). The structure of our societies and how individuals come to exhibit charitable, philanthropic, and other pro-social, altruistic actions for the common good is a largely researched topic within the field. The American Sociology Association (ASA) acknowledges Public sociology saying, "The intrinsic scientific, policy, and public relevance of this field of investigation in helping to construct 'good societies' is unquestionable" ("Altruism, Morality, and Social Solidarity" ASA). This type of sociology seeks contributions that aid grassroots and theoretical understandings of what motivates altruism and how it is organized, and promotes an altruistic focus in order to benefit the world and people it studies. How altruism is framed, organized, carried out, and what motivates it at the group level is an area of focus that sociologists seek to investigate in order to contribute back to the groups it studies and "build the good society".

Most, if not all, of the world's religions promote altruism as a very important moral value. Buddhism, Christianity, Hinduism, Islam, Jainism, Judaism and Sikhism, etc., place particular emphasis on altruistic morality.


Altruism figures prominently in Buddhism. Love and compassion are components of all forms of Buddhism, and are focused on all beings equally: love is the wish that all beings be happy, and compassion is the wish that all beings be free from suffering. "Many illnesses can be cured by the one medicine of love and compassion. These qualities are the ultimate source of human happiness, and the need for them lies at the very core of our being" (Dalai Lama).

Since "all beings" includes the individual, love and compassion in Buddhism are outside the opposition between self and other. It is even said that the distinction between self and other is part of the root cause of our suffering. In practical terms, however, since most of us are spontaneously self-centered, Buddhism encourages us to focus love and compassion on others, and thus can be characterized as "altruistic." Many would agree with the Dalai Lama that Buddhism as a religion is kindness toward others.

Still, the notion of altruism is modified in such a world-view, since the belief is that such a practice promotes our own happiness: "The more we care for the happiness of others, the greater our own sense of well-being becomes" (Dalai Lama).

In the context of larger ethical discussions on moral action and judgment, Buddhism is characterized by the belief that negative (unhappy) consequences of our actions derive not from punishment or correction based on moral judgment, but from the law of karma, which functions like a natural law of cause and effect. A simple illustration of such cause and effect is the case of experiencing the effects of what I cause: if I cause suffering, then as a natural consequence I will experience suffering; if I cause happiness, then as a natural consequence I will experience happiness.
In Buddhism, karma (PÄli kamma) is strictly distinguished from vipÄka, meaning "fruit" or "result". Karma is categorized within the group or groups of cause (PÄli hetu) in the chain of cause and effect, where it comprises the elements of "volitional activities" (Pali sankhara) and "action" (Pali bhava). Any action is understood to create "seeds" in the mind that sprout into the appropriate results (PÄli vipaka) when they meet the right conditions. Most types of karmas, with good or bad results, will keep one in the wheel of samsÄra; others will liberate one to nirvÄna.

Buddhism relates karma directly to motives behind an action. Motivation usually makes the difference between "good" and "bad", but motivation also includes the aspect of ignorance; so a well-intended action from an ignorant mind can easily be "bad" in that it creates unpleasant results for the "actor."

In Buddhism, karma is not the only cause of all that happens. As taught in the early texts, the commentarial tradition classified causal mechanisms governing the universe in five categories, known as Niyama Dhammas:

Kamma Niyama â€” Consequences of one's actions
Utu Niyama â€” Seasonal changes and climate
Biija Niyama â€” Laws of heredity
Citta Niyama â€” Will of mind
Dhamma Niyama â€” Nature's tendency to produce a perfect type

The fundamental principles of Jainism revolve around the concept of altruism, not only for humans but for all sentient beings. Jainism preaches the view of Ahimsa â€“ to live and let live, thereby not harming sentient beings, i.e. uncompromising reverence for all life. It also considers all living things to be equal. The first Thirthankar, Rishabh introduced the concept of altruism for all living beings, from extending knowledge and experience to others to donation, giving oneself up for others, non-violence and compassion for all living things.

Jainism prescribes a path of non-violence to progress the soul to this ultimate goal. Jains believe that to attain enlightenment and ultimately liberation, one must practice the following ethical principles (major vows) in thought, speech and action. The degree to which these principles are practiced is different for householders and monks. They are:
Non-violence (Ahimsa);
Truthfulness (Satya);
Non-stealing (Asteya);
Celibacy (Brahmacharya);
Non-possession or non-materialism (Aparigraha);

A major characteristic of Jain belief is the emphasis on the consequences of not only physical but also mental behaviors. One's unconquered mind with anger, pride (ego), deceit, greed and uncontrolled sense organs are the powerful enemies of humans. Anger spoils good relations, pride destroys humility, deceit destroys peace and greed destroys everything. Jainism recommends conquering anger by forgiveness, pride (ego) by humility, deceit by straight-forwardness and greed by contentment.

The principle of non-violence seeks to minimize karmas which limit the capabilities of the soul. Jainism views every soul as worthy of respect because it has the potential to become Siddha (Param-atma â€“ "highest soul"). Because all living beings possess a soul, great care and awareness is essential in one's actions. Jainism emphasizes the equality of all life, advocating harmlessness towards all, whether the creatures are great or small. This policy extends even to microscopic organisms. Jainism acknowledges that every person has different capabilities and capacities to practice and therefore accepts different levels of compliance for ascetics and householders. The "great vows" (mahavrata) are prescribed for monks and "limited vows" (anuvrata) are prescribed for householders. In other words, the house-holders are encouraged to practice the five cardinal principles of non-violence, truthfulness, non-stealing, celibacy and non-possessiveness with their current practical limitations while the monks have to observe them very strictly. With consistent practice, it will be possible to overcome the limitations gradually, accelerating the spiritual progress.


Altruism is central to the teachings of Jesus found in the Gospel, especially in the Sermon on the Mount and the Sermon on the Plain. From biblical to medieval Christian traditions, tensions between self-affirmation and other-regard were sometimes discussed under the heading of "disinterested love", as in the Pauline phrase "love seeks not its own interests." In his book Indoctrination and Self-deception, Roderick Hindery tries to shed light on these tensions by contrasting them with impostors of authentic self-affirmation and altruism, by analysis of other-regard within creative individuation of the self, and by contrasting love for the few with love for the many. Love confirms others in their freedom, shuns propaganda and masks, assures others of its presence, and is ultimately confirmed not by mere declarations from others, but by each person's experience and practice from within. As in practical arts, the presence and meaning of love becomes validated and grasped not by words and reflections alone, but in the making of the connection.

St Thomas Aquinas interprets 'You should love your neighbour as yourself' as meaning that love for ourselves is the exemplar of love for others. Considering that "the love with which a man loves himself is the form and root of friendship" and quotes Aristotle that "the origin of friendly relations with others lies in our relations to ourselves," he concluded that though we are not bound to love others more than ourselves, we naturally seek the common good, the good of the whole, more than any private good, the good of a part. However, he thinks we should love God more than ourselves and our neighbours, and more than our bodily lifeâ€”since the ultimate purpose of loving our neighbour is to share in eternal beatitude: a more desirable thing than bodily well being. In coining the word Altruism, as stated above, Comte was probably opposing this Thomistic doctrine, which is present in some theological schools within Catholicism.

Many biblical authors draw a strong connection between love of others and love of God. 1 John 4 states that for one to love God one must love his fellowman, and that hatred of one's fellowman is the same as hatred of God. Thomas Jay Oord has argued in several books that altruism is but one possible form of love. An altruistic action is not always a loving action. Oord defines altruism as acting for the other's good, and he agrees with feminists who note that sometimes love requires acting for one's own good when the other's demands undermine overall well-being.

German philosopher Max Scheler distinguishes two ways in which the strong can help the weak. One way is a sincere expression of Christian love, "motivated by a powerful feeling of security, strength, and inner salvation, of the invincible fullness of oneâ€™s own life and existence". Another way is merely "one of the many modern substitutes for love, ... nothing but the urge to turn away from oneself and to lose oneself in other peopleâ€™s business." At its worst, Scheler says, "love for the small, the poor, the weak, and the oppressed is really disguised hatred, repressed envy, an impulse to detract, etc., directed against the opposite phenomena: wealth, strength, power, largesse."

In Islam, the concept 'Ä«thÄr' (Ø¥ÙŠØ«Ø§Ø±) (altruism) is the notion of 'preferring others to oneself'. For Sufis, this means devotion to others through complete forgetfulness of one's own concerns, where concern for others is rooted to be a demand made by Allah on the human body, considered to be property of Allah alone. The importance lies in sacrifice for the sake of the greater good; Islam considers those practicing Ä«thÄr as abiding by the highest degree of nobility.
This is similar to the notion of chivalry, but unlike that European concept, in i'thar attention is focused on everything in existence. A constant concern for Allah (i.e. God) results in a careful attitude towards people, animals, and other things in this world.
This concept was emphasized by Sufis of Islam like Rabia al-Adawiyya who paid attention to the difference between dedication to Allah (i.e. God) and dedication to people. Thirteenth-century Turkish Sufi poet Yunus Emre explained this philosophy as "YaratÄ±lanÄ± severiz, Yaratandan Ã¶tÃ¼rÃ¼" or We love the creature, because of The Creator. For many Muslims, i'thar must be practiced as a religious obligation during specific Islamic holidays. However, i'thar is also still an Islamic ideal to which all Muslims should strive to adhere at all times.

Judaism defines altruism as the desired goal of creation. The famous Rabbi Abraham Isaac Kook stated that love is the most important attribute in humanity. This is defined as bestowal, or giving, which is the intention of altruism. This can be altruism towards humanity that leads to altruism towards the creator or God. Kabbalah defines God as the force of giving in existence. Rabbi Moshe Chaim Luzzatto in particular focused on the 'purpose of creation' and how the will of God was to bring creation into perfection and adhesion with this upper force.

Modern Kabbalah developed by Rabbi Yehuda Ashlag, in his writings about the future generation, focuses on how society could achieve an altruistic social framework. Ashlag proposed that such a framework is the purpose of creation, and everything that happens is to raise humanity to the level of altruism, love for one another. Ashlag focused on society and its relation to divinity.

Altruism is essential to the Sikh religion. 
The central faith in Sikhism is that the greatest deed any one can do is to imbibe and live the godly qualities like love, affection, sacrifice, patience, harmony, truthfulness.
The fifth Nanak, Guru Arjun Dev sacrificed his life to uphold 22 carats of pure truth, the greatest gift to humanity, the Guru Granth.  The ninth Nanak, Guru Tegh Bahadur, sacrificed his head to protect weak and defenseless people against atrocity. In the late seventeenth century, Guru Gobind Singh Ji (the tenth guru in Sikhism), was in war with the Moghul rulers to protect the people of different faiths when a fellow Sikh, Bhai Kanhaiya, attended the troops of the enemy. He gave water to both friends and foes who were wounded on the battlefield. Some of the enemy began to fight again and some Sikh warriors were annoyed by Bhai Kanhaiya as he was helping their enemy. Sikh soldiers brought Bhai Kanhaiya before Guru Gobind Singh Ji, and complained of his action that they considered counter-productive to their struggle on the battlefield.
"What were you doing, and why?" asked the Guru. "I was giving water to the wounded because I saw your face in all of them," replied Bhai Kanhaiya.
The Guru responded, "Then you should also give them ointment to heal their wounds. You were practicing what you were coached in the house of the Guru."

It was under the tutelage of the Guru that Bhai Kanhaiya subsequently founded a volunteer corps for altruism. This volunteer corps still to date is engaged in doing good to others and trains new volunteering recruits for doing the same.

Swami Sivananda, an Advaita scholar, reiterates the same views in his commentary synthesising Vedanta views on the Brahma Sutras, a Vedantic text. In his commentary on Chapter 3 of the Brahma Sutras, Sivananda notes that karma is insentient and short-lived, and ceases to exist as soon as a deed is executed. Hence, karma cannot bestow the fruits of actions at a future date according to one's merit. Furthermore, one cannot argue that karma generates apurva or punya, which gives fruit. Since apurva is non-sentient, it cannot act unless moved by an intelligent being such as a god. It cannot independently bestow reward or punishment.

There exists a wide range of philosophical views on man's obligations or motivations to act altruistically. Proponents of ethical altruism maintain that individuals are morally obligated to act altruistically. The opposing view is ethical egoism, which maintains that moral agents should always act in their own self-interest. Both ethical altruism and ethical egoism contrast with utilitarianism, which is the view that every individual's well-being (including one's own) is of equal moral importance.

A related concept in descriptive ethics is psychological egoism, the thesis that humans always act in their own self-interest and that true altruism is impossible. Rational egoism is the view that rationality consists in acting in one's self-interest (without specifying how this affects one's moral obligations).

Bibliography
Comte, Auguste, Catechisme positiviste (1852) or Catechism of Positivism, tr. R. Congreve, (London: Kegan Paul, 1891)

Kropotkin, Peter,  (1902)

Nietzsche, Friedrich, Beyond Good and Evil
Pierre-Joseph Proudhon, The Philosophy of Poverty (1847)
Lysander Spooner, Natural Law
Matt Ridley, The Origins of Virtue
Oliner, Samuel P. and Pearl M. Towards a Caring Society: Ideas into Action. West Port, CT: Praeger, 1995.

General
Society
 from Altruists International
Philosophy and religion
 from Kabbalah.info
 by Sungtaek Cho
Science
, by Dan Batson and Nadia Ahmad

 at Humboldt State University


Ayn Rand (; born Alisa Zinov'yevna Rosenbaum; â€“ March 6, 1982) was a Russian-American novelist, philosopher, playwright, and screenwriter. She is known for her two best-selling novels, The Fountainhead and Atlas Shrugged, and for developing a philosophical system she called Objectivism. Born and educated in Russia, Rand moved to the United States in 1926. She had a play produced on Broadway in 1935â€“1936. After two early novels that were initially unsuccessful in America, she achieved fame with her 1943 novel, The Fountainhead.

In 1957, she published her best-known work, the novel Atlas Shrugged. Afterward, she turned to non-fiction to promote her philosophy, publishing her own magazines and releasing several collections of essays until her death in 1982. Rand advocated reason as the only means of acquiring knowledge and rejected faith and religion. She supported rational and ethical egoism, and rejected altruism. In politics, she condemned the initiation of force as immoral and opposed collectivism and statism as well as anarchism, instead supporting laissez-faire capitalism, which she defined as the system based on recognizing individual rights. In art, Rand promoted romantic realism. She was sharply critical of most philosophers and philosophical traditions known to her, except for some Aristotelians and classical liberals.

Literary critics received Rand's fiction with mixed reviews, and academia generally ignored or rejected her philosophy, though academic interest has increased in recent decades. The Objectivist movement attempts to spread her ideas, both to the public and in academic settings. She has been a significant influence among libertarians and American conservatives.


Rand was born Alisa Zinov'yevna Rosenbaum () on February 2, 1905, to a Russian Jewish bourgeois family living in Saint Petersburg. She was the eldest of the three daughters of Zinovy Zakharovich Rosenbaum and his wife, Anna Borisovna (nÃ©e Kaplan), largely non-observant Jews. Zinovy Rosenbaum was a successful pharmacist and businessman, eventually owning a pharmacy and the building in which it was located. With a passion for the liberal arts, Rand later said she found school unchallenging and she began writing screenplays at the age of eight and novels at the age of ten. At the prestigious Stoiunina Gymnasium, her closest friend was Vladimir Nabokov's younger sister, Olga. The two girls shared an intense interest in politics and would engage in debates: while Nabokova defended constitutional monarchy, Rand supported republican ideals. She was twelve at the time of the February Revolution of 1917, during which she favored Alexander Kerensky over Tsar Nicholas II.

The subsequent October Revolution and the rule of the Bolsheviks under Vladimir Lenin disrupted the life the family had previously enjoyed. Her fatherâ€™s business was confiscated and the family displaced. They fled to the Crimean Peninsula, which was initially under control of the White Army during the Russian Civil War. She later recalled that, while in high school, she determined that she was an atheist and that she valued reason above any other human virtue. After graduating from high school in the Crimea at 16, Rand returned with her family to Petrograd (as Saint Petersburg was renamed at that time), where they faced desperate conditions, on occasion nearly starving.
After the Russian Revolution, universities were opened to women, allowing Rand to be in the first group of women to enroll at Petrograd State University, where, at the age of 16, she began her studies in the department of social pedagogy, majoring in history. At the university she was introduced to the writings of Aristotle and Plato, who would be her greatest influence and counter-influence, respectively. A third figure whose philosophical works she studied heavily was Friedrich Nietzsche. Able to read French, German and Russian, Rand also discovered the writers Fyodor Dostoevsky, Victor Hugo, Edmond Rostand, and Friedrich Schiller, who became her perennial favorites.

Along with many other "bourgeois" students, Rand was purged from the university shortly before graduating. However, after complaints from a group of visiting foreign scientists, many of the purged students were allowed to complete their work and graduate, which Rand did in October 1924. She subsequently studied for a year at the State Technicum for Screen Arts in Leningrad. For one of her assignments, she wrote an essay about the Polish-American actress Pola Negri, which became her first published work.

By this time she had decided her professional surname for writing would be Rand, possibly as a Cyrillic contraction of her birth surname, and she adopted the first name Ayn, either from a Finnish name Aino or from the Hebrew word  (ayin, meaning "eye").


In the autumn of 1925, Rand was granted a visa to visit American relatives. She departed on January 17, 1926. When she arrived in New York City on February 19, 1926, she was so impressed with the skyline of Manhattan that she cried what she later called "tears of splendor". Intent on staying in the United States to become a screenwriter, she lived for a few months with relatives in Chicago, one of whom owned a movie theater and allowed her to watch dozens of films for free. She then set out for Hollywood, California.

Initially, Rand struggled in Hollywood and took odd jobs to pay her basic living expenses. A chance meeting with famed director Cecil B. DeMille led to a job as an extra in his film The King of Kings as well as subsequent work as a junior screenwriter. While working on The King of Kings, she met an aspiring young actor, Frank O'Connor; the two were married on April 15, 1929, around the time her last visa extension was set to expire. She became a permanent US resident in July 1929, and became an American citizen on March 3, 1931. Taking various jobs during the 1930s to support her writing, she worked for a time as the head of the costume department at RKO Studios. She made several attempts to bring her parents and sisters to the United States, but they were unable to acquire permission to emigrate.


Rand's first literary success came with the sale of her screenplay Red Pawn to Universal Studios in 1932, although it was never produced. This was followed by the courtroom drama Night of January 16th, first produced by E.E. Clive in Hollywood in 1934 and then successfully reopened on Broadway in 1935. Each night the "jury" was selected from members of the audience, and one of the two different endings, depending on the jury's "verdict", would then be performed. In 1941, Paramount Pictures produced a movie loosely based on the play. Rand did not participate in the production and was highly critical of the result.

Rand's first novel, the semi-autobiographical We the Living, was published in 1936. Set in Soviet Russia, it focused on the struggle between the individual and the state. In a 1959 foreword to the novel, Rand stated that We the Living "is as near to an autobiography as I will ever write. It is not an autobiography in the literal, but only in the intellectual sense. The plot is invented, the background is not..." Initial sales were slow and the American publisher let it go out of print, although European editions continued to sell. After the success of her later novels, Rand was able to release a revised version in 1959 that has since sold over three million copies. In 1942, without Rand's knowledge or permission, the novel was made into a pair of Italian films, Noi vivi and Addio, Kira. Rediscovered in the 1960s, these films were re-edited into a new version which was approved by Rand and re-released as We the Living in 1986.

Her novella Anthem was written during a break from the writing of her next major novel, The Fountainhead. It presents a vision of a dystopian future world in which totalitarian collectivism has triumphed to such an extent that even the word 'I' has been forgotten and replaced with 'we'. It was published in England in 1938, but Rand initially could not find an American publisher. As with We the Living, Rand's later success allowed her to get a revised version published in 1946, which has sold more than 3.5 million copies.


During the 1940s, Rand became politically active. Both she and her husband worked full-time in volunteer positions for the 1940 presidential campaign of Republican Wendell Willkie. This work led to Rand's first public speaking experiences, including fielding the sometimes hostile questions from New York City audiences who had just viewed pro-Willkie newsreels, an experience she greatly enjoyed. This activity also brought her into contact with other intellectuals sympathetic to free-market capitalism. She became friends with journalist Henry Hazlitt and his wife, and Hazlitt introduced her to the Austrian School economist Ludwig von Mises. Despite her philosophical differences with them, Rand strongly endorsed the writings of both men throughout her career, and both of them expressed admiration for her. Once Mises referred to Rand as "the most courageous man in America", a compliment that particularly pleased her because he said "man" instead of "woman". Rand also developed a friendship with libertarian writer Isabel Paterson. Rand questioned the well-informed Paterson about American history and politics long into the night during their numerous meetings and gave Paterson ideas for her only nonfiction book, The God of the Machine.

Rand's first major success as a writer came with The Fountainhead in 1943, a romantic and philosophical novel that she wrote over a period of seven years. The novel centers on an uncompromising young architect named Howard Roark and his struggle against what Rand described as "second-handers"â€”those who attempt to live through others, placing others above themselves. It was rejected by twelve publishers before finally being accepted by the Bobbs-Merrill Company on the insistence of editor Archibald Ogden, who threatened to quit if his employer did not publish it. While completing the novel, Rand was prescribed Benzedrine, a brand of amphetamine, to fight fatigue. The drug helped her to work long hours to meet her deadline for delivering the finished novel, but when the book was done, she was so exhausted that her doctor ordered two weeks' rest. Her use of the drug for approximately three decades may have contributed to what some of her later associates described as volatile mood swings.

The Fountainhead eventually became a worldwide success, bringing Rand fame and financial security. In 1943, Rand sold the rights for a film version to Warner Bros., and she returned to Hollywood to write the screenplay. Finishing her work on that screenplay, she was hired by producer Hal Wallis as a screenwriter and script-doctor. Her work for Wallis included the screenplays for the Oscar-nominated Love Letters and You Came Along. This role gave Rand time to work on other projects, including a planned nonfiction treatment of her philosophy to be called The Moral Basis of Individualism. Although the planned book was never completed, a condensed version was published as an essay titled "The Only Path to Tomorrow", in the January 1944 edition of Reader's Digest magazine.
Rand extended her involvement with free-market and anti-communist activism while working in Hollywood. She became involved with the Motion Picture Alliance for the Preservation of American Ideals, a Hollywood anti-Communist group, and wrote articles on the group's behalf. She also joined the anti-Communist American Writers Association. A visit by Isabel Paterson to meet with Rand's California associates led to a final falling out between the two when Paterson made comments that Rand saw as rude to valued political allies. In 1947, during the Second Red Scare, Rand testified as a "friendly witness" before the United States House Un-American Activities Committee. Her testimony described the disparity between her personal experiences in the Soviet Union and the portrayal of it in the 1944 film Song of Russia. Rand argued that the film grossly misrepresented conditions in the Soviet Union, portraying life there as being much better and happier than it actually was. She wanted to also criticize the lauded 1946 film The Best Years of Our Lives for what she interpreted as its negative presentation of the business world, but she was not allowed to testify about it. When asked after the hearings about her feelings on the effectiveness of the investigations, Rand described the process as "futile".

After several delays, the film version of The Fountainhead was released in 1949. Although it used Rand's screenplay with minimal alterations, she "disliked the movie from beginning to end", complaining about its editing, acting, and other elements.


In the years following the publication of The Fountainhead, Rand received numerous letters from readers, some of whom it profoundly influenced. In 1951 Rand moved from Los Angeles to New York City, where she gathered a group of these admirers around her. This group (jokingly designated "The Collective") included future Federal Reserve Chairman Alan Greenspan, a young psychology student named Nathan Blumenthal (later Nathaniel Branden) and his wife Barbara, and Barbara's cousin Leonard Peikoff. At first the group was an informal gathering of friends who met with Rand on weekends at her apartment to discuss philosophy. Later she began allowing them to read the drafts of her new novel, Atlas Shrugged, as the manuscript pages were written. In 1954 Rand's close relationship with the younger Nathaniel Branden turned into a romantic affair, with the consent of their spouses.

Atlas Shrugged, published in 1957, was considered Rand's magnum opus. Rand described the theme of the novel as "the role of the mind in man's existenceâ€”and, as a corollary, the demonstration of a new moral philosophy: the morality of rational self-interest." It advocates the core tenets of Rand's philosophy of Objectivism and expresses her concept of human achievement. The plot involves a dystopian United States in which the most creative industrialists, scientists, and artists go on strike and retreat to a mountainous hideaway where they build an independent free economy. The novel's hero and leader of the strike, John Galt, describes the strike as "stopping the motor of the world" by withdrawing the minds of the individuals most contributing to the nation's wealth and achievement. With this fictional strike, Rand intended to illustrate that without the efforts of the rational and productive, the economy would collapse and society would fall apart. The novel includes elements of romance, mystery, and science fiction, and it contains Rand's most extensive statement of Objectivism in any of her works of fiction, a lengthy monologue delivered by Galt.

Despite many negative reviews, Atlas Shrugged became an international bestseller, and in an interview with Mike Wallace, Rand declared herself "the most creative thinker alive". After completing the novel, Rand fell into a severe depression. Atlas Shrugged was Rand's last completed work of fiction; a turning point in her life, it marked the end of Rand's career as a novelist and the beginning of her role as a popular philosopher.

In 1958 Nathaniel Branden established Nathaniel Branden Lectures, later incorporated as the Nathaniel Branden Institute (NBI), to promote Rand's philosophy. Collective members gave lectures for NBI and wrote articles for Objectivist periodicals that she edited. Rand later published some of these articles in book form. Critics, including some former NBI students and Branden himself, have described the culture of NBI as one of intellectual conformity and excessive reverence for Rand, with some describing NBI or the Objectivist movement itself as a cult or religion. Rand expressed opinions on a wide range of topics, from literature and music to sexuality and facial hair, and some of her followers mimicked her preferences, wearing clothes to match characters from her novels and buying furniture like hers. Rand was unimpressed with many of the NBI students and held them to strict standards, sometimes reacting coldly or angrily to those who disagreed with her. However, some former NBI students believe the extent of these behaviors has been exaggerated, with the problem being concentrated among Rand's closest followers in New York.

Throughout the 1960s and 1970s, Rand developed and promoted her Objectivist philosophy through her nonfiction works and by giving talks to students at institutions such as Yale, Princeton, Columbia, Harvard, and MIT. She received an honorary doctorate from Lewis & Clark College in 1963. She also began delivering annual lectures at the Ford Hall Forum, responding afterward to questions from the audience. During these speeches and Q sessions, she often took controversial stances on political and social issues of the day. These included supporting abortion rights, opposing the Vietnam War and the military draft (but condemning many draft dodgers as "bums"), supporting Israel in the Yom Kippur War of 1973 against a coalition of Arab nations as "civilized men fighting savages", saying European colonists had the right to take land from American Indians, and calling homosexuality "immoral" and "disgusting", while also advocating the repeal of all laws about it. She also endorsed several Republican candidates for President of the United States, most strongly Barry Goldwater in 1964, whose candidacy she promoted in several articles for The Objectivist Newsletter.

In 1964 Nathaniel Branden began an affair with the young actress Patrecia Scott, whom he later married. Nathaniel and Barbara Branden kept the affair hidden from Rand. When she learned of it in 1968, though her romantic relationship with Branden had already ended, Rand terminated her relationship with both Brandens, which led to the closure of NBI. Rand published an article in The Objectivist repudiating Nathaniel Branden for dishonesty and other "irrational behavior in his private life". Branden later apologized in an interview to "every student of Objectivism" for "perpetuating the Ayn Rand mystique" and for "contributing to that dreadful atmosphere of intellectual repressiveness that pervades the Objectivist movement." In subsequent years, Rand and several more of her closest associates parted company.

Rand underwent surgery for lung cancer in 1974 after decades of heavy smoking.
In 1976, she retired from writing her newsletter and, despite her initial objections, allowed Evva Pryor, a social worker from her attorney's office, to enroll her in Social Security and Medicare. During the late 1970s her activities within the Objectivist movement declined, especially after the death of her husband on November 9, 1979. One of her final projects was work on a never-completed television adaptation of Atlas Shrugged.

Rand died of heart failure on March 6, 1982, at her home in New York City, and was interred in the Kensico Cemetery, Valhalla, New York. Rand's funeral was attended by some of her prominent followers, including Alan Greenspan. A  floral arrangement in the shape of a dollar sign was placed near her casket. In her will, Rand named Leonard Peikoff the heir to her estate.


Rand called her philosophy "Objectivism", describing its essence as "the concept of man as a heroic being, with his own happiness as the moral purpose of his life, with productive achievement as his noblest activity, and reason as his only absolute." She considered Objectivism a systematic philosophy and laid out positions on metaphysics, epistemology, ethics, political philosophy and aesthetics.

In metaphysics, Rand supported philosophical realism, and opposed anything she regarded as mysticism or supernaturalism, including all forms of religion. 

In epistemology, she considered all knowledge to be based on sense perception, the validity of which she considered axiomatic, and reason, which she described as "the faculty that identifies and integrates the material provided by man's senses." She rejected all claims of non-perceptual or a priori knowledge, including "'instinct,' 'intuition,' 'revelation,' or any form of 'just knowing. Rand argued that the requirements of cognition determine the objective criteria of conceptualization, which she summarized in the form of a philosophical razor. Known as Rand's razor it states that "concepts are not to be multiplied beyond necessityâ€”the corollary of which is: nor are they to be integrated in disregard of necessity."  In her Introduction to Objectivist Epistemology, Rand presented a theory of concept formation and rejected the analyticâ€“synthetic dichotomy.

In ethics, Rand argued for rational and ethical egoism (rational self-interest), as the guiding moral principle. She said the individual should "exist for his own sake, neither sacrificing himself to others nor sacrificing others to himself." She referred to egoism as "the virtue of selfishness" in her book of that title, in which she presented her solution to the is-ought problem by describing a meta-ethical theory that based morality in the needs of "man's survival qua man". She condemned ethical altruism as incompatible with the requirements of human life and happiness, and held that the initiation of force was evil and irrational, writing in Atlas Shrugged that "Force and mind are opposites."

Rand's political philosophy emphasized individual rights (including property rights), and she considered laissez-faire capitalism the only moral social system because in her view it was the only system based on the protection of those rights. She opposed statism, which she understood to include theocracy, absolute monarchy, Nazism, fascism, communism, democratic socialism, and dictatorship. Rand believed that rights should be enforced by a constitutionally limited government. Although her political views are often classified as conservative or libertarian, she preferred the term "radical for capitalism". She worked with conservatives on political projects, but disagreed with them over issues such as religion and ethics. She denounced libertarianism, which she associated with anarchism. She rejected anarchism as a naÃ¯ve theory based in subjectivism that could only lead to collectivism in practice.

Rand's aesthetics defined art as a "selective re-creation of reality according to an artist's metaphysical value-judgments." According to Rand, art allows philosophical concepts to be presented in a concrete form that can be easily grasped, thereby fulfilling a need of human consciousness. As a writer, the art form Rand focused on most closely was literature, where she considered romanticism to be the approach that most accurately reflected the existence of human free will. She described her own approach to literature as "romantic realism".

Rand acknowledged Aristotle as her greatest influence and remarked that in the history of philosophy she could only recommend "three A's"â€”Aristotle, Aquinas, and Ayn Rand. In a 1959 interview with Mike Wallace, when asked where her philosophy came from, she responded, "Out of my own mind, with the sole acknowledgement of a debt to Aristotle, the only philosopher who ever influenced me. I devised the rest of my philosophy myself." However, she also found early inspiration in Friedrich Nietzsche, and scholars have found indications of his influence in early notes from Rand's journals, in passages from the first edition of We the Living (which Rand later revised), and in her overall writing style. However, by the time she wrote The Fountainhead, Rand had turned against Nietzsche's ideas, and the extent of his influence on her even during her early years is disputed. Among the philosophers Rand held in particular disdain was Immanuel Kant, whom she referred to as a "monster", although philosophers George Walsh and Fred Seddon have argued that she misinterpreted Kant and exaggerated their differences.

Rand said her most important contributions to philosophy were her "theory of concepts,  ethics, and  discovery in politics that evilâ€”the violation of rightsâ€”consists of the initiation of force." She believed epistemology was a foundational branch of philosophy and considered the advocacy of reason to be the single most significant aspect of her philosophy, stating, "I am not primarily an advocate of capitalism, but of egoism; and I am not primarily an advocate of egoism, but of reason. If one recognizes the supremacy of reason and applies it consistently, all the rest follows."

During Rand's lifetime, her work evoked both extreme praise and condemnation. Rand's first novel, We the Living, was admired by the literary critic H. L. Mencken, her Broadway play Night of January 16th was both a critical and popular success, and The Fountainhead was hailed by a reviewer in The New York Times as "masterful". Rand's novels were derided by some critics when they were first published as being long and melodramatic. However, they became bestsellers largely through word of mouth.

The first reviews Rand received were for Night of January 16th. Reviews of the production were largely positive, but Rand considered even positive reviews to be embarrassing because of significant changes made to her script by the producer. Rand believed that her first novel, We the Living, was not widely reviewed, but Rand scholar Michael S. Berliner says "it was the most reviewed of any of her works", with approximately 125 different reviews being published in more than 200 publications. Overall these reviews were more positive than the reviews she received for her later work. Her 1938 novella Anthem received little attention from reviewers, both for its first publication in England and for subsequent re-issues.

Rand's first bestseller, The Fountainhead, received far fewer reviews than We the Living, and reviewers' opinions were mixed. There was a positive review in The New York Times that Rand greatly appreciated. The reviewer called Rand "a writer of great power" who wrote "brilliantly, beautifully and bitterly", and stated that "you will not be able to read this masterful book without thinking through some of the basic concepts of our time". There were other positive reviews, but Rand dismissed most of them as either not understanding her message or as being from unimportant publications. Some negative reviews focused on the length of the novel, such as one that called it "a whale of a book" and another that said "anyone who is taken in by it deserves a stern lecture on paper-rationing". Other negative reviews called the characters unsympathetic and Rand's style "offensively pedestrian".

Rand's 1957 novel Atlas Shrugged was widely reviewed, and many of the reviews were strongly negative. In the National Review, conservative author Whittaker Chambers called the book "sophomoric" and "remarkably silly". He described the tone of the book as "shrillness without reprieve" and accused Rand of supporting a godless system (which he related to that of the Soviets), claiming "From almost any page of Atlas Shrugged, a voice can be heard, from painful necessity, commanding: 'To a gas chamberâ€”go! Atlas Shrugged received positive reviews from a few publications, including praise from the noted book reviewer John Chamberlain, but Rand scholar Mimi Reisel Gladstein later wrote that "reviewers seemed to vie with each other in a contest to devise the cleverest put-downs", calling it "execrable claptrap" and "a nightmare"; they said it was "written out of hate" and showed "remorseless hectoring and prolixity". Author Flannery O'Connor wrote in a letter to a friend that "The fiction of Ayn Rand is as low as you can get re fiction. I hope you picked it up off the floor of the subway and threw it in the nearest garbage pail."

Rand's nonfiction received far fewer reviews than her novels had. The tenor of the criticism for her first nonfiction book, For the New Intellectual, was similar to that for Atlas Shrugged, with philosopher Sidney Hook likening her certainty to "the way philosophy is written in the Soviet Union", and author Gore Vidal calling her viewpoint "nearly perfect in its immorality". Her subsequent books got progressively less attention from reviewers.

On the 100th anniversary of Rand's birth in 2005, Edward Rothstein, writing for The New York Times, referred to her fictional writing as quaint utopian "retro fantasy" and programmatic neo-Romanticism of the misunderstood artist, while criticizing her characters' "isolated rejection of democratic society". In 2007, book critic Leslie Clark described her fiction as "romance novels with a patina of pseudo-philosophy". In 2009, GQs critic columnist Tom Carson described her books as "capitalism's version of middlebrow religious novels" such as  and the Left Behind series.


In 1991, a survey conducted for the Library of Congress and the Book-of-the-Month Club asked club members what the most influential book in the respondent's life was. Rand's Atlas Shrugged was the second most popular choice, after the Bible. Rand's books continue to be widely sold and read, with over 29million copies sold as of 2013 (with about 10% of that total purchased for free distribution to schools by the Ayn Rand Institute). Although Rand's influence has been greatest in the United States, there has been international interest in her work. Rand's work continues to be among the top sellers among books in India.

Rand's contemporary admirers included fellow novelists, such as Ira Levin, Kay Nolte Smith and L. Neil Smith, and later writers such as Erika Holzer and Terry Goodkind have been influenced by her. Other artists who have cited Rand as an important influence on their lives and thought include comic book artist Steve Ditko and musician Neil Peart of Rush. Rand provided a positive view of business, and in response business executives and entrepreneurs have admired and promoted her work. John Allison of BB and Ed Snider of Comcast Spectacor have funded the promotion of Rand's ideas, while Mark Cuban, owner of the Dallas Mavericks, and John P. Mackey, CEO of Whole Foods, among others, have said they consider Rand crucial to their success.

Rand and her works have been referred to in a variety of media: on television shows including animated sitcoms, live-action comedies, dramas, and game shows, as well as in movies and video games. She, or characters based on her, figure prominently (in positive and negative lights) in literary and science fiction novels by prominent American authors. Nick Gillespie, editor in chief of Reason, has remarked that "Rand's is a tortured immortality, one in which she's as likely to be a punch line as a protagonist..." and that "jibes at Rand as cold and inhuman, run through the popular culture". Two movies have been made about Rand's life. A 1997 documentary film, , was nominated for the Academy Award for Documentary Feature. The Passion of Ayn Rand, a 1999 television adaptation of the book of the same name, won several awards. Rand's image also appears on a U.S. postage stamp designed by artist Nick Gaetano.


Although she rejected the labels "conservative" and "libertarian", Rand has had continuing influence on right-wing politics and libertarianism. Jim Powell, a senior fellow at the Cato Institute, considers Rand one of the three most important women (along with Rose Wilder Lane and Isabel Paterson) of modern American libertarianism, and David Nolan, one of the founders of the Libertarian Party, stated that "without Ayn Rand, the libertarian movement would not exist". In his history of the libertarian movement, journalist Brian Doherty described her as "the most influential libertarian of the twentieth century to the public at large", and biographer Jennifer Burns referred to her as "the ultimate gateway drug to life on the right".
She faced intense opposition from William F. Buckley, Jr. and other contributors for the National Review magazine. They published numerous criticisms in the 1950s and 1960s by Whittaker Chambers, Garry Wills, and M. Stanton Evans. Nevertheless, her influence among conservatives forced Buckley and other National Review contributors to reconsider how traditional notions of virtue and Christianity could be integrated with support for capitalism.

The political figures who cite Rand as an influence are usually conservatives (often members of the United States Republican Party), despite Rand taking some positions that are atypical for conservatives, such as being pro-choice and an atheist. A 1987 article in The New York Times referred to her as the Reagan administration's "novelist laureate". Republican Congressmen and conservative pundits have acknowledged her influence on their lives and recommended her novels.

The late-2000s financial crisis spurred renewed interest in her works, especially Atlas Shrugged, which some saw as foreshadowing the crisis, and opinion articles compared real-world events with the plot of the novel. During this time, signs mentioning Rand and her fictional hero John Galt appeared at Tea Party protests. There was also increased criticism of her ideas, especially from the political left, with critics blaming the economic crisis on her support of selfishness and free markets, particularly through her influence on Alan Greenspan. For example, Mother Jones remarked that "Rand's particular genius has always been her ability to turn upside down traditional hierarchies and recast the wealthy, the talented, and the powerful as the oppressed", while equating Randian individual well-being with that of the Volk according to Goebbels, Corey Robin of The Nation alleged similarities between the "moral syntax of Randianism" and fascism.

During Rand's lifetime her work received little attention from academic scholars. When the first academic book about Rand's philosophy appeared in 1971, its author declared writing about Rand "a treacherous undertaking" that could lead to "guilt by association" for taking her seriously. A few articles about Rand's ideas appeared in academic journals before her death in 1982, many of them in The Personalist. One of these was "On the Randian Argument" by libertarian philosopher Robert Nozick, who argued that her meta-ethical argument is unsound and fails to solve the isâ€“ought problem posed by David Hume. Some responses to Nozick by other academic philosophers were also published in The Personalist arguing that Nozick misstated Rand's case. Academic consideration of Rand as a literary figure during her life was even more limited. Academic Mimi Gladstein was unable to find any scholarly articles about Rand's novels when she began researching her in 1973, and only three such articles appeared during the rest of the 1970s.

Since Rand's death, interest in her work has gradually increased. Historian Jennifer Burns has identified "three overlapping waves" of scholarly interest in Rand, the most recent of which is "an explosion of scholarship" since the year 2000. However, few universities currently include Rand or Objectivism as a philosophical specialty or research area, with many literature and philosophy departments dismissing her as a pop culture phenomenon rather than a subject for serious study.

Gladstein, Chris Matthew Sciabarra, Allan Gotthelf, Edwin A. Locke and Tara Smith have taught her work in academic institutions. Sciabarra co-edits the Journal of Ayn Rand Studies, a nonpartisan peer-reviewed journal dedicated to the study of Rand's philosophical and literary work. In 1987 Gotthelf helped found the Ayn Rand Society with George Walsh and David Kelley, and has been active in sponsoring seminars about Rand and her ideas. Smith has written several academic books and papers on Rand's ideas, including Ayn Rand's Normative Ethics: The Virtuous Egoist, a volume on Rand's ethical theory published by Cambridge University Press. Rand's ideas have also been made subjects of study at Clemson and Duke universities. Scholars of English and American literature have largely ignored her work, although attention to her literary work has increased since the 1990s.

Rand scholars Douglas Den Uyl and Douglas B. Rasmussen, while stressing the importance and originality of her thought, describe her style as "literary, hyperbolic and emotional". Philosopher Jack Wheeler says that despite "the incessant bombast and continuous venting of Randian rage", Rand's ethics are "a most immense achievement, the study of which is vastly more fruitful than any other in contemporary thought." In the Literary Encyclopedia entry for Rand written in 2001, John David Lewis declared that "Rand wrote the most intellectually challenging fiction of her generation". In a 1999 interview in the Chronicle of Higher Education, Sciabarra commented, "I know they laugh at Rand", while forecasting a growth of interest in her work in the academic community.

Libertarian philosopher Michael Huemer has argued that very few people find Rand's ideas convincing, especially her ethics, which he believes is difficult to interpret and may lack logical coherence. He attributes the attention she receives to her being a "compelling writer", especially as a novelist. Thus, Atlas Shrugged outsells not only the works of other philosophers of classical liberalism such as Ludwig von Mises, Friedrich Hayek, or Frederic Bastiat, but also Rand's own non-fiction works.

Political scientist Charles Murray, while praising Rand's literary accomplishments, criticizes her claim that her only "philosophical debt" was to Aristotle, instead asserting that her ideas were derivative of previous thinkers such as John Locke and Friedrich Nietzsche.

Although Rand maintained that Objectivism was an integrated philosophical system, philosopher Robert H. Bass has argued that her central ethical ideas are inconsistent and contradictory to her central political ideas.


In 1985, Rand's heir Leonard Peikoff established the Ayn Rand Institute, a nonprofit organization dedicated to promoting Rand's ideas and works. In 1990, philosopher David Kelley founded the Institute for Objectivist Studies, now known as The Atlas Society. In 2001 historian John McCaskey organized the Anthem Foundation for Objectivist Scholarship, which provides grants for scholarly work on Objectivism in academia. The charitable foundation of BB Corporation has also given grants for teaching Rand's ideas or works. The University of Texas at Austin, the University of Pittsburgh, and University of North Carolina at Chapel Hill are among the schools that have received grants. In some cases these grants have been controversial due to their requiring research or teaching related to Rand.

Novels
1936 We the Living
1943 The Fountainhead
1957 Atlas Shrugged
Other fiction
1934 Night of January 16th
1938 Anthem

Non-fiction
1961 For the New Intellectual
1964 The Virtue of Selfishness
1966 
1969 The Romantic Manifesto
1971 
1979 Introduction to Objectivist Epistemology
1982 

 from the Objectivism Reference Center
 from the Ayn Rand Institute
 by the Cato Institute
 â€“ searchable database
 at C-SPAN's 


Alain Connes (; born 1 April 1947) is a French mathematician, currently Professor at the CollÃ¨ge de France, IHÃ‰S, The Ohio State University and Vanderbilt University.

Alain Connes studies operator algebras. In his early work on von Neumann algebras in the 1970s, he succeeded in obtaining the almost complete classification of injective factors. Following this he made contributions in operator K-theory and index theory, which culminated in the 
Baum-Connes conjecture. He also introduced cyclic cohomology in the early 1980s as a first step in the study of noncommutative differential geometry.

Connes has applied his work in areas of mathematics and theoretical physics, including number theory, differential geometry and particle physics.


Connes was awarded the Fields Medal in 1982, the Crafoord Prize in 2001 and the gold medal of the CNRS in 2004.  He is a member of the French Academy of Sciences and several foreign academies and societies, including the Danish Academy of Sciences, Norwegian Academy of Sciences, Russian Academy of Sciences, and US National Academy of Sciences.

Alain Connes and Matilde Marcolli: Noncommutative Geometry, Quantum Fields and Motives, Colloquium Publications, American Mathematical Society, 2007, ISBN 978-0821842102 
Alain Connes, Andre Lichnerowicz, Marcel Paul Schutzenberger, Jennifer Gage (translator): Triangle of Thought, American Mathematical Society, 2001, ISBN 978-0821826140
Jean-Pierre Changeux, Alain Connes, M. B. DeBevoise (translator): Conversations on Mind, Matter, and Mathematics, Princeton University Press, 1998, ISBN 978-0691004051
Alain Connes: Noncommutative Geometry, Academic Press, 1994, ISBN 978-0121858605

Cyclic homology
Factor (functional analysis)
Higgs boson
C*-algebra
M Theory
Groupoid
Criticism of non-standard analysis


 containing , and his book , ISBN 0-12-185860-X.
An  and a 


Allan Dwan (3 April 1885 â€“ 28 December 1981) was a pioneering Canadian-born American motion picture director, producer and screenwriter.

Born Joseph Aloysius Dwan in Toronto, Ontario, Canada, Dwan moved with his family to the United States when he was eight (8) years old, on April 15, 1893. At the University of Notre Dame, he trained as an engineer and began working for a lighting company in Chicago. However, he had a strong interest in the fledgling motion picture industry and when Essanay Studios offered him the opportunity to become a scriptwriter, he took the job. At that time, some of the East Coast movie makers began to spend winters in California where the climate allowed them to continue productions requiring warm weather. Soon, a number of movie companies worked there year-round and, in 1911, Dwan began working part-time in Hollywood. While still in New York, in 1917 he was the founding president of the East Coast chapter of the Motion Picture Directors Association. 

Dwan operated Flying A Studios in La Mesa, California from August 1911 to July 1912. Flying A was one of the first motion pictures studios in California history. On 12 August 2011, a plaque was unveiled on the Wolff building at Third Ave and La Mesa Bl commemorating Dwan and the Flying A Studios origins in La Mesa, California.

After making a series of westerns and comedies, Dwan directed fellow Canadian-American Mary Pickford in several very successful movies as well as her husband, Douglas Fairbanks, notably in the acclaimed 1922 Robin Hood. Dwan directed Gloria Swanson in eight feature films, and one short film made in the short-lived sound-on-film process Phonofilm. This short, also featuring Thomas Meighan and Henri de la Falaise, was produced as a joke, for the 26 April 1925 "Lambs' Gambol" for The Lambs, with the film showing Swanson crashing the all-male club.

Following the introduction of the talkies, Dwan directed child-star Shirley Temple in Heidi (1937) and Rebecca of Sunnybrook Farm (1938).

Dwan helped launch the career of two other very successful Hollywood directors, Victor Fleming, who went on to direct The Wizard of Oz and Gone With the Wind, and Marshall Neilan, who became an actor, director, writer and producer. Over a long career spanning almost 50 years, Dwan directed over 400motion pictures, many of them highly acclaimed, such as the 1949 box office smash, Sands of Iwo Jima. He directed his last movie in 1961. 

He died in Los Angeles at the age of ninety-six, and is interred in the San Fernando Mission Cemetery, Mission Hills, California.

Allan Dwan has a star on the Hollywood Walk of Fame at 6263 Hollywood Boulevard.


The Gold Lust (1911)
The Picket Guard (1913)
The Restless Spirit (1913)
Back to Life (1913)
Bloodhounds of the North (1913)
The Lie (1914)
The Honor of the Mounted (1914)
Remember Mary Magdalen (1914)
Discord and Harmony (1914)
The Embezzler (1914)
The Lamb, the Woman, the Wolf (1914)
The End of the Feud (1914)
The Tragedy of Whispering Creek (1914)
The Unlawful Trade (1914)
The Forbidden Room (1914)
The Hopes of Blind Alley (1914)
Richelieu (1914)
Wildflower (1914)
A Small Town Girl (1915)
David Harum (1915)
A Girl of Yesterday (1915)
The Pretty Sister of Jose (1915)
Jordan Is a Hard Road  (1915)
Betty of Graystone (1916)
The Habit of Happiness (1916)
The Good Bad Man (1916)
An Innocent Magdalene (1916)
The Half-Breed (1916)
Manhattan Madness (1916)
Accusing Evidence (1916)
Panthea (1917)
A Modern Musketeer (1917)
Headin' South (1918)
Mr. Fix-It (1918)
He Comes Up Smiling (1918)
Getting Mary Married (1919)
In The Heart of a Fool (1920) also producer
The Forbidden Thing (1920) also producer
Robin Hood (1922)
Zaza (1923)
Big Brother (1923)
Manhandled (1924)
Night Life of New York (1925)
Stage Struck (1925)
Gloria Swanson Dialogue (1925) short film made in Phonofilm for The Lambs annual "Gambol" held at Metropolitan Opera House
Tin Gods (1926)
The Joy Girl (1927)
East Side, West Side (1927)
The Big Noise (1928)
The Iron Mask (1929)
Tide of Empire (1929)
The Far Call (1929)
What a Widow! (1930)
Man to Man  (1930)
Chances (1931)
Wicked (1931)
While Paris Sleeps (1932)
Counsel's Opinion (1933)
Black Sheep (1935)
High Tension (1936)
15 Maiden Lane (1936)
One Mile From Heaven (1937)
Heidi (1937)
Rebecca of Sunnybrook Farm (1938)
Suez (1938)
Josette (1938)
The Three Musketeers (1939)
The Gorilla (1939)
Frontier Marshal (1939)
Sailor's Lady (1940)
Young People (1940)
Trail of the Vigilantes (1940)
Look Who's Laughing (1941) also producer
Rise and Shine (1941)
Friendly Enemies (1942)
Around the World (1943) also producer
Up in Mabel's Room (1944)
Abroad with Two Yanks (1944)
Getting Gertie's Garter (1945) also screenwriter
Brewster's Millions (1945)
Rendezvous with Annie (1946)
Driftwood (1947)
Calendar Girl (1947)
Northwest Outpost (1947) also associate producer
The Inside Story (1948)
Sands of Iwo Jima (1949)
Surrender (1950)
Belle Le Grand (1951)
Wild Blue Yonder (1951)
I Dream of Jeanie (1952)
Montana Belle (1952)
Woman They Almost Lynched (1953)
Sweethearts on Parade (1953)
Silver Lode (1954)
Passion (1954)
Cattle Queen of Montana (1954)
Tennessee's Partner (1955)
Pearl of the South Pacific (1955)
Escape to Burma (1955)
Slightly Scarlet (1956)
Hold Back the Night (1956)
The Restless Breed (1957)
The River's Edge (1957)
Enchanted Island (1958)
Most Dangerous Man Alive (1961)
Canadian pioneers in early Hollywood

Brownlow, Kevin, The Parade's Gone By... (1968) ISBN 0520030680 ISBN 978-0520030688
Bogdanovich, Peter, Allan Dwan: The Last Pioneer (1971) ISBN 0289701228 ISBN 978-0289701225 
Foster, Charles, Stardust and Shadows: Canadians in Early Hollywood (2000) ISBN 1-55002-348-9

Lombardi, Frederic,  Allan Dwan and the Rise and Decline of the Hollywood Studios (2013)
Print ISBN 978-0-7864-3485-5 Ebook ISBN 978-0-7864-9040-0

,  virtual-history.com; accessed 16 June 2014.


Algeria ( or ; Literary Arabic:  ; Algerian Arabic, Tamazight: Dzayer, ; ), officially People's Democratic Republic of Algeria, is a country in North Africa on the Mediterranean coast. Its capital and most populous city is Algiers. With a total area of , 90% of which is desert, Algeria is the tenth-largest country in the world and the largest in the Arab world, Africa (following the partition of Sudan) and on the Mediterranean. The country is bordered in the northeast by Tunisia, in the east by Libya, in the west by Morocco, in the southwest by Western Sahara, Mauritania, and Mali, in the southeast by Niger, and in the north by the Mediterranean Sea.

The territory of today's Algeria was the home of many prehistoric cultures, including Aterian and Capsian and the Proto-Berber cultures. Its area has known many empires and dynasties, including ancient Numidians, Phoenicians, Carthaginians, Romans, Vandals, Byzantines, Umayyads, Abbasids, Fatimids, Hammadids, Almoravids, Almohads, Ottomans and the French colonial empire. In recent decades, Algeria has experienced increased identity recognition demands, in response to which, Tamazight, the language of their 13,000 year old people, has been constitutionalized as a national language. Algeria is a semi-presidential republic consisting of 48 provinces and 1541 communes. With a population of 39.5 million, it is the 33rd most populated country on Earth. Abdelaziz Bouteflika has been the President of Algeria since 1999 and has won four consecutive elections.

Algeria's economy is largely based on hydrocarbons, due to which manufacturing has suffered from Dutch disease. The country supplies large amounts of natural gas to Europe, and energy exports are the backbone of the economy. According to OPEC Algeria has the 17th largest reserves of oil in the world, and the second largest in Africa, while it has the 9th largest reserves of natural gas. Sonatrach, the national oil company, is the largest company in Africa.

Algeria has the second largest military in North Africa with the largest defence budget in Africa. Algeria has had a peaceful nuclear program since the 1990s. Algeria is a member of the African Union, the Arab League, OPEC, and the United Nations, and is a founding member of the Arab Maghreb Union.

The country's name derives from the city of Algiers. The city's name in turn derives from the Arabic al-JazÄ'ir (Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±, "The Islands"), a truncated form of the older JazÄ'ir BanÄ« Mazghanna (Ø¬Ø²Ø§Ø¦Ø± Ø¨Ù†ÙŠ Ù…Ø²ØºÙ†Ø©, "Islands of the Mazghanna Tribe"), employed by medieval geographers such as al-Idrisi.


In the region of Ain Hanech (SaÃ¯da Province), early remnants (200,000 BC) of hominid occupation in North Africa were found. Neanderthal tool makers produced hand axes in the Levalloisian and Mousterian styles (43,000 BC) similar to those in the Levant.

Algeria was the site of the highest state of development of Middle Paleolithic Flake tool techniques. Tools of this era, starting about 30,000 BC, are called Aterian (after the archeological site of Bir el Ater, south of Tebessa).

The earliest blade industries in North Africa are called Iberomaurusian (located mainly in Oran region). This industry appears to have spread throughout the coastal regions of the Maghreb between 15,000 and 10,000 BC. Neolithic civilization (animal domestication and agriculture) developed in the Saharan and Mediterranean Maghreb perhaps as early as 11,000 BC or as late as between 6000 and 2000 BC. This life, richly depicted in the Tassili n'Ajjer paintings, predominated in Algeria until the classical period.

The amalgam of peoples of North Africa coalesced eventually into a distinct native population that came to be called Berbers, who are the indigenous peoples of northern Africa.
From their principal center of power at Carthage, the Carthaginians expanded and established small settlements along the North African coast; by 600 BC, a Phoenician presence existed at Tipasa, east of Cherchell, Hippo Regius (modern Annaba) and Rusicade (modern Skikda). These settlements served as market towns as well as anchorages.

As Carthaginian power grew, its impact on the indigenous population increased dramatically. Berber civilization was already at a stage in which agriculture, manufacturing, trade, and political organization supported several states. Trade links between Carthage and the Berbers in the interior grew, but territorial expansion also resulted in the enslavement or military recruitment of some Berbers and in the extraction of tribute from others.
By the early 4th century BC, Berbers formed the single largest element of the Carthaginian army. In the Revolt of the Mercenaries, Berber soldiers rebelled from 241 to 238 BC after being unpaid following the defeat of Carthage in the First Punic War. They succeeded in obtaining control of much of Carthage's North African territory, and they minted coins bearing the name Libyan, used in Greek to describe natives of North Africa. The Carthaginian state declined because of successive defeats by the Romans in the Punic Wars.

In 146 BC the city of Carthage was destroyed. As Carthaginian power waned, the influence of Berber leaders in the hinterland grew. By the 2nd century BC, several large but loosely administered Berber kingdoms had emerged. Two of them were established in Numidia, behind the coastal areas controlled by Carthage. West of Numidia lay Mauretania, which extended across the Moulouya River in modern day Morocco to the Atlantic Ocean. The high point of Berber civilization, unequaled until the coming of the Almohads and Almoravids more than a millennium later, was reached during the reign of Massinissa in the 2nd century BC.

After Masinissa's death in 148 BC, the Berber kingdoms were divided and reunited several times. Massinissa's line survived until 24 AD, when the remaining Berber territory was annexed to the Roman Empire.
For several centuries Algeria was ruled by the Romans, who founded many colonies in the region. Like the rest of North Africa, Algeria was one of the breadbaskets of the empire, exporting cereals and other agricultural products. The Vandals of Geiseric moved into North Africa in 429, and by 435 controlled coastal Numidia. They did not make any significant settlement on the land, as they were harassed by local tribes, in fact by the time the Byzantines arrived Lepcis Magna was abandoned and the Msellata region was occupied by the indigenous Laguatan who had been busy facilitating an Amazigh political, military and cultural revival.
After negligible resistance from the locals, the Arabs conquered Algeria in the mid-7th century and a large number of the indigenous people converted to the new faith. After the fall of the Umayyad Caliphate, numerous local dynasties emerged, including the Aghlabids, Almohads, Abdalwadid, Zirids, Rustamids, Hammadids, Almoravids and the Fatimids.

During the Middle Ages, North Africa was home to many great Scholars, Saints and Sovereigns including Judah Ibn Quryash the first grammarian to suggest the Afro-Asiatic Language Family, the great Sufi masters Sidi Boumediene (Abu Madyan) and Sidi El Houari, as well as the Emirs Abd Al Mu'min and YÄghmÅ«rasen. It was during this time period that the Fatimids or children of Fatima, daughter of Muhammad, came to the Maghreb. These "Fatimids" went on to found a long lasting dynasty stretching across the Maghreb, Hejaz, and the Levant, boasting a secular inner government, as well as a powerful army and navy, primarily made of Arabs and levantians extending from Algeria to their capital state of Cairo. The Fatimid caliphate began to collapse when its governors the Zirids seceded. In order to punish them the Fatimids sent the Arab Banu Hilal and Banu Sulaym against them. The resultant war is recounted in the epic TÄghribÄt. In Al-TÄghrÄ«bÄt the Amazigh Zirid Hero KhÄlÄ«fÄ Al-ZÄnatÄ« asks daily, for duels, to defeat the Hilalan hero Ä€bu Zayd al-HilalÄ« and many other Arab knights in a string of victories. The Zirids however were ultimately defeated ushering in an adoption of Arab customs and culture. The indigenous Amazigh tribes however remained largely independent, and depending on tribe, location, and time controlled varying parts of the Maghreb, at times unifying it (as under the Fatimids). The Fatimid Islamic state, also known as Fatimid Caliphate made an Islamic empire that included North Africa, Sicily, Palestine, Jordan, Lebanon, Syria, the Red Sea coast of Africa, Tihamah, Hejaz, and Yemen. Caliphates from Northern Africa traded with the other empires of their time, as well as forming part of a confederated support and trade network with other Islamic states during the Islamic Era.

The Amazighs historically consisted of several tribes. The two main branches were the Botr and BarnÃ¨s tribes, who were divided into tribes, and again into sub-tribes. Each region of the Maghreb contained several tribes (for example, Sanhadja, Houaras, Zenata, Masmouda, Kutama, Awarba, and Berghwata). All these tribes made independent territorial decisions.

Several Amazigh dynasties emerged during the Middle Ages in the Maghreb and other nearby lands. Ibn Khaldun provides a table summarizing the Amazigh dynasties of the Maghreb region, the Zirid, Banu Ifran, Maghrawa, Almoravid, Hammadid, Almohad, Merinid, Abdalwadid, Wattasid, Meknassa and Hafsid dynasties.

In the early 16th century, Spain constructed fortified outposts (presidios) on or near the Algerian coast. Spain took control of few coastal towns like Mers el Kebir in 1505; Oran in 1509; and Tlemcen, Mostaganem, and TÃ©nÃ¨s, in 1510. In the same year, few merchants of Algiers ceded one of the rocky islets in their harbor to Spain, which built a fort on it. The presidios in North Africa turned out to be a costly and largely ineffective military endeavor that did not guarantee access for Spain's merchant fleet.

The region of Algeria was ruled by Ottomans for five centuries from 1516 to the 20th century. In 1516 the Turkish privateer brothers Aruj and Hayreddin Barbarossa, who operated successfully under the Hafsids, moved their base of operations to Algiers. They succeeded in conquering Jijel and Algiers from the Spaniards but eventually assumed control over the city and the surrounding region, forcing the previous ruler, Abu Hamo Musa III of the Bani Ziyad dynasty, to flee. When Aruj was killed in 1518 during his invasion of Tlemcen, Hayreddin succeeded him as military commander of Algiers. The Ottoman sultan gave him the title of beylerbey and a contingent of some 2,000 janissaries. With the aid of this force, Hayreddin conquered the whole area between Constantine and Oran (although the city of Oran remained in Spanish hands until 1791).

The next beylerbey was Hayreddin's son Hasan, who assumed the position in 1544. Until 1587 the area was governed by officers who served terms with no fixed limits. Subsequently, with the institution of a regular Ottoman administration, governors with the title of pasha ruled for three-year terms. The pasha was assisted by janissaries, known in Algeria as the ojaq and led by an agha. Discontent among the ojaq rose in the mid-1600s because they were not paid regularly, and they repeatedly revolted against the pasha. As a result, the agha charged the pasha with corruption and incompetence and seized power in 1659.

Plague had repeatedly struck the cities of North Africa. Algiers lost from 30,000 to 50,000 inhabitants to the plague in 1620â€“21, and suffered high fatalities in 1654â€“57, 1665, 1691, and 1740â€“42.

In 1671, the taifa rebelled, killed the agha, and placed one of its own in power. The new leader received the title of dey. After 1689, the right to select the dey passed to the divan, a council of some sixty nobles. It was at first dominated by the ojaq; but by the 18th century, it had become the dey's instrument. In 1710, the dey persuaded the sultan to recognize him and his successors as regent, replacing the pasha in that role. Although Algiers remained a part of the Ottoman Empire.

The dey was in effect a constitutional autocrat. The dey was elected for a life term, but in the 159 years (1671â€“1830) that the system survived, fourteen of the twenty-nine deys were assassinated. Despite usurpation, military coups, and occasional mob rule, the day-to-day operation of Ottomon government was remarkably orderly. Although the regency patronized the tribal chieftains, it never had the unanimous allegiance of the countryside, where heavy taxation frequently provoked unrest. Autonomous tribal states were tolerated, and the regency's authority was seldom applied in the Kabylie.


The Barbary pirates preyed on Christian and other non-Islamic shipping in the western Mediterranean Sea. The pirates often took the passengers and crew on the ships and sold them or used them as slaves. They also did a brisk business in ransoming some of the captives. According to Robert Davis, from the 16th to 19th century, pirates captured 1 million to 1.25 million Europeans as slaves. They often made raids, called Razzias, on European coastal towns to capture Christian slaves to sell at slave markets in North Africa and the Ottoman Empire.

In 1544, Hayreddin captured the island of Ischia, taking 4,000 prisoners, and enslaved some 9,000 inhabitants of Lipari, almost the entire population. In 1551, Turgut Reis enslaved the entire population of the Maltese island of Gozo, between 5,000 and 6,000, sending the captives to Libya. In 1554, pirates sacked Vieste in southern Italy and took an estimated 7,000 captives as slaves.

In 1558, Barbary corsairs captured the town of Ciutadella (Minorca), destroyed it, slaughtered the inhabitants and took 3,000 survivors as slaves to Istanbul. Barbary pirates often attacked the Balearic Islands, and in response, the residents built many coastal watchtowers and fortified churches. The threat was so severe that residents abandoned the island of Formentera.

Between 1609 to 1616, England lost 466 merchant ships to Barbary pirates.

In July 1627 two pirate ships from Algiers sailed as far as Iceland, raiding and capturing slaves. Two weeks earlier another pirate ship from SalÃ© in Morocco had also raided in Iceland. Some of the slaves brought to Algiers were later ransomed back to Iceland, but some chose to stay in Algeria. In 1629 pirate ships from Algeria raided the Faroe Islands.

In the 19th century, the pirates forged affiliations with Caribbean powers, paying a "license tax" in exchange for safe harbor of their vessels. One American slave reported that the Algerians had enslaved 130 American seamen in the Mediterranean and Atlantic from 1785 to 1793.

Piracy on American vessels in the Mediterranean resulted in the United States initiating the First (1801â€“1805) and Second Barbary Wars (1815). Following those wars, Algeria was weaker, and Europeans, with an Anglo-Dutch fleet commanded by the British Lord Exmouth, attacked Algiers. After a nine-hour bombardment, they obtained a treaty from the Dey that reaffirmed the conditions imposed by Decatur (US navy) concerning the demands of tributes. In addition, the Dey agreed to end the practice of enslaving Christians.

On the pretext of a slight to their consul, the French invaded and captured Algiers in 1830. The conquest of Algeria by the French took some time and resulted in considerable bloodshed. A combination of violence and disease epidemics caused the indigenous Algerian population to decline by nearly one-third from 1830 to 1872. The population of Algeria, which stood at about 1.5 million in 1830, reached nearly 11 million in 1960. French policy was predicated on "civilizing" the country. Algeria's social fabric suffered during the occupation: literacy plummeted. During this period, a small but influential French-speaking indigenous elite was formed, made up of Berbers mostly from Kabyles.
As a consequence, French government favored the Kabyles. About 80% of Indigenous Schools were constructed for Kabyles.
From 1848 until independence, France administered the whole Mediterranean region of Algeria as an integral part and dÃ©partement of the nation. One of France's longest-held overseas territories, Algeria became a destination for hundreds of thousands of European immigrants, who became known as colons and later, as Pied-Noirs. Between 1825 and 1847, 50,000 French people emigrated to Algeria. These settlers benefited from the French government's confiscation of communal land from tribal peoples, and the application of modern agricultural techniques that increased the amount of arable land.

Gradually, dissatisfaction among the Muslim population, which lacked political and economic status in the colonial system, gave rise to demands for greater political autonomy, and eventually independence, from France. Tensions between the two population groups came to a head in 1954, when the first violent events of what was later called the Algerian War began. Historians have estimated that between 30,000 and 150,000 Harkis and their dependents were killed by the Front de LibÃ©ration Nationale (FLN) or by lynch mobs in Algeria. The FLN used hit and run attacks in Algeria and France as part of its war, and the French conducted severe reprisals. The war led to the death of hundreds of thousands of Algerians and hundreds of thousands of injuries. The war concluded in 1962, when Algeria gained complete independence following the March 1962 Evian agreements and the July 1962 self-determination referendum.

Algeria's first president was the FLN leader Ahmed Ben Bella. Morocco's claim to portions of western Algeria led to the Sand War in 1963. Ben Bella was overthrown in 1965 by Houari Boumediene, his former ally and defense minister. Under Ben Bella, the government had become increasingly socialist and authoritarian; BoumÃ©dienne continued this trend. But, he relied much more on the army for his support, and reduced the sole legal party to a symbolic role. He collectivised agriculture and launched a massive industrialization drive. Oil extraction facilities were nationalized. This was especially beneficial to the leadership after the international 1973 oil crisis.

In the 1960s and 1970s under President Houari Boumediene, Algeria pursued a programme of industrialisation within a state-controlled socialist economy. Boumediene's successor, Chadli Bendjedid, introduced some liberal economic reforms. He promoted a policy of Arabisation in Algerian society and public life. Teachers of Arabic, brought in from other Muslim countries, spread radical Islamic thought in schools and sowed the seeds of political Islamism.

The Algerian economy became increasingly dependent on oil, leading to hardship when the price collapsed during the 1980s oil glut. Economic recession caused by the crash in world oil prices resulted in Algerian social unrest during the 1980s; by the end of the decade, Bendjedid introduced a multi-party system. Political parties developed, such as the Islamic Salvation Front (FIS), a broad coalition of Islamist groups.

In December 1991 the Islamic Salvation Front dominated the first of two rounds of legislative elections. Fearing the election of an Islamist government, the authorities intervened on 11 January 1992, cancelling the elections. Bendjedid resigned and a High Council of State was installed to act as Presidency. It banned the FIS, triggering a civil insurgency between the Front's armed wing, the Armed Islamic Group, and the national armed forces, in which more than 100,000 people are thought to have died. The Islamist militants conducted a violent campaign of civilian massacres. At several points in the conflict, the situation in Algeria became a point of international concern, most notably during the crisis surrounding Air France Flight 8969, a hijacking perpetrated by the Armed Islamic Group. The Armed Islamic Group declared a ceasefire in October 1997.

Algeria held elections in 1999, considered biased by international observers and most opposition groups which were won by President Abdelaziz Bouteflika. He worked to restore political stability to the country and announced a 'Civil Concord' initiative, approved in a referendum, under which many political prisoners were pardoned, and several thousand members of armed groups were granted exemption from prosecution under a limited amnesty, in force until 13 January 2000. The AIS disbanded and levels of insurgent violence fell rapidly. The Groupe Salafiste pour la PrÃ©dication et le Combat (GSPC), a splinter group of the Group Islamic ArmÃ©e, continued a terrorist campaign against the Government.
Bouteflika was re-elected in the April 2004 presidential election after campaigning on a programme of national reconciliation. The programme comprised economic, institutional, political and social reform to modernise the country, raise living standards, and tackle the causes of alienation. It also included a second amnesty initiative, the Charter for Peace and National Reconciliation, which was approved in a referendum in September 2005. It offered amnesty to most guerrillas and Government security forces.

In November 2008, the Algerian Constitution was amended following a vote in Parliament, removing the two-term limit on Presidential incumbents. This change enabled Bouteflika to stand for re-election in the 2009 presidential elections, and he was re-elected in April 2009. During his election campaign and following his re-election, Bouteflika promised to extend the programme of national reconciliation and a $150-billion spending programme to create three million new jobs, the construction of one million new housing units, and to continue public sector and infrastructure modernisation programmes.

A continuing series of protests throughout the country started on 28 December 2010, inspired by similar protests across the Middle East and North Africa. On 24 February 2011, the government lifted Algeria's 19-year-old state of emergency. The government enacted legislation dealing with political parties, the electoral code, and the representation of women in elected bodies. In April 2011, Bouteflika promised further constitutional and political reform. However, elections are routinely criticized by opposition groups as unfair and international human rights groups say that media censorship and harassment of political opponents continue.

Algeria is the largest country in Africa, the Arab world, and the Mediterranean Basin. Its southern part includes a significant portion of the Sahara. To the north, the Tell Atlas form with the Saharan Atlas, further south, two parallel sets of reliefs in approaching eastbound, and between which are inserted vast plains and highlands. Both Atlas tend to merge in eastern Algeria. The vast mountain ranges of Aures and Nememcha occupy the entire northeastern Algeria and are delineated by the Tunisian border. The highest point is Mount Tahat ( m).

Algeria lies mostly between latitudes 19Â° and 37Â°N (a small area is north of 37Â°), and longitudes 9Â°W and 12Â°E. Most of the coastal area is hilly, sometimes even mountainous, and there are a few natural harbours. The area from the coast to the Tell Atlas is fertile. South of the Tell Atlas is a steppe landscape ending with the Saharan Atlas; farther south, there is the Sahara desert.

The Ahaggar Mountains (), also known as the Hoggar, are a highland region in central Sahara, southern Algeria. They are located about  south of the capital, Algiers, and just west of Tamanghasset. Algiers, Oran, Constantine, and Annaba are Algeria's main cities.

In this region, midday desert temperatures can be hot year round. After sunset, however, the clear, dry air permits rapid loss of heat, and the nights are cool to chilly. Enormous daily ranges in temperature are recorded.

The highest official temperature was  at In Salah.

Rainfall is fairly plentiful along the coastal part of the Tell Atlas, ranging from  annually, the amount of precipitation increasing from west to east. Precipitation is heaviest in the northern part of eastern Algeria, where it reaches as much as  in some years.

Farther inland, the rainfall is less plentiful. Algeria also has ergs, or sand dunes, between mountains. Among these, in the summer time when winds are heavy and gusty, temperatures can get up to .

The varied vegetation of Algeria includes coastal, mountainous and grassy desert-like regions which all support a wide range of wildlife. Many of the creatures comprising the Algerian wildlife live in close proximity to civilization. The most commonly seen animals include the wild boars, jackals, and gazelles, although it is not uncommon to spot fennecs (foxes), and jerboas. Algeria also has a small leopard and cheetah population, but these are seldom seen.

A variety of bird species makes the country an attraction for bird watchers. The forests are inhabited by boars and jackals. Barbary macaques are the sole native monkey. Snakes, monitor lizards, and numerous other reptiles can be found living among an array of rodents throughout the semi arid regions of Algeria. Many animals are now extinct, including the Barbary lions and bears.

In the north, some of the native flora includes Macchia scrub, olive trees, oaks, cedars and other conifers. The mountain regions contain large forests of evergreens (Aleppo pine, juniper, and evergreen oak) and some deciduous trees. Fig, eucalyptus, agave, and various palm trees grow in the warmer areas. The grape vine is indigenous to the coast. In the Sahara region, some oases have palm trees. Acacias with wild olives are the predominant flora in the remainder of the Sahara.

Camels are used extensively; the desert also abounds with poisonous and nonpoisonous snakes, scorpions, and numerous insects.

Algeria is an authoritarian regime, according to the Democracy Index 2014. The Freedom of the Press 2015 report gives it a rating of "Not Free".

Elected politicians are considered to have relatively little sway over Algeria. Instead, a group of unelected civilian and military "dÃ©cideurs", known as "le pouvoir" ("the power"), actually rule the country, even deciding who should be president. The most powerful man may be Mohamed MediÃ¨ne, head of the military intelligence. In recent years, many of these generals have died or retired. After the death of General Larbi Belkheir, Bouteflika put loyalists in key posts, notably at Sonatrach, and secured constitutional amendments that make him re-electable indefinitely.

The head of state is the president of Algeria, who is elected for a five-year term. The president was formerly limited to two five-year terms, but a constitutional amendment passed by the Parliament on 11 November 2008 removed this limitation. Algeria has universal suffrage at 18 years of age. The President is the head of the army, the Council of Ministers and the High Security Council. He appoints the Prime Minister who is also the head of government.

The Algerian parliament is bicameral; the lower house, the People's National Assembly, has 462 members who are directly elected for five-year terms, while the upper house, the Council of the Nation, has 144 members serving six-year terms, of which 96 members are chosen by local assemblies and 48 are appointed by the president. According to the constitution, no political association may be formed if it is "based on differences in religion, language, race, gender, profession or region". In addition, political campaigns must be exempt from the aforementioned subjects.

Parliamentary elections were last held in May 2012, and were judged to be largely free by international monitors, though local groups alleged fraud and irregularities. In the elections, the FLN won 221 seats, the military-backed National Rally for Democracy won 70, and the Islamist Green Algeria Alliance won 47.

In October 2009, Algeria cancelled a weapons deal with France over the possibility of inclusion of Israeli parts in them.

Tensions between Algeria and Morocco in relation to the Western Sahara have been an obstacle to tightening the Arab Maghreb Union, nominally established in 1989, but which has carried little practical weight.

Algeria is included in the European Union's European Neighbourhood Policy (ENP) which aims at bringing the EU and its neighbours closer.
Giving incentives and rewarding best performers, as well as offering funds in a faster and more flexible manner, are the two main principles underlying the European Neighbourhood Instrument (ENI) that came into force in 2014. It has a budget of â‚¬15.4 billion and provides the bulk of funding through a number of programmes.

The military of Algeria consists of the People's National Army (ANP), the Algerian National Navy (MRA), and the Algerian Air Force (QJJ), plus the Territorial Air Defense Force. It is the direct successor of the National Liberation Army (ArmÃ©e de LibÃ©ration Nationale or ALN), the armed wing of the nationalist National Liberation Front which fought French colonial occupation during the Algerian War of Independence (1954â€“62).

Total military personnel include 147,000 active, 150,000 reserve, and 187,000 paramilitary staff (2008 estimate). Service in the military is compulsory for men aged 19â€“30, for a total of 12 months. The military expenditure was 4.3% of the gross domestic product (GDP) in 2012. Algeria has the second largest military in North Africa with the largest defense budget in Africa ($10 billion).

In 2007, the Algerian Air Force signed a deal with Russia to purchase 49 MiG-29SMT and 6 MiG-29UBT at an estimated cost of $1.9billion. It also agreed to return old aircraft purchased from the former USSR. Russia is also building two 636-type diesel submarines for Algeria.


Algeria is divided into 48 provinces (wilayas), 553 districts (daÃ¯ras) and 1,541 municipalities (baladiyahs). Each province, district, and municipality is named after its seat, which is usually the largest city.

The administrative divisions have changed several times since independence. When introducing new provinces, the numbers of old provinces are kept, hence the non-alphabetical order. With their official numbers, currently (since 1983) they are
Algeria is classified as an upper middle income country by the World Bank. Algeriaâ€™s currency is the dinar (DZD). The economy remains dominated by the state, a legacy of the country's socialist post-independence development model. In recent years, the Algerian government has halted the privatization of state-owned industries and imposed restrictions on imports and foreign involvement in its economy.

Algeria has struggled to develop industries outside hydrocarbons in part because of high costs and an inert state bureaucracy. The government's efforts to diversify the economy by attracting foreign and domestic investment outside the energy sector have done little to reduce high youth unemployment rates or to address housing shortages. The country is facing a number of short-term and medium-term problems, including the need to diversify the economy, strengthen political, economic and financial reforms, improve the business climate and reduce inequalities amongst regions.

A wave of economic protests in February and March 2011 prompted the Algerian government to offer more than $23 billion in public grants and retroactive salary and benefit increases. Public spending has increased by 27% annually during the past 5 years. The 2010â€“14 public-investment programme will cost US$286 billion, 40% of which will go to human development.

The Algerian economy grew by 2.6% in 2011, driven by public spending, in particular in the construction and public-works sector, and by growing internal demand. If hydrocarbons are excluded, growth has been estimated at 4.8%. Growth of 3% is expected in 2012, rising to 4.2% in 2013. The rate of inflation was 4% and the budget deficit 3% of GDP. The current-account surplus is estimated at 9.3% of GDP and at the end of December 2011, official reserves were put at US$182 billion. Inflation, the lowest in the region, has remained stable at 4% on average between 2003 and 2007.
In 2011 Algeria announced a budgetary surplus of $26.9 billion, 62% increase in comparison to 2010 surplus. In general, the country exported $73 billion worth of commodities while it imported $46 billion.

Thanks to strong hydrocarbon revenues, Algeria has a cushion of $173 billion in foreign currency reserves and a large hydrocarbon stabilization fund. In addition, Algeria's external debt is extremely low at about 2% of GDP. The economy remains very dependent on hydrocarbon wealth, and, despite high foreign exchange reserves (US$178 billion, equivalent to three years of imports), current expenditure growth makes Algeria's budget more vulnerable to the risk of prolonged lower hydrocarbon revenues.

In 2011, the agricultural sector and services recorded growth of 10% and 5.3%, respectively. About 14% of the labor force are employed in the agricultural sector. Fiscal policy in 2011 remained expansionist and made it possible to maintain the pace of public investment and to contain the strong demand for jobs and housing.

Algeria has not joined the WTO, despite several years of negotiations.

In March 2006, Russia agreed to erase $4.74billion of Algeria's Soviet-era debt during a visit by Russian President Vladimir Putin to the country, the first by a Russian leader in half a century. In return, Algerian President Abdelaziz Bouteflika agreed to buy $7.5billion worth of combat planes, air-defense systems and other arms from Russia, according to the head of Russia's state arms exporter Rosoboronexport.


Algeria, whose economy is reliant on petroleum, has been an OPEC member since 1969. Its crude oil production stands at around 1.1 million barrels/day, but it is also a major gas producer and exporter, with important links to Europe. Hydrocarbons have long been the backbone of the economy, accounting for roughly 60% of budget revenues, 30% of GDP, and over 95% of export earnings. Algeria has the 10th-largest reserves of natural gas in the world and is the sixth-largest gas exporter. The U.S. Energy Information Administration reported that in 2005, Algeria had  of proven natural-gas reserves. It also ranks 16th in oil reserves.

Non-hydrocarbon growth for 2011 was projected at 5%. To cope with social demands, the authorities raised expenditure, especially on basic food support, employment creation, support for SMEs, and higher salaries. High hydrocarbon prices have improved the current account and the already large international reserves position.

Income from oil and gas rose in 2011 as a result of continuing high oil prices, though the trend in production volume is downwards. Production from the oil and gas sector in terms of volume, continues to decline, dropping from 43.2 million tonnes to 32 million tonnes between 2007 and 2011. Nevertheless, the sector accounted for 98% of the total volume of exports in 2011, against 48% in 1962, and 70% of budgetary receipts, or USD 71.4 billion.

The Algerian national oil company is Sonatrach, which plays a key role in all aspects of the oil and natural gas sectors in Algeria. All foreign operators must work in partnership with Sonatrach, which usually has majority ownership in production-sharing agreements.

Despite a decline in total unemployment, youth and women unemployment is high. Unemployment particularly affects the young, with a jobless rate of 21.5% among the 15â€“24 age group.

The overall rate of unemployment was 10% in 2011, but remained higher among young people, with a rate of 21.5% for those aged between 15 and 24. The government strengthened in 2011 the job programmes introduced in 1988, in particular in the framework of the programme to aid those seeking work (Dispositif d'Aide Ã  l'Insertion Professionnelle).

The development of the tourism sector in Algeria had previously been hampered by a lack of facilities, but since 2004 a broad tourism development strategy has been implemented resulting in many hotels of a high modern standard being built.

There are several UNESCO World Heritage Sites in Algeria including Al Qal'a of Beni Hammad, the first capital of the Hammadid empire; Tipasa, a Phoenician and later Roman town; and DjÃ©mila and Timgad, both Roman ruins; M'Zab Valley, a limestone valley containing a large urbanized oasis; also the Casbah of Algiers is an important citadel. The only natural World Heritage Sites is the Tassili n'Ajjer, a mountain range.

The Algerian road network is the densest in Africa; its length is estimated at 180,000km of highways, with more than 3,756 structures and a paving rate of 85%. This network will be complemented by the East-West Highway, a major infrastructure project currently under construction. It is a 3-way, 1,216km long highway, linking Annaba in the extreme east to the Tlemcen in the far west. Algeria is also crossed by the Trans-Sahara Highway, which is now completely paved. This road is supported by the Algerian government to increase trade between the six countries crossed: Algeria, Mali, Niger, Nigeria, Chad and Tunisia.

In January 2013 Algeria's population was an estimated 37.9million, who are mainly Arab-Berber ethnically. At the outset of the 20th century, its population was approximately four million. About 90% of Algerians live in the northern, coastal area; the inhabitants of the Sahara desert are mainly concentrated in oases, although some 1.5million remain nomadic or partly nomadic. 28.1% of Algerians are under the age of 15.

Women make up 70% of the country's lawyers and 60% of its judges and also dominate the field of medicine. Increasingly, women are contributing more to household income than men. 60% of university students are women, according to university researchers.

Between 90,000 and 165,000 Sahrawis from Western Sahara live in the Sahrawi refugee camps, in the western Algerian Sahara desert. There are also more than 4,000 Palestinian refugees, who are well integrated and have not asked for assistance from the United Nations High Commissioner for Refugees (UNHCR). In 2009, 35,000 Chinese migrant workers lived in Algeria.

The largest concentration of Algerian migrants outside Algeria is in France, which has reportedly over 1.7 million Algerians of up to the second generation.


The Berbers are the indigenous ethnic group of Algeria and are believed to be the ancestral stock on which elements from the Phoenicians, Romans, Byzantines, Arabs, Turks as well as other ethnic groups have contributed to the ethnic makeup of Algeria. Descendants of Andalusian refugees are also present in the population of Algiers and other cities. Moreover, Spanish was spoken by these Aragonese and Castillian Morisco descendants deep into the 18th century, and even Catalan was spoken at the same time by Catalan Morisco descendants in the small town of Grish El-Oued.
There are 600,000 to 2 million erstwhile Algerian Turks, descendants of Turk rulers, soldiers, doctors and others who ruled the region during the Ottoman rule in North Africa. Today's Turkish descendants are often called Kouloughlis, meaning descendants of Turkish men and native Algerian women.

The majority of Algerians identifies with an Arabic-based culture. Berbers are divided into many groups with varying languages. The largest of these are the Kabyles, who live in the Kabylie region east of Algiers, the Chaoui of North-East Algeria, the Tuaregs in the southern desert and the Shenwa people of North Algeria.

During the colonial period, there was a large (10% in 1960) European population who became known as Pied-Noirs. They were primarily of French, Spanish and Italian origin. Almost all of this population left during the war of independence or immediately after its end.

Modern Standard Arabic is the official language. Algerian Arabic (Darja) is the language used by the majority of the population. Colloquial Algerian Arabic is heavily infused with borrowings from French and Berber.

Berber is spoken by one quarter of the population and has been recognized as a "national language" by the constitutional amendment since 8 May 2002. Kabyle, the predominant Berber language, is taught and is partially co-official (with a few restrictions) in parts of Kabylie.

Although French has no official status, Algeria is the second-largest Francophone country in the world in terms of speakers, and French is widely used in government, media (newspapers, radio, local television), and both the education system (from primary school onwards) and academia due to Algeria's colonial history. It can be regarded as the de facto co-official language of Algeria. In 2008, 11.2 million Algerians could read and write in French.)," and "1. Nombre de personnes Ã¢gÃ©es de cinq ans et plus dÃ©clarant savoir lire et Ã©crire le franÃ§ais, dâ€™aprÃ¨s les donnÃ©es du recensement de 2008 communiquÃ©es par lâ€™Office national des statistiques dâ€™AlgÃ©rie." An Abassa Institute study in April 2000 found that 60% of households could speak and understand French. In recent decades the government has reinforced the study of French and TV programs have reinforced use of the language.

Algeria emerged as a bilingual state after 1962. Colloquial Algerian Arabic is spoken by about 72% of the population and Berber by 27â€“30%.

Islam is the predominant religion with 99% of the population. There are about 150,000 Ibadis in the M'zab Valley in the region of Ghardaia.

There were an estimated 10,000 Christians in Algeria in 2008. In a 2009 study the UNO estimated there were 45,000 Catholics and 50,000â€“100,000 Protestants in Algeria.

Following the Revolution and Algerian independence, all but 6,500 of the country's 140,000 Jews left the country, of whom about 90% moved to France with the Pied-Noirs and 10% moved to Israel.


Below is a list of the most important Algerian cities:
Modern Algerian literature, split between Arabic, Tamazight and French, has been strongly influenced by the country's recent history. Famous novelists of the 20th century include Mohammed Dib, Albert Camus, Kateb Yacine and Ahlam Mosteghanemi while Assia Djebar is widely translated. Among the important novelists of the 1980s were Rachid Mimouni, later vice-president of Amnesty International, and Tahar Djaout, murdered by an Islamist group in 1993 for his secularist views.

Malek Bennabi and Frantz Fanon are noted for their thoughts on decolonization; Augustine of Hippo was born in Tagaste (modern-day Souk Ahras); and Ibn Khaldun, though born in Tunis, wrote the Muqaddima while staying in Algeria. The works of the Sanusi family in pre-colonial times, and of Emir Abdelkader and Sheikh Ben Badis in colonial times, are widely noted. The Latin author Apuleius was born in Madaurus (Mdaourouch), in what later became Algeria.

Contemporary Algerian cinema is various in terms of genre, exploring a wider range of themes and issues. There has been a transition from cinema which focused on the war of independence to films more concerned with the everyday lives of Algerians.

Algerian painters, like  or Baya, attempted to revive the prestigious Algerian past prior to French colonization, at the same time that they have contributed to the preservation of the authentic values of Algeria. In this line, Mohamed Temam, Abdelkhader Houamel have also returned through this art, scenes from the history of the country, the habits and customs of the past and the country life. Other new artistic currents including the one of M'hamed Issiakhem, Mohammed Khadda and Bachir Yelles, appeared on the scene of Algerian painting, abandoning figurative classical painting to find new pictorial ways, in order to adapt Algerian paintings to the new realities of the country through its struggle and its aspirations. Mohammed Khadda and M'hamed Issiakhem have been notable in recent years.

The historic roots of Algerian literature goes back to the Numidian era, when Apuleius wrote The Golden Ass, the only Latin novel to survive in its entirety. This period had also known Augustine of Hippo, Nonius Marcellus and Martianus Capella, among many others. The Middle Ages have known many Arabic writers who revolutionized the Arab world literature, with authors like Ahmad al-Buni, Ibn Manzur and Ibn Khaldoun, who wrote the Muqaddimah while staying in Algeria, and many others.

Albert Camus was an Algerian-born French Pied-Noir author. In 1957 he was awarded the Nobel Prize in literature.
Today Algeria contains, in its literary landscape, big names having not only marked the Algerian literature, but also the universal literary heritage in Arabic and French.

As a first step, Algerian literature was marked by works whose main concern was the assertion of the Algerian national entity, there is the publication of novels as the Algerian trilogy of Mohammed Dib, or even Nedjma of Kateb Yacine novel which is often regarded as a monumental and major work. Other known writers will contribute to the emergence of Algerian literature whom include Mouloud Feraoun, Malek Bennabi, Malek Haddad, Moufdi Zakaria, Abdelhamid Ben Badis, Mohamed LaÃ¯d Al-Khalifa, Mouloud Mammeri, Frantz Fanon, and Assia Djebar.

In the aftermath of the independence, several new authors emerged on the Algerian literary scene, they will attempt through their works to expose a number of social problems, among them there are Rachid Boudjedra, Rachid Mimouni, Leila Sebbar, Tahar Djaout and Tahir Wattar.

Currently, a part of Algerian writers tends to be defined in a literature of shocking expression, due to the terrorism that occurred during the 1990s, the other party is defined in a different style of literature who staged an individualistic conception of the human adventure. Among the most noted recent works, there is the writer, the swallows of Kabul and the attack of Yasmina Khadra, the oath of barbarians of Boualem Sansal, memory of the flesh of Ahlam Mosteghanemi and the last novel by Assia Djebar nowhere in my father's House.

ChaÃ¢bi music is a typically Algerian musical genre characterized by specific rhythms and of Qacidate (Popular poems) in Arabic dialect. The undisputed master of this music is El Hadj M'Hamed El Anka. The Constantinois Malouf style is saved by musician from whom Mohamed Tahar Fergani is one of the best performers.
Folk music styles include Bedouin music, characterized by the poetic songs based on long kacida (poems); Kabyle music, based on a rich repertoire that is poetry and old tales passed through generations; Shawiya music, a folklore from diverse areas of the AurÃ¨s Mountains. Rahaba music style is unique to the Aures. Souad Massi is a rising Algerian folk singer. Other Algerian singers of the diaspora include Manel Filali in Germany and Kenza Farah in France. Tergui music is sung in Tuareg languages generally, Tinariwen had a world wide success. Finally, the staÃ¯fi music is born in SÃ©tif and remains a unique style of its kind.

Modern music is available in several facets, RaÃ¯ music is a style typical of Western Algeria. Rap, relatively recent style in Algeria, is experiencing significant growth.

The Algerian stateâ€™s interest in film-industry activities can be seen in the annual budget of DZD 200 million (EUR 1.8) allocated to production, specific measures and an ambitious programme plan implemented by the Ministry of Culture in order to promote national production, renovate the cinema stock and remedy the weak links in distribution and exploitation.

The financial support provided by the state, through the Fund for the Development of the Arts, Techniques and the Film Industry (FDATIC) and the Algerian Agency for Cultural Influence (AARC), plays a key role in the promotion of national production. Between 2007 and 2013, FDATIC subsidised 98 films (feature films, documentaries and short films). In mid-2013, AARC had already supported a total of 78 films, including 42 feature films, 6 short films and 30 documentaries.

According to the European Audiovisual Observatoryâ€™s LUMIERE database, 41 Algerian films were distributed in Europe between 1996 and 2013. 21 films in this repertoire are Algerian-French co productions. IndigÃ¨nes (Days of Glory) (2006) and Hors la loi (Outside the Law) (2010) by the director Rachid Bouchareb are two films in this repertoire that recorded the highest number of admissions in the European Union, with 3,172,612 admissions for IndigÃ¨nes and 474,722 for Hors la loi.

The European Audiovisual Observatory and Euromed Audiovisual releases  in May 2014.

Various games have existed in Algeria since antiquity. In the Aures, people played several games such As El Kherdba or El khergueba (chess variant). Playing cards, checkers and chess games are part of Algerian culture. Racing (fantasia) and the rifle shooting are part of cultural recreation of the Algerians.

The first Algerian, Arab and African gold medalist is Boughera El Ouafi in 1928 Olympics of Amsterdam in the Marathon. The second Algerian Medalist was Alain Mimoun in 1956 Summer Olympics in Melbourne. Several men and women were champions in athletics in the 1990s including Noureddine Morceli, Hassiba Boulmerka, Nouria Merah-Benida, and Taoufik Makhloufi, all specialized in middle distance running.

Football is the most popular sport in Algeria. Several names are engraved in the history of the sport, including Lakhdar Belloumi, Rachid Mekhloufi, Hassen Lalmas, Rabah Madjer, Salah Assad and Djamel Zidane. The Algeria national football team qualified for the 1982 FIFA World Cup, 1986 FIFA World Cup, 2010 FIFA World Cup and 2014 FIFA World Cup. In addition, several football clubs have won continental and international trophies as the club ES SÃ©tif or JS Kabylia. The Algerian Football Federation is an association of Algeria football clubs organizing national competitions and international matches of the selection of Algeria national football team.

Algerian cuisine is rich and diverse. The country was considered as the "granary of Rome". It offers a component of dishes and varied dishes, depending on the region and according to the seasons. The cuisine uses cereals as the main products, since they are always produced with abundance in the country. There is not a dish where cereals are not present.

Algerian cuisine varies from one region to another, according to seasonal vegetables. It can be prepared using meat, fish and vegetables. Among the dishes known, couscous, the chorba, the Rechta, the Chakhchoukha, the Berkoukes, the Shakshouka, the Mthewem, the Chtitha, the Mderbel, the Dolma, the Brik or Bourek, the Garantita, Lham'hlou, etc. Merguez sausage is very used in Algeria, but it differs, depending on the region and on the added spices.

The cakes are marketed and can be found in cities either in Algeria or in Europe or North America. However, traditional cakes made at home have a vast directory of revenue, according to the habits and customs of each family. Among these cakes, there are Tamina, Chrik, Garn logzelles, Griouech, Kalb el-louz, Makroud, Mbardja, Mchewek, Samsa, Tcharak, Baghrir, Khfaf, Zlabia, Aarayech, Ghroubiya, Mghergchette. The Algerian pastry also contains Tunisian or French cakes and it is marketed. The bread may be cooked such as Kessra or Khmira or Harchaya, chopsticks and so-called washers Khoubz dar or Matloue.
Other tradionel meals (Chakhchokha-Hassoua-T'chicha-Mahjouba and Doubara) are famous in Biskra.


In 2002, Algeria had inadequate numbers of physicians (1.13 per 1,000 people), nurses (2.23 per 1,000 people), and dentists (0.31 per 1,000 people). Access to "improved water sources" was limited to 92% of the population in urban areas and 80% of the population in rural areas. Some 99% of Algerians living in urban areas, but only 82% of those living in rural areas, had access to "improved sanitation". According to the World Bank, Algeria is making progress toward its goal of "reducing by half the number of people without sustainable access to improved drinking water and basic sanitation by 2015". Given Algeria's young population, policy favors preventive health care and clinics over hospitals. In keeping with this policy, the government maintains an immunization program. However, poor sanitation and unclean water still cause tuberculosis, hepatitis, measles, typhoid fever, cholera and dysentery. The poor generally receive health care free of charge.

Health records have been maintained in Algeria since 1882 and began adding Muslims living in the South to their Vital record database in 1905 during French rule.

Since the 1970s, in a centralized system that was designed to significantly reduce the rate of illiteracy, the Algerian government introduced a decree by which the school became compulsory for all children aged between 6 and 15 years who have the ability to track their learning through the 20 facilities built since independence, now the literacy rate is around 78.7%.

Since 1972, Arabic is used as the language of instruction during the first nine years of schooling. From the third year, French is taught and it is also the language of instruction for science classes. The students can also learn English, Italian, Spanish and German. In 2008, new programs at the elementary appeared, therefore the compulsory schooling does not start at the age of six anymore, but at the age of five.
Apart from the 122 private, learning at school, the Universities of the State are free of charge. After nine years of primary school, students can go to the high school or to an educational institution. The school offers two programs: general or technical. At the end of the third year of secondary school, students pass the exam of the Bachelor's degree, which allows once it is successful to pursue graduate studies in universities and institutes.

Education is officially compulsory for children between the ages of six and 15. In 2008, the illiteracy rate for people over 10 was 22.3%, 15.6% for men and 29.0% for women. The province with the lowest rate of illiteracy was Algiers Province at 11.6%, while the province with the highest rate was Djelfa Province at 35.5%.

Algeria has 26 universities and 67 institutions of higher education, which must accommodate a million Algerians and 80,000 foreign students in 2008. The University of Algiers, founded in 1879, is the oldest, it offers education in various disciplines (law, medicine, science and letters). 25 of these universities and almost all of the institutions of higher education were founded after the independence of the country.

Even if some of them offer instruction in Arabic like areas of law and the economy, most of the other sectors as science and medicine continue to be provided in French and English. Among the most important universities, there are the University of Sciences and Technology Houari Boumediene, the University of Mentouri Constantine, University of Oran Es-Senia. Best universities of qualifications remain the University of Abou Bekr BelkaÃ¯d in Tlemcen and University of Batna Hadj Lakhdar, they occupy the 26th and 45th row in Africa.


Index of Algeria-related articles
Outline of Algeria

Ageron, Charles-Robert (1991). Modern Algeriaâ€“ A History from 1830 to the Present. Translated from French and edited by Michael Brett. London: Hurst. ISBN 978-0-86543-266-6.
Aghrout, Ahmed; Bougherira, Redha M. (2004). Algeria in Transitionâ€“ Reforms and Development Prospects. Routledge. ISBN 978-0-415-34848-5.
Bennoune, Mahfoud (1988). The Making of Contemporary Algeriaâ€“ Colonial Upheavals and Post-Independence Development, 1830â€“1987. Cambridge: Cambridge University Press. ISBN 978-0-521-30150-3.
Fanon, Frantz (1966; 2005 paperback). The Wretched of the Earth. Grove Press. ASIN B0007FW4AW, ISBN 978-0-8021-4132-3.
Horne, Alistair (1977). A Savage War of Peace: Algeria 1954â€“1962. Viking Adult. ISBN 978-0-670-61964-1, ISBN 978-1-59017-218-6 (2006 reprint)
Laouisset, Djamel (2009). A Retrospective Study of the Algerian Iron and Steel Industry. New York City: Nova Publishers. ISBN 978-1-61761-190-2.
Roberts, Hugh (2003). The Battlefieldâ€“ Algeria, 1988â€“2002. Studies in a Broken Polity. London: Verso Books. ISBN 978-1-85984-684-1.
Ruedy, John (1992). Modern Algeriaâ€“ The Origins and Development of a Nation. Bloomington: Indiana University Press. ISBN 978-0-253-34998-9.
Stora, Benjamin (2001). Algeria, 1830â€“2000â€“ A Short History. Ithaca, New York: Cornell University Press. ISBN 978-0-8014-3715-1.
Sidaoui, Riadh (2009). "Islamic Politics and the Militaryâ€“ Algeria 1962â€“2008". . Farnham: Ashgate Publishing. ISBN 0-7546-7418-5.
 official government website  / 
 from the BBC News

 from International Futures


This is a list of characters in Ayn Rand's novel Atlas Shrugged.

The following are major characters from the novel.



Dagny Taggart is the protagonist of the novel. She is Vice-President in Charge of Operations for Taggart Transcontinental, under her brother, James Taggart. Given James' incompetence, Dagny is responsible for all the workings of the railroad.

Francisco d'Anconia is one of the central characters in Atlas Shrugged, an owner by inheritance of the world's largest copper mining operation. He is a childhood friend, and the first love, of Dagny Taggart. A child prodigy of exceptional talents, Francisco was dubbed the "climax" of the d'Anconia line, an already prestigious family of skilled industrialists. He was a classmate of John Galt and Ragnar DanneskjÃ¶ld and student of both Hugh Akston and Robert Stadler. He began working while still in school, proving that he could have made a fortune without the aid of his family's wealth and power. Later, Francisco bankrupts the d'Anconia business to put it out of others' reach. His full name is given as "Francisco Domingo Carlos Andres SebastiÃ¡n d'Anconia".


John Galt is the primary male hero of Atlas Shrugged. He initially appears as an unnamed menial worker for Taggart Transcontinental, who often dines with Eddie Willers in the employees' cafeteria, and leads Eddie to reveal important information about Dagny Taggart and Taggart Transcontinental. Only Eddie's side of their conversations is given in the novel. Later in the novel, the reader discovers this worker's true identity.

Before working for Taggart Transcontinental, Galt worked as an engineer for the Twentieth Century Motor Company, where he secretly invented a generator of usable electric energy from ambient static electricity, but abandoned his prototype, and his employment, when dissatisfied by an easily corrupted novel system of payment. This prototype was found by Dagny Taggart and Hank Rearden. Galt himself remains concealed, throughout much of the novel, in a valley concealed by himself, where he unites the most skillful inventors and business leaders under his leadership. Much of the book's third division is given to his broadcast speech, which presents the author's philosophy of Objectivism.

Henry (known as "Hank") Rearden is one of the central characters in Atlas Shrugged. He owns the most important steel company in the United States, and invents Rearden Metal, an alloy stronger than steel (with similar properties to stainless steel). He lives in Philadelphia with his wife Lillian, his brother Philip, and his elderly mother. Rearden represents a type of self-made man or prototypical hero, and illustrates Rand's theory of sex in so far as he accepts the traditional view of sexual congress as a subhuman instinct, but responds sexually to Dagny Taggart.

Edwin "Eddie" Willers is the Special Assistant to the Vice-President in Charge of Operations at Taggart Transcontinental. His father and grandfather worked for the Taggarts, and himself likewise. He is completely loyal to Dagny and to Taggart Transcontinental. Willers does not possess the creative ability of Galt's associates, but matches them in moral courage and is capable of appreciating and making use of their creations. After Dagny shifts her attention and loyalty to saving the captive Galt, Willers maintains the railroad until its collapse.

One of Galt's first followers, and world famous as a pirate, who seizes relief ships sent from the United States to the People's States of Europe. He works to  ensure that once those espousing Galt's philosophy are restored to their rightful place in society, they have enough capital to rebuild the world. Kept in the background for much of the book, DanneskjÃ¶ld makes a personal appearance to encourage Rearden to persevere in his increasingly difficult situation, and gives him a bar of gold as compensation for the income taxes he has paid over the last several years. DanneskjÃ¶ld is married to the actress Kay Ludlow; their relationship is kept hidden from the outside world, which only knows of Ludlow as a retired film star. Considered a misfit by Galt's other adherents, he views his actions as a means to speed the world along in understanding Galt's perspective.

According to Barbara Branden, who was closely associated with Rand at the time the book was written, there were sections written describing DanneskjÃ¶ld's adventures at sea, cut from the final published text. In a 1974 comment at a lecture, Ayn Rand admitted that DanneskjÃ¶ld's name was a tribute to Victor Hugo's novel, Hans of Iceland, wherein the hero becomes the first of the Counts of DanneskjÃ¶ld. In the published book, DanneskjÃ¶ld is always seen through the eyes of others (Dagny Taggart or Hank Rearden), except for a brief paragraph in the very last chapter.


The President of Taggart Transcontinental and the book's most important antagonist. Taggart is an expert influence peddler but incapable of making operational decisions on his own. He relies on his sister, Dagny Taggart, to actually run the railroad, but nonetheless opposes her in almost every endeavor. In a sense, he is the antithesis of Dagny. This contradiction leads to the recurring absurdity of his life: the desire to overcome those on whom his life depends, and the horror that he will succeed at this. In the final chapters of the novel, he suffers a complete mental breakdown upon realizing that he can no longer deceive himself in this respect.

The unsupportive wife of Hank Rearden, who dislikes his habits and (secretly at first) seeks to ruin Rearden to prove her own value. Lillian achieves this, when she passes information to James Taggart about her husband's affair with his sister. This information is used to persuade Rearden to sign a Gift Certificate which delivers all the property rights of Rearden Metal to others. Lillian thereafter uses James Taggart for sexual satisfaction, until Hank abandons her.


Ferris is a biologist who works as "co-ordinator" at the State Science Institute. He uses his position there to deride reason and productive achievement, and publishes a book entitled Why Do You Think You Think? He clashes on several occasions with Hank Rearden, and twice attempts to blackmail Rearden into giving up Rearden Metal. He is also one of the group of looters who tries to get Rearden to agree to the Steel Unification Plan. Ferris hosts the demonstration of the Project X weapon, and is the creator of the Ferris Persuader, a torture machine. When John Galt is captured by the looters, Ferris uses the device on Galt, but it breaks down before extracting the information Ferris wants from Galt. Ferris represents the group which uses brute force on the heroes to achieve the ends of the looters.

A former professor at Patrick Henry University, and along with colleague Hugh Akston, mentor to Francisco d'Anconia, John Galt and Ragnar DanneskjÃ¶ld. He has since become a sell-out, one who had great promise but squandered it for social approval, to the detriment of the free. He works at the State Science Institute where all his inventions are perverted for use by the military, including the instrument of his demise: Project X (Xylophone). The character was, in part, modeled on J. Robert Oppenheimer, whom Rand had interviewed for an earlier project, and his part in the creation of nuclear weapons. To his former student Galt, Stadler represents the epitome of human evil, as the "man who knew better" but chose not to act for the good.

The incompetent and treacherous lobbyist whom Hank Rearden reluctantly employs in Washington, who rises to prominence and authority throughout the novel through trading favours and disloyalty. In return for betraying Hank by helping broker the Equalization of Opportunity Bill (which, by restricting the number of businesses each person may own to one, forces Hank to divest most of his companies), he is given a senior position at the Bureau of Economic Planning and National Resources. Later in the novel he becomes its Top Co-ordinator, a position that eventually becomes Economic Dictator of the country.

The following secondary characters also appear in the novel.

Hugh Akston is identified as "One of the last great advocates of reason." He was a renowned philosopher and the head of the Department of Philosophy at Patrick Henry University, where he taught Francisco d'Anconia, John Galt, and Ragnar DanneskjÃ¶ld. He was, along with Robert Stadler, a father figure to these three. Akston's name is so hallowed that a young lady, on hearing that Francisco had studied under him, is shocked. She thought he must have been one of those great names from an earlier century. He now works as a cook in a roadside diner, and proves extremely skillful at the job. When Dagny tracks him down, and before she discovers his true identity, he rejects her enthusiastic offer to manage the dining car services for Taggart Transcontinental. He is based on Aristotle.
Jeff Allen is a tramp who stows away on a Taggart train during one of Dagny's cross-country trips. Instead of throwing him out, she allows him to ride as her guest. It is from Allen that she learns the full story behind the collapse of the Twentieth Century Motor Company (Rand's extensive metaphor for the inherent flaws of communism), as well as a hint of John Galt's true background.
Calvin Atwood is owner of Atwood Light and Power Company and joins Galt's strike.
Mayor Bascom is the mayor of Rome, Wisconsin, who reveals part of the history of the Twentieth Century Motor Company.
Dr. Blodgett is the scientist who pulls the lever to demonstrate Project X.
Orren Boyle is the head of Associated Steel, antithesis of Hank Rearden and a friend of James Taggart. He is an investor in the San SebastiÃ¡n Mines. He disappears from the story after having a nervous breakdown following the failed 'unification' of the steel industry.
Laura Bradford is an actress and Kip Chalmers's mistress.
Bill Brent is the chief dispatcher for the Colorado Division of Taggart Transcontinental, who tries to prevent the Taggart Tunnel disaster.
Cherryl Brooks is a dime store shopgirl who marries James Taggart after a chance encounter in her store the night the John Galt Line was falsely deemed his greatest success. She marries him thinking he is the heroic person behind Taggart Transcontinental. Cherryl is at first harsh towards Dagny, having believed Jim Taggart's descriptions of his sister, until she questions employees of the railroad. Upon learning that her scorn had been misdirected, Cherryl puts off apologizing to Dagny out of shame until the night before she commits suicide, when she confesses to Dagny that when she married Jim, she thought he had the heroic qualities that she had looked up to - she thought she was marrying someone like Dagny. She eventually commits suicide, unable to live with her worthless husband, and unable to escape.
Millie Bush was "a mean, ugly little eight-year-old" girl voted to receive gold braces to straighten her teeth by the Marxist "family" committee who determined how pay was allocated at The Twentieth Century Motor Company. Her teeth are later knocked out by a man denied an allowance by the committee to purchase the things he valued.
Emma Chalmers, Kip Chalmers' mother, gains some influence after his death. Known as "Kip's Ma," she starts a soybean-growing project in Louisiana and commandeers thousands of railcars to move the harvest. As a result, the year's wheat crop from Minnesota never reaches the rest of the country, but instead rots in storage; also, the soybean crop is lost, having been reaped too early.
Kip Chalmers is a Washington man who has decided to run for election as Legislator from California. On the way to his campaign, the Taggart Transcontinental train that is carrying him encounters a split rail, resulting in the destruction of its diesel engine. His demands lead to a coal-burning steam engine being attached to his train in its stead and used to pull it through an eight-mile tunnel. The result is the suffocation of all passengers and the destruction of the Taggart Tunnel.
Dan Conway is the middle-aged president of the Phoenix-Durango railroad. Running a railroad is just about the only thing he knows. When the Anti-dog-eat-dog Rule is used to drive his business out of Colorado, he loses the will to fight, and resigns himself to a quiet life of books and fishing.
Ken Danagger owns Danagger Coal in Pennsylvania. He helps Hank Rearden illegally make Rearden Metal, then later decides to quit and join Galt's strike moments before Dagny arrives to try to persuade him otherwise.
Quentin Daniels is an enterprising engineer hired by Dagny Taggart to reconstruct John Galt's motor. Partway through this process, Quentin withdraws his effort for the same reasons John Galt himself had. Dagny's pursuit of Quentin leads her to Galt's Gulch.
Sebastian d'Anconia was the 16th (or 17th) Century founder of the d'Anconia dynasty. Escaped from Spain because of expressing his opinions too freely and coming in conflict with the Inquisition, leaving behind a palace and his beloved. Started a small mine in South America, which became the beginning of a mining empire and a new fortune (and a new palace). Eventually sent for his beloved who had waited for him many years. He is the role model which Francisco d'Anconia looks to, as Dagny Taggart looks to Nathaniel Taggart. Francisco remarks that their respective ancestors would have liked each other.
Balph Eubank is called "the literary leader of the age", despite the fact that no book he has written has sold more than 3,000 copies. He complains that it is disgraceful that artists are treated as peddlers, and that there should be a law limiting the sales of books to 10,000 copies. He is a misogynist who thinks it disgusting that Dagny Taggart is a railroad vice-president.
The Fishwife is one of the strikers, who earns her living by providing the fish for Hammondâ€™s grocery market; she is described as having "dark, disheveled hair and large eyes", and is a writer. Galt says she "wouldn't be published outside. She believes that when one deals with words, one deals with the mind." According to Barbara Branden in her book The Passion of Ayn Rand, "The Fishwife is Ayn's Hitchcock-like appearance in Atlas Shrugged." So says too Leonard Peikoff.
Lawrence Hammond runs Hammond Cars in Colorado, one of the few companies in existence that still produces top-quality vehicles. He eventually quits and joins the strike.
Richard Halley is Dagny Taggart's favorite composer, who mysteriously disappeared after the evening of his greatest triumph. Halley spent years as a struggling and unappreciated composer. At age 24, his opera Phaethon was performed for the first time, to an audience who booed and heckled it. After 19 years, Phaethon was performed again, but this time it was received to the greatest ovation the opera house had ever heard. The following day, Halley retired, sold the rights to his music, and disappeared. It is later revealed that he has joined the strike and settled in Galt's Gulch.
Mrs. William Hastings is the widow of the chief engineer at the Twentieth Century Motor Company. Her husband quit shortly after Galt did and joined the strike some years later. Her lead allows Dagny to find Hugh Akston.
Dr. Thomas Hendricks is a famous brain surgeon who developed a new method of preventing strokes. He joined Galt's strike when the American medical system was put under government control.
Tinky Holloway is one of the "looters" and is frequently referred to and quoted by other characters in the story, but he has only one major appearance: during the Washington meeting with Hank Rearden.
Lee Hunsacker is in charge of a company called Amalgamated Service when takes over the Twentieth Century Motor Company. He files a lawsuit that eventually leads to Midas Mulligan and Judge Narragansett joining the strike. A failed businessman, he laments constantly that no-one ever gave him a chance.
Gwen Ives is Hank Rearden's secretary.
Owen Kellogg is Assistant to the Manager of the Taggart Terminal in New York. He catches Dagny Taggart's eye as one of the few competent men on staff. After seeing the sorry state of the Ohio Division, she decides to make him its new Superintendent. However, as soon as she returns to New York, Kellogg informs her that he is quitting his job. Owen Kellogg eventually reaches, and settles in, Galt's Gulch.
Gilbert Keith-Worthing is a British novelist of erstwhile fame, now neglected but still considered a "walking classic". Rand introduces him only for a few pages as a guest of Kip Chalmers ("for no reason that either of them could discover") to then implicitly have him asphyxiated in the Tunnel catastrophe, after giving some statements.
Fred Kinnan is a labor leader and member of the looter cabal. Unlike the others, however, Kinnan is straightforward and honest about his purpose. Kinnan is the only one to openly state the true motivations of himself and his fellow conspirators. At the end of Galt's three-hour speech, he expresses admiration for the man, as he says what he means. Despite this, Kinnan admits that he is one of the people Galt is out to destroy.
Paul Larkin is an unsuccessful, middle-aged businessman, a friend of the Rearden family. He meets with the other Looters to work out a plan to bring Rearden down. James Taggart knows he is friends with Hank Rearden and challenges his loyalty, and Larkin assures Taggart that he will go along with them.
Eugene Lawson heads the Community Bank of Madison, then gets a job with the government when it his bank goes bankrupt. One of the looter's cabal, he is a collectivist who abhors production and money-making.
Mort Liddy is a hack composer who writes trite scores for movies and modern symphonies to which no one listens. He believes melody is a primitive vulgarity. He is one of Lillian Rearden's friends and a member of the cultural elite.
Clifton Locey is a friend of Jim Taggart who takes the position of vice-president of operation when Dagny Taggart quits.
Pat Logan is the engineer on the first run of the John Galt Line. He later strikes.
Kay Ludlow is a beautiful actress and the wife of Ragnar DanneskjÃ¶ld.
Dick McNamara is a contractor who finished the San Sebastian Line. Dagny Taggart plans to hire him to lay the new Rearden Metal track for the Rio Norte Line, but before she does so, he mysteriously disappears. She later discovers that he has joined the strike and settled in Galt's Gulch.
Cuffy Meigs is the Director of Unification for the railroad business. He carries a pistol and a lucky rabbit's foot, and he dresses in a military uniform, and has been described as "impervious to thought". Meigs seizes control of Project X and accidentally destroys it, demolishing the country's last railroad bridge across the Mississippi River and killing himself, his men, and Dr. Stadler.
Dave Mitchum is a state-hired superintendent of the Colorado Division of Taggart Transcontinental. He is partially responsible for the Taggart Tunnel disaster.
Chick Morrison holds the position of "Morale Conditioner" in the government. He quits when society begins to collapse and flees to a stronghold in Tennessee. His fellow looters consider it unlikely that he will survive.
Horace Bussby Mowen is the president of the Amalgamated Switch and Signal Company, Inc. of Connecticut. He is a businessman who sees nothing wrong with the moral code that is destroying society and would never dream of saying he is in business for any reason other than the good of society. Dagny Taggart hires Mowen to produce switches made of Rearden Metal. He is reluctant to build anything with this unproven technology, and has to be cajoled into accepting the contract. When pressured by public opinion, he discontinues production of the switches, forcing Dagny to find an alternative source.
Midas Mulligan is a wealthy banker who mysteriously disappeared in protest after he was given a court order to lend money to an incompetent applicant. When the order came down, he liquidated his entire business, paid off his depositors, and joined Galt's strike. He is the legal owner of the land where Galt's Gulch is located. Mulligan's birth name was Michael, but he had it legally changed after a news article called him "Midas" in a derogatory fashion, which Mulligan took as a compliment.
Judge Narragansett is an American jurist who ruled in favor of Midas Mulligan during the case brought against him by the incompetent loan applicant. When Narragansett's ruling was reversed on appeal, he retired and joined the strike. At the end of the novel, he is seen editing the United States Constitution, crossing out the contradicting amendments of it and adding an amendment to prohibit Congress from passing laws that restrain freedom of trade.
Ben Nealy is a railroad contractor whom Dagny Taggart hires to replace the track on the Rio Norte Line with Rearden Metal. Nealy is incompetent, but Dagny can find no one better in all the country. Nealy believes that anything can get done with enough muscle power. He sees no role for intelligence in human achievement. He relies on Dagny and Ellis Wyatt to run things, and resents them for doing it, because it appears to him like they are just bossing people around.
Ted Nielsen is the head of Nielsen Motors. He eventually goes on strike, along with most of the other industrialist "producer" types, by closing his motor factory. Dagny later finds him when she visits Galt's Gulch for the first time.
Betty Pope is a wealthy socialite who is having a meaningless sexual affair with James Taggart. She is deliberately crude in a way that casts ridicule on her high social position.
Dr. Potter holds some undefined position with the State Science Institute. He is sent to try to obtain the rights to Rearden Metal.
Dr. Simon Pritchett is the prestigious head of the Department of Philosophy at Patrick Henry University and is considered the leading philosopher of the age. He believes that man is nothing but a collection of chemicals, reason is a superstition, it is futile to seek meaning in life, and the duty of a philosopher is to show that nothing can be understood.
Rearden's mother, whose name is not mentioned, lives with Rearden at his home in Philadelphia. She is involved in charity work, and berates Rearden whenever she can. She dotes on her weak son Philip Rearden.
Philip Rearden is the younger brother of Hank Rearden. He lives in his brother's home in Philadelphia and is completely dependent on him. He is resentful of his brother's charity.
Dwight Sanders owns Sanders Aircraft, a producer of high-quality airplanes, and joins the strike.
Bertram Scudder is an editorial writer for the magazine The Future. He typically bashes business and businessmen, but he never says anything specific in his articles, relying on innuendo, sneers, and denunciation. He wrote a hatchet job on Hank Rearden called The Octopus. He is also vocal in support of the Equalization of Opportunity Bill. Scudder claims that the most important thing in life is "brother love" but seems to have nothing but hatred for those around him. He loses his job after Dagny Taggart reveals her affair with Hank Rearden over air on his radio show.
Claude Slagenhop is president of political organization Friends of Global Progress and one of Lillian Rearden's friends. He believes that ideas are just air, that this is no time for talk, but for action. Global Progress is a sponsor of the Equalization of Opportunity Bill.
Gerald and Ivy Starnes are the two surviving children of Jed Starnes, the founder of the Twentieth Century Motor Company. Together with their since-deceased brother Eric, they instituted a communistic payment-and-benefits program that drove the company into bankruptcy. Gerald, a dying alcoholic, and Ivy, a pseudo-Buddhist ascetic, continue to insist that the plan was perfect and that the failure of their father's company was entirely due to the workers. Eric was a weak, attention-seeking man with a pathological desire to be loved. He committed suicide after the woman he loved married another man. Gerald claims that he always acted for the good of the employees, but he was vain and incompetent and often threw lavish parties using company funds. Ivy, on the other hand, is described as a sadist who relishes seeing others in poverty, but who has no desire for wealth of her own.
Andrew Stockton runs the Stockton Foundry in Stockton, Colorado. When he joins the strike, he opens a foundry in Galt's Gulch.
Nathaniel "Nat" Taggart was the founder of Taggart Transcontinental. He built his railroad without any government handouts, and ran the business for no other reason than to turn a profit. He began as a penniless adventurer and ended up as one of the wealthiest men in the country. He never earned money by force or fraud (except for bribing government officials and throwing an opponent down a flight of stairs), and never apologized for becoming wealthy and successful. He was one of the most hated men of his time. Dagny is often inspired by looking at a statue of Nat Taggart at the railroad headquarters, and draws a dollar sign on its base as a signal to Francisco when she is ready to join Galt's strike.
Mr. Thompson is the "Head of the State" for the United States. He is not particularly intelligent and has a very undistinguished look. He knows politics, however, and is a master of public relations and back-room deals. Rand's notes indicate that she modeled him on President Harry S. Truman, and that she deliberately decided not to call him "President of the United States" as this title has "honorable connotations" which the character does not deserve.
Lester Tuck is the press agent for Kip Chalmers.
Clem Weatherby is a government representative on the board of directors of Taggart Transcontinental. Dagny considers him the least bad of the government representatives, since he does have some real knowledge on the running of trains. She notices, however, that he is the least appreciated by his own bosses.
The Wet Nurse (Tony) is a young bureaucrat sent by the government to watch over Reardenâ€™s mills. Though he starts out as a cynical follower of the lootersâ€™ code, his experience at the mills transforms him, and he comes to respect and admire the producers. He is shot attempting to inform Hank Rearden about a government plot, but does succeed in warning Rearden just before he dies.
Ellis Wyatt is the head of Wyatt Oil. He has almost single-handedly revived the economy of Colorado by discovering a new process for extracting more oil from what were thought to be exhausted oil wells. When first introduced, he is aggressive towards Dagny, whom he does not yet know and whom he blames for what are, in fact, her brother's policies which directly threaten his business. When the government passes laws and decrees which make it impossible for him to continue, he sets all his oil wells on fire, leaving a jeering note: "I am leaving it as I found it. Take over. It's yours." One particular burning well that resists all efforts to extinguish it becomes known as "Wyatt's Torch". Later Dagny meets him in Galt's Gulch.

Companies in Atlas Shrugged


Anthropology  is the study of humans. Its main subdivisions are cultural anthropology, which describes the workings of societies around the world, and biological anthropology, which concerns long-term development of the human organism. A related discipline, archaeology, studies past human cultures through investigation of physical evidence.

The Oxford Dictionaries define it as "The study of humankind", to include "cultural or social anthropology" and "physical anthropology." The Encyclopedia Britannica has a synonymous statement, "the science of humanity," but lists a slightly different catalogue: "biology and evolutionary history," and "society and culture." The American Anthropological Association offers: "the study of humans, past and present," which "draws and builds upon ... the social and biological sciences as well as the humanities and physical sciences." Eric R. Wolf states "Ideas about race, culture and peoplehood or ethnicity have long served to orient anthropology's inquiries ...."


The term  is a compound of Greek  anthrÅpos, "human being" (understood to mean "humankind" or "humanity"), and -Î»Î¿Î³Î¯Î± , "study." Unknown in ancient Greek or Latin, it first appears in the scholarly Latin anthropologia of Renaissance France, where it spawns the French word anthropologie, transferred into English as anthropology. It belongs to a class of words produced with the â€“logy suffix, such as archeo-logy, bio-logy, etc., "the study (or science) of.

Anthropologie has been in use since as early as the 17th century as translation of anthropologia. The Bartholins, founders of the University of Copenhagen, defined it as follows:

Anthropology, that is to say the science that treats of man, is divided ordinarily and with reason into Anatomy, which considers the body and the parts, and Psychology, which speaks of the soul.

Sporadic use of the term for some of the subject matter occurred subsequently, such as the use by Ã‰tienne Serres in 1838 to describe the natural history, or paleontology, of man, based on comparative anatomy, and the creation of a chair in anthropology and ethnography in 1850 at the National Museum of Natural History (France) by Jean Louis Armand de Quatrefages de BrÃ©au. Various short-lived organizations  of anthropologists had already been formed. The SociÃ©tÃ© Ethnologique de Paris, the first to use Ethnology, was formed in 1839. Its members were primarily anti-slavery activists. When slavery was abolished in France in 1848 the SociÃ©tÃ© was abandoned.

Meanwhile the Ethnological Society of New York, currently the American Ethnological Society, was founded on its model in 1842, as well as the Ethnological Society of London in 1843, a break-away group of the Aborigines' Protection Society. These anthropologists of the times were liberal, anti-slavery, and pro-human-rights activists. They maintained international connections.

Anthropology and many other current fields are the intellectual results of the comparative methods developed in the earlier 19th century. Theorists in such diverse fields as anatomy, linguistics, and  Ethnology, making  feature-by-feature comparisons of their subject matters, were beginning to suspect that similarities between animals, languages, and folkways were the result of processes or laws unknown to them then. For them, the publication of Charles Darwinâ€™s On the Origin of Species was the epiphany of everything they had begun to suspect. Darwin himself arrived at his conclusions through comparison of species he had seen in agronomy and in the wild.

Darwin and Wallace unveiled evolution in the late 1850â€™s. There was an immediate rush to bring it into the social sciences. Paul Broca in Paris was in the process of breaking away from the  SociÃ©tÃ© de biologie to form the first of the explicitly anthropological societies, the SociÃ©tÃ© dâ€™Anthropologie de Paris, meeting for the first time in Paris in 1859. When he read Darwin he became an immediate convert to Transformisme, as the French called evolutionism. His definition now became "the study of the human group, considered as a whole, in its details, and in relation to the rest of nature".

Broca, being what today would be called a neurosurgeon, had taken an interest in the pathology of speech. He wanted to localize the difference between man and the other animals, which appeared to reside in speech. He discovered the speech center of the human brain, today called Broca's area after him. His interest was mainly in Biological anthropology, but a German philosopher specializing in psychology, Theodor Waitz, took up the theme of general and social anthropology in his six-volume work, entitled Die Anthropologie der NaturvÃ¶lker, 1859-1864. The title was soon translated as "The Anthropology of Primitive Peoples".  The last two volumes were published posthumously.

Waitz defined anthropology as "the science of the nature of man". By nature he meant matter animated by "the Divine breath"; i.e., he was an animist. Following Brocaâ€™s lead, Waitz points out that anthropology is a new field, which would gather material from other fields, but would differ from them in the use of comparative anatomy, physiology, and psychology to differentiate man from "the animals nearest to him". He stresses that the data of comparison must be empirical, gathered by experimentation. The history of civilization as well as ethnology are to be brought into the comparison. It is to be presumed fundamentally that the species, man, is a unity, and that "the same laws of thought are applicable to all men".

Waitz was influential among the British ethnologists. In 1863 the explorer, Richard Francis Burton and the speech therapist, James Hunt broke away from the Ethnological Society of London to form the Anthropological Society of London, which henceforward would follow the path of the new anthropology rather than just ethnology. It was the 2nd society dedicated to general anthropology in existence. Representatives from the French SociÃ©tÃ© were present, though not Broca. In his keynote address, printed in the first volume of its new publication, The Anthropological Review, Hunt stressed the work of Waitz, adopting his definitions as a standard. Among the first associates were the young Edward Burnett Tylor, inventor of cultural anthropology, and his brother Alfred Tylor, a geologist. Previously Edward had referred to himself as an ethnologist; subsequently, an anthropologist.

Similar organizations in other countries followed: The American Anthropological Association in 1902, the Anthropological Society of Madrid (1865), the Anthropological Society of Vienna (1870), the Italian Society of Anthropology and Ethnology (1871), and many others subsequently. The majority of these were evolutionist. One notable exception was the  (1869) founded by Rudolph Virchow, known for his vituperative attacks on the evolutionists. Not religious himself, he insisted that Darwinâ€™s conclusions lacked empirical foundation.

During the last three decades of the 19th century a proliferation of anthropological societies and associations occurred, most independent, most publishing their own journals, and all international in membership and association.  The major theorists belonged to these organizations. They supported the gradual osmosis of anthropology curricula into the major institutions of higher learning. By 1898 the American Association for the Advancement of Science was able to report that 48 educational institutions in 13 countries had some curriculum in anthropology. None of the 75 faculty members were under a department named anthropology.

This meagre statistic expanded in the 20th century to comprise anthropology departments in the majority of the worldâ€™s higher educational institutions, many thousands in number. Anthropology has diversified from a few major subdivisions to dozens more. Practical anthropology, the use of anthropological knowledge and technique to solve specific problems, has arrived; for example, the presence of buried victims might stimulate the use of a forensic archaeologist to recreate the final scene. Organization has reached global level. For example, the World Council of Anthropological Associations (WCAA), "a network of national, regional and international associations that aims to promote worldwide communication and cooperation in anthropology", currently contains members from about three dozen nations.

Since the work of Franz Boas and BronisÅ‚aw Malinowski in the late 19th and early 20th centuries, social anthropology in Great Britain and cultural anthropology in the US have been distinguished from other social sciences by its emphasis on cross-cultural comparisons, long-term in-depth examination of context, and the importance it places on participant-observation or experiential immersion in the area of research. Cultural anthropology in particular has emphasized cultural relativism, holism, and the use of findings to frame cultural critiques. This has been particularly prominent in the United States, from Boas' arguments against 19th-century racial ideology, through Margaret Mead's advocacy for gender equality and sexual liberation, to current criticisms of post-colonial oppression and promotion of multiculturalism. Ethnography is one of its primary research designs as well as the text that is generated from anthropological fieldwork.

In Great Britain and the Commonwealth countries, the British tradition of social anthropology tends to dominate. In the United States, anthropology has traditionally been divided into the four field approach developed by Franz Boas in the early 20th century: biological or physical anthropology; social, cultural, or sociocultural anthropology; and archaeology; plus anthropological linguistics. These fields frequently overlap, but tend to use different methodologies and techniques.

European countries with overseas colonies tended to practice more ethnology (a term coined and defined by Adam F. KollÃ¡r in 1783). In non-colonial European countries, social anthropology is now defined as the study of social organization in non-state societies. It is sometimes referred to as sociocultural anthropology in the parts of the world that were influenced by the European tradition.

Anthropology is a global discipline where humanities, social, and natural sciences are forced to confront one another. Anthropology builds upon knowledge from natural sciences, including the discoveries about the origin and evolution of Homo sapiens, human physical traits, human behavior, the variations among different groups of humans, how the evolutionary past of Homo sapiens has influenced its social organization and culture, and from social sciences, including the organization of human social and cultural relations, institutions, social conflicts, etc. Early anthropology originated in Classical Greece and Persia and studied and tried to understand observable cultural diversity. As such, anthropology has been central in the development of several new (late 20th century) interdisciplinary fields such as cognitive science, global studies, and various ethnic studies.

According to Clifford Geertz, 
Sociocultural anthropology has been heavily influenced by structuralist and postmodern theories, as well as a shift toward the analysis of modern societies. During the 1970s and 1990s, there was an epistemological shift away from the positivist traditions that had largely informed the discipline. During this shift, enduring questions about the nature and production of knowledge came to occupy a central place in cultural and social anthropology. In contrast, archaeology and biological anthropology remained largely positivist. Due to this difference in epistemology, the four sub-fields of anthropology have lacked cohesion over the last several decades.


Sociocultural anthropology draws together the principle axes of cultural anthropology and social anthropology. Cultural anthropology is the comparative study of the manifold ways in which people make sense of the world around them, while social anthropology is the study of the relationships among persons and groups. Cultural anthropology is more related to philosophy, literature and the arts (how one's culture affects experience for self and group, contributing to more complete understanding of the people's knowledge, customs, and institutions), while social anthropology is more related to sociology and history. in that it helps develop understanding of social structures, typically of others and other populations (such as minorities, subgroups, dissidents, etc.).  There is no hard-and-fast distinction between them, and these categories overlap to a considerable degree.

Inquiry in sociocultural anthropology is guided in part by cultural relativism, the attempt to understand other societies in terms of their own cultural symbols and values. Accepting other cultures in their own terms moderates reductionism in cross-cultural comparison. This project is often accommodated in the field of ethnography. Ethnography can refer to both a methodology and the product of ethnographic research, i.e. an ethnographic monograph. As methodology, ethnography is based upon long-term fieldwork within a community or other research site. Participant observation is one of the foundational methods of social and cultural anthropology.  Ethnology involves the systematic comparison of different cultures. The process of participant-observation can be especially helpful to understanding a culture from an emic (conceptual, vs. etic,  or technical) point of view.

The study of kinship and social organization is a central focus of sociocultural anthropology, as kinship is a human universal. Sociocultural anthropology also covers economic and political organization, law and conflict resolution, patterns of consumption and exchange, material culture, technology, infrastructure, gender relations, ethnicity, childrearing and socialization, religion, myth, symbols, values, etiquette, worldview, sports, music, nutrition, recreation, games, food, festivals, and language (which is also the object of study in linguistic anthropology).

Comparison across cultures is a key element of method in sociocultural anthropology, including the industrialized (and de-industrialized) West. Cultures in the Standard Cross-Cultural Sample (SCCS) of world societies are:
Biological Anthropology and Physical Anthropology are synonymous terms to describe anthropological research focused on the study of humans and non-human primates in their biological, evolutionary, and demographic dimensions. It examines the biological and social factors that have affected the evolution of humans and other primates, and that generate, maintain or change contemporary genetic and physiological variation.

Archaeology is the study of the human past through its material remains.  Artifacts, faunal remains, and human altered landscapes are evidence of the cultural and material lives of past societies.  Archaeologists examine these material remains in order to deduce patterns of past human behavior and cultural practices.  Ethnoarchaeology is a type of archaeology that studies the practices and material remains of living human groups in order to gain a better understanding of the evidence left behind by past human groups, who are presumed to have lived in similar ways.


Linguistic anthropology (also called anthropological linguistics) seeks to understand the processes of human communications, verbal and non-verbal, variation in language across time and space, the social uses of language, and the relationship between language and culture. It is the branch of anthropology that brings linguistic methods to bear on anthropological problems, linking the analysis of linguistic forms and processes to the interpretation of sociocultural processes. Linguistic anthropologists often draw on related fields including sociolinguistics, pragmatics, cognitive linguistics, semiotics, discourse analysis, and narrative analysis.


One of the central problems in the anthropology of art concerns the universality of 'art' as a cultural phenomenon. Several anthropologists have noted that the Western categories of 'painting', 'sculpture', or 'literature', conceived as independent artistic activities, do not exist, or exist in a significantly different form, in most non-Western contexts. To surmount this difficulty, anthropologists of art have focused on formal features in objects which, without exclusively being 'artistic', have certain evident 'aesthetic' qualities. Boas' Primitive Art, Claude LÃ©vi-Strauss' The Way of the Masks (1982) or Geertz's 'Art as Cultural System' (1983) are some examples in this trend to transform the anthropology of 'art' into an anthropology of culturally specific 'aesthetics'.

Anthropology of media (also anthropology of mass media, media anthropology) emphasizes ethnographic studies as a means of understanding producers, audiences, and other cultural and social aspects of mass media. The types of ethnographic contexts explored range from contexts of media production (e.g., ethnographies of newsrooms in newspapers, journalists in the field, film production) to contexts of media reception, following audiences in their everyday responses to media.  Other types include cyber anthropology, a relatively new area of internet research, as well as ethnographies of other areas of research which happen to involve media, such as development work, social movements, or health education. This is in addition to many classic ethnographic contexts, where media such as radio, the press, new media and television have started to make their presences felt since the early 1990s.


Ethnomusicology is an academic field encompassing various approaches to the study of music (broadly defined), that emphasize its cultural, social, material, cognitive, biological, and other dimensions or contexts instead of or in addition to its isolated sound component or any particular repertoire.


Visual anthropology  is concerned, in part, with the study and production of ethnographic photography, film and, since the mid-1990s, new media.  While the term is sometimes used interchangeably with ethnographic film, visual anthropology also encompasses the anthropological study of visual representation, including areas such as performance, museums, art, and the production and reception of mass media.  Visual representations from all cultures, such as sandpaintings, tattoos, sculptures and reliefs, cave paintings, scrimshaw, jewelry, hieroglyphics, paintings and photographs are included in the focus of visual anthropology.


Economic anthropology attempts to explain human economic behavior in its widest historic, geographic and cultural scope. It has a complex relationship with the discipline of economics, of which it is highly critical. Its origins as a sub-field of anthropology begin with the Polish-British founder of Anthropology, Bronislaw Malinowski, and his French compatriot, Marcel Mauss, on the nature of gift-giving exchange (or reciprocity) as an alternative to market exchange. Economic Anthropology remains, for the most part, focused upon exchange. The school of thought derived from Marx and known as Political Economy focuses on production, in contrast. Economic Anthropologists have abandoned the primitivist niche they were relegated to by economists, and have now turned to examine corporations, banks, and the global financial system from an anthropological perspective.


Political economy in anthropology is the application of the theories and methods of Historical Materialism to the traditional concerns of anthropology, including, but not limited to, non-capitalist societies. Political Economy introduced questions of history and colonialism to ahistorical anthropological theories of social structure and culture. Three main areas of interest rapidly developed. The first of these areas was concerned with the "pre-capitalist" societies that were subject to evolutionary "tribal" stereotypes. Sahlins work on Hunter-gatherers as the 'original affluent society' did much to dissipate that image. The second area was concerned with the vast majority of the world's population at the time, the peasantry, many of whom were involved in complex revolutionary wars such as in Vietnam. The third area was on colonialism, imperialism, and the creation of the capitalist world-system. More recently, these Political Economists have more directly addressed issues of industrial (and post-industrial) capitalism around the world.


Applied Anthropology refers to the application of the method and theory of anthropology to the analysis and solution of practical problems. It is a, "complex of related, research-based, instrumental methods which produce change or stability in specific cultural systems through the provision of data, initiation of direct action, and/or the formulation of policy". More simply, applied anthropology is the practical side of anthropological research; it includes researcher involvement and activism within the participating community. It is closely related to Development anthropology (distinct from the more critical Anthropology of development).


Anthropology of development  tends to view development from a critical perspective. The kind of issues addressed and implications for the approach simply involve pondering why, if a key development goal is to alleviate poverty, is poverty increasing? Why is there such a gap between plans and outcomes? Why are those working in development so willing to disregard history and the lessons it might offer? Why is development so externally driven rather than having an internal basis? In short why does so much planned development fail?


Kinship can refer both to the study of the patterns of social relationships in one or more human cultures, or it can refer to the patterns of social relationships themselves. Over its history, anthropology has developed a number of related concepts and terms, such as descent, descent groups, lineages, affines, cognates and even fictive kinship. Broadly, kinship patterns may be considered to include people related both by descent (one's social relations during development), and also relatives by marriage.


Feminist anthropology is a four field approach to anthropology (archeological, biological, cultural, linguistic) that seeks to reduce male bias in research findings, anthropological hiring practices, and the scholarly production of knowledge. Anthropology engages often with feminists from non-Western traditions, whose perspectives and experiences can differ from those of white European and American feminists. Historically, such 'peripheral' perspectives have sometimes been marginalized and regarded as less valid or important than knowledge from the western world. Feminist anthropologists have claimed that their research helps to correct this systematic bias in mainstream feminist theory. Feminist anthropologists are centrally concerned with the construction of gender across societies. Feminist anthropology is inclusive of  as a specialization.

Medical anthropology is an interdisciplinary field which studies "human health and disease, health care systems, and biocultural adaptation".  Currently, research in medical anthropology is one of the main growth areas in the field of anthropology as a whole. It focuses on  the following six basic fields:
the development of systems of medical knowledge and medical care
the patient-physician relationship
the integration of alternative medical systems in culturally diverse environments
the interaction of social, environmental and biological factors which influence health and illness both in the individual and the community as a whole
the critical analysis of interaction between psychiatric services and migrant populations ("critical ethnopsychiatry": Beneduce 2004, 2007)
the impact of biomedicine and biomedical technologies in non-Western settings
Other subjects that have become central to medical anthropology worldwide are violence and social suffering (Farmer, 1999, 2003; Beneduce, 2010) as well as other issues that involve physical and psychological harm and suffering that are not a result of illness. On the other hand, there are fields that intersect with medical anthropology in terms of research methodology and theoretical production, such as cultural psychiatry and transcultural psychiatry or ethnopsychiatry.

Nutritional anthropology is a synthetic concept that deals with the interplay between economic systems, nutritional status and food security, and how changes in the former affect the latter. If economic and environmental changes in a community affect access to food, food security, and dietary health, then this interplay between culture and biology is in turn connected to broader historical and economic trends associated with globalization. Nutritional status affects overall health status, work performance potential, and the overall potential for economic development (either in terms of human development or traditional western models) for any given group of people.


Psychological anthropology is an interdisciplinary subfield of anthropology that studies the interaction of cultural and mental processes. This subfield tends to focus on ways in which humans' development and enculturation within a particular cultural groupâ€”with its own history, language, practices, and conceptual categoriesâ€”shape processes of human cognition, emotion, perception, motivation, and mental health. It also examines how the understanding of cognition, emotion, motivation, and similar psychological processes inform or constrain our models of cultural and social processes.


Cognitive anthropology seeks to explain patterns of shared knowledge, cultural innovation, and transmission over time and space using the methods and theories of the cognitive sciences (especially experimental psychology and evolutionary biology) often through close collaboration with historians, ethnographers, archaeologists, linguists, musicologists and other specialists engaged in the description and interpretation of cultural forms. Cognitive anthropology is concerned with what people from different groups know and how that implicit knowledge changes the way people perceive and relate to the world around them.


Transpersonal anthropology studies the relationship between altered states of consciousness and culture. As with transpersonal psychology, the field is much concerned with altered states of consciousness (ASC) and transpersonal experience. However, the field differs from mainstream transpersonal psychology in taking more cognizance of cross-cultural issuesâ€”for instance, the roles of myth, ritual, diet, and texts in evoking and interpreting extraordinary experiences (Young and Goulet 1994).


Political anthropology concerns the structure of political systems, looked at from the basis of the structure of societies. Political anthropology developed as a discipline concerned primarily with politics in stateless societies, a new development started from the 1960s, and is still unfolding: anthropologists started increasingly to study more "complex" social settings in which the presence of states, bureaucracies and markets entered both ethnographic accounts and analysis of local phenomena. The turn towards complex societies meant that political themes  were taken up at two main levels. First of all, anthropologists continued to study political organization and political phenomena that lay outside the state-regulated sphere (as in patron-client relations or tribal political organization). Second of all, anthropologists slowly started to develop a disciplinary concern with states and their institutions (and of course on the relationship between formal and informal political institutions). An anthropology of the state developed, and it is a most thriving field today. Geertz' comparative work on "Negara", the Balinese state is an early, famous example.


Legal anthropology, also known as Anthropology of Law specializes in "the cross-cultural study of social ordering".  Earlier legal anthropological research often focused more narrowly on conflict management, crime, sanctions, or formal regulation. More  recent applications include issues such as human rights, legal pluralism,  and political uprisings.


Public Anthropology was created by Robert Borofsky, a professor at Hawaii Pacific University, to "demonstrate the ability of anthropology and anthropologists to effectively address problems beyond the discipline - illuminating larger social issues of our times as well as encouraging broad, public conversations about them with the explicit goal of fostering social change" ().


Cyborg anthropology originated as a sub-focus group within the American Anthropological Association's annual meeting in 1993. The sub-group was very closely related to STS and the Society for the Social Studies of Science. Donna Haraway's 1985 Cyborg Manifesto could be considered the founding document of cyborg anthropology by first exploring the philosophical and sociological ramifications of the term. Cyborg anthropology studies humankind and its relations with the technological systems it has built, specifically modern technological systems that have reflexively shaped notions of what it means to be human beings.


Digital anthropology is the study of the relationship between humans and digital-era technology, and extends to various areas where anthropology and technology intersect. It is sometimes grouped with sociocultural anthropology, and sometimes considered part of material culture.  The field is new, and thus has a variety of names with a variety of emphases. These include techno-anthropology, digital ethnography, cyberanthropology, and virtual anthropology.


Ecological anthropology is defined as the "study of cultural adaptations to environments". The sub-field is also defined as, "the study of relationships between a population of humans and their biophysical environment". The focus of its research concerns "how cultural beliefs and practices helped human populations adapt to their environments, and how people used elements of their culture to maintain their ecosystems."


Environmental anthropology is a sub-specialty  within the field of anthropology that takes an active role in examining the relationships between humans and their environment across space and time. The contemporary perspective of environmental anthropology, and arguably at least the backdrop, if not the focus of most of the ethnographies and cultural fieldworks of today, is political ecology. Many characterize this new perspective as more informed with culture, politics and power, globalization, localized issues, and more. The focus and data interpretation is often used for arguments for/against or creation of policy, and to prevent corporate exploitation and damage of land. Often, the observer has become an active part of the struggle either directly (organizing, participation) or indirectly (articles, documentaries, books, ethnographies). Such is the case with environmental justice advocate Melissa Checker and her relationship with the people of Hyde Park.


Ethnohistory is the study of ethnographic cultures and indigenous customs by examining historical records. It is also the study of the history of various ethnic groups that may or may not exist today. Ethnohistory uses both historical and ethnographic data as its foundation. Its historical methods and materials go beyond the standard use of documents and manuscripts. Practitioners recognize the utility of such source material as maps, music, paintings, photography, folklore, oral tradition, site exploration, archaeological materials, museum collections, enduring customs, language, and place names.

The anthropology of religion involves the study of religious institutions in relation to other social institutions, and the comparison of religious beliefs and practices across cultures. Modern anthropology assumes that there is complete continuity between magical thinking and religion, and that every religion is a cultural product, created by the human community that worships it.


Urban anthropology is concerned with issues of urbanization, poverty, and neoliberalism. Ulf Hannerz quotes a 1960s remark that traditional anthropologists were "a notoriously agoraphobic lot, anti-urban by definition". Various social processes in the Western World as well as in the "Third World" (the latter being the habitual focus of attention of anthropologists) brought the attention of "specialists in 'other cultures'" closer to their homes. There are two principle approaches in urban anthropology: by examining the types of cities or examining the social issues within the cities. These two methods are overlapping and dependent of each other. By defining different types of cities, one would use social factors as well as economic and political factors to categorize the cities. By directly looking at the different social issues, one would also be studying how they affect the dynamic of the city.


Anthrozoology (also called humanâ€“animal studies or HAS) is the study of interaction between living things. It is a modern interdisciplinary and burgeoning field that overlaps with a number of other disciplines, including  anthropology, ethology, medicine, psychology, veterinary medicine and zoology.  A major focus of anthrozoologic research is the quantifying of the positive effects of human-animal relationships on either party and the study of their interactions. It includes scholars from a diverse range of fields, including anthropology, sociology, biology, and philosophy.


Biocultural anthropology is the scientific exploration of the relationships between human biology and culture. Physical anthropologists throughout the first half of the 20th century viewed this relationship from a racial perspective; that is, from the assumption that typological human biological differences lead to cultural differences. After World War II the emphasis began to shift toward an effort to explore the role culture plays in shaping human biology.


Evolutionary anthropology is the interdisciplinary study of the evolution of human physiology and human behaviour and the relation between hominins and non-hominin primates. Evolutionary anthropology is based in natural science and social science, combining the human development with socioeconomic factors. Evolutionary anthropology is concerned with both biological and cultural evolution of humans, past and present. It is based on a scientific approach, and brings together fields such as archaeology, behavioral ecology, psychology, primatology, and genetics. It is a dynamic and interdisciplinary field, drawing on many lines of evidence to understand the human experience, past and present.


Forensic anthropology  is the application of the science of physical anthropology and human osteology in a legal setting, most often in criminal cases where the victim's remains are in the advanced stages of decomposition. A forensic anthropologist can assist in the identification of deceased individuals whose remains are decomposed, burned, mutilated or otherwise unrecognizable. The adjective "forensic" refers to the application of this subfield of science to a court of law.


Paleoanthropology combines the disciplines of paleontology and physical anthropology. It is the study of ancient humans, as found in fossil hominid evidence such as petrifacted bones and footprints.

Contemporary anthropology is an established science with academic departments at most universities and colleges. The single largest organization of Anthropologists is the American Anthropological Association (AAA), which was founded in 1903. Membership is made up of anthropologists from around the globe.

In 1989, a group of European and American scholars in the field of anthropology established the European Association of Social Anthropologists (EASA) which serves as a major professional organization for anthropologists working in Europe. The EASA seeks to advance the status of anthropology in Europe and to increase visibility of marginalized anthropological traditions and thereby contribute to the project of a global anthropology or world anthropology.

Hundreds of other organizations exist in the various sub-fields of anthropology, sometimes divided up by nation or region, and many anthropologists work with collaborators in other disciplines, such as geology, physics, zoology, paleontology, anatomy, music theory, art history, sociology and so on, belonging to professional societies in those disciplines as well.

American Anthropological Association
American Ethnological Society
AsociaciÃ³n de AntropÃ³logos Iberoamericanos en Red, AIBR
Moving Anthropology Student Network
Anthropological Society of London
Center for World Indigenous Studies
Ethnological Society of London
Institute of Anthropology and Ethnography
Max Planck Institute for Evolutionary Anthropology
Network of Concerned Anthropologists
N. N. Miklukho-Maklai Institute of Ethnology and Anthropology

Royal Anthropological Institute of Great Britain and Ireland
Society for anthropological sciences
Society for Applied Anthropology
USC Center for Visual Anthropology
Anthropologists, like other researchers (especially historians and scientists engaged in field research), have over time assisted state policies and projects, especially colonialism.

Some commentators have contended:

That the discipline grew out of colonialism, perhaps was in league with it, and derived some of its key notions from it, consciously or not.   (See, for example, Gough, Pels and Salemink, but cf. Lewis 2004).
That ethnographic work was often ahistorical, writing about people as if they were "out of time" in an "ethnographic present" (Johannes Fabian, Time and Its Other).

At the same time, anthropologists urge, as part of their quest for scientific objectivity, cultural relativism, which has an influence on all the sub-fields of anthropology. This is the notion that particular cultures should not be judged by one culture's values or viewpoints, but that all cultures should be viewed as relative to each other. There should be no notions, in good anthropology, of one culture being better or worse than another culture.

Ethical commitments in anthropology include noticing and documenting genocide, infanticide, racism, mutilation including circumcision and subincision, and torture. Topics like racism, slavery or human sacrifice, therefore, attract anthropological attention and theories ranging from nutritional deficiencies to genes to acculturation have been proposed, not to mention theories of colonialism and many others as root causes of Man's inhumanity to man. To illustrate the depth of an anthropological approach, one can take just one of these topics, such as "racism" and find thousands of anthropological references, stretching across all the major and minor sub-fields.

Anthropologists' involvement with the U.S. government, in particular, has caused bitter controversy within the discipline. Franz Boas publicly objected to US participation in World War I, and after the war he published a brief expose and condemnation of the participation of several American archaeologists in espionage in Mexico under their cover as scientists.

But by the 1940s, many of Boas' anthropologist contemporaries were active in the allied war effort against the "Axis" (Nazi Germany, Fascist Italy, and Imperial Japan). Many served in the armed forces, while others worked in intelligence (for example, Office of Strategic Services and the Office of War Information). At the same time, David H. Price's work on American anthropology during the Cold War provides detailed accounts of the pursuit and dismissal of several anthropologists from their jobs for communist sympathies.

Attempts to accuse anthropologists of complicity with the CIA and government intelligence activities during the Vietnam War years have turned up surprisingly little (although anthropologist Hugo Nutini was active in the stillborn Project Camelot). Many anthropologists (students and teachers) were active in the antiwar movement.  Numerous resolutions condemning the war in all its aspects were passed overwhelmingly at the annual meetings of the American Anthropological Association (AAA).

Professional anthropological bodies often object to the use of anthropology for the benefit of the state. Their codes of ethics or statements may proscribe anthropologists from giving secret briefings. The Association of Social Anthropologists of the UK and Commonwealth (ASA) has called certain scholarship ethically dangerous. The AAA's current 'Statement of Professional Responsibility' clearly states that "in relation with their own government and with host governments... no secret research, no secret reports or debriefings of any kind should be agreed to or given."

Anthropologists, along with other social scientists, are working with the US military as part of the US Army's strategy in Afghanistan. The Christian Science Monitor reports that "Counterinsurgency efforts focus on better grasping and meeting local needs" in Afghanistan, under the Human Terrain System (HTS) program; in addition, HTS teams are working with the US military in Iraq. In 2009, the American Anthropological Association's Commission on the Engagement of Anthropology with the US Security and Intelligence Communities released its final report concluding, in part, that, "When ethnographic investigation is determined by military missions, not subject to external review, where data collection occurs in the context of war, integrated into the goals of counterinsurgency, and in a potentially coercive environment â€“ all characteristic factors of the HTS concept and its application â€“ it can no longer be considered a legitimate professional exercise of anthropology. In summary, while we stress that constructive engagement between anthropology and the military is possible, CEAUSSIC suggests that the AAA emphasize the incompatibility of HTS with disciplinary ethics and practice for job seekers and that it further recognize the problem of allowing HTS to define the meaning of "anthropology" within DoD."

Before WWII British 'social anthropology' and American 'cultural anthropology' were still distinct traditions. After the war, enough British and American anthropologists borrowed ideas and methodological approaches from one another that some began to speak of them collectively as 'sociocultural' anthropology.

There are several characteristics that tend to unite anthropological work. One of the central characteristics is that anthropology tends to provide a comparatively more holistic account of phenomena and tends to be highly empirical. The quest for holism leads most anthropologists to study a particular place, problem or phenomenon in detail, using a variety of methods, over a more extensive period than normal in many parts of academia.

In the 1990s and 2000s (decade), calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.

Biological anthropologists are interested in both human variation and in the possibility of human universals (behaviors, ideas or concepts shared by virtually all human cultures).  They use many different methods of study, but modern population genetics, participant observation and other techniques often take anthropologists "into the field," which means traveling to a community in its own setting, to do something called "fieldwork."  On the biological or physical side, human measurements, genetic samples, nutritional data may be gathered and published as articles or monographs.

Along with dividing up their project by theoretical emphasis, anthropologists typically divide the world up into relevant time periods and geographic regions. Human time on Earth is divided up into relevant cultural traditions based on material, such as the Paleolithic and the Neolithic, of particular use in archaeology. Further cultural subdivisions according to tool types, such as Olduwan or Mousterian or Levalloisian help archaeologists and other anthropologists in understanding major trends in the human past. Anthropologists and geographers share approaches to Culture regions as well, since mapping cultures is central to both sciences. By making comparisons across cultural traditions (time-based) and cultural regions (space-based), anthropologists have developed various kinds of comparative method, a central part of their science.

Because anthropology developed from so many different enterprises (see History of Anthropology), including but not limited to fossil-hunting, exploring, documentary film-making, paleontology, primatology, antiquity dealings and curatorship, philology, etymology, genetics, regional analysis, ethnology, history, philosophy, and religious studies, it is difficult to characterize the entire field in a brief article, although attempts to write histories of the entire field have been made.

Some authors argue that anthropology originated and developed as the study of "other cultures", both in terms of time (past societies) and space (non-European/non-Western societies). For example, the classic of urban anthropology, Ulf Hannerz in the introduction to his seminal Exploring the City: Inquiries Toward an Urban Anthropology mentions that the "Third World" had habitually received most of attention; anthropologists who traditionally specialized in "other cultures" looked for them far away and started to look "across the tracks" only in late 1960s.

Now there exist many works focusing on peoples and topics very close to the author's "home". It is also argued that other fields of study, like History and Sociology, on the contrary focus disproportionately on the West.

In France, the study of Western societies has been traditionally left to sociologists, but this is increasingly changing, starting in the 1970s from scholars like Isac Chiva and journals like Terrain ("fieldwork"), and developing with the center founded by Marc AugÃ© (Le Centre d'anthropologie des mondes contemporains, the Anthropological Research Center of Contemporary Societies).

Since the 1980s it has become common for social and cultural anthropologists to set ethnographic research in the North Atlantic region, frequently examining the connections between locations rather than limiting research to a single locale. There has also been a related shift toward broadening the focus beyond the daily life of ordinary people; increasingly, research is set in settings such as scientific laboratories, social movements, governmental and nongovernmental organizations and businesses.

Anthropological Index Online (AIO)
Anthropological science fiction
Engaged theory
Ethnology
Ethnobiology
Ethology
Folklore
Human ethology
Human evolution
Human Relations Area Files
Intangible Cultural Heritage
Memetics
Origins of society
Prehistoric medicine
Qualitative research
Sociology
Theological anthropology, a sub-field of theology
Philosophical anthropology, a sub-field of philosophy
Anthropology in Tinbergen's four questions
.
.


Agricultural science is a broad multidisciplinary field of biology that encompasses the parts of exact, natural, economic and social sciences that are used in the practice and understanding of agriculture. (Veterinary science, but not animal science, is often excluded from the definition.)


The two terms are often confused. However, they cover different concepts:
Agriculture is the set of activities that transform the environment for the production of animals and plants for human use. Agriculture concerns techniques, including the application of agronomic research.
Agronomy is research and development related to studying and improving plant-based crops.
Agricultural sciences include research and development on:
Production techniques (e.g., irrigation management, recommended nitrogen inputs)
Improving agricultural productivity in terms of quantity and quality (e.g., selection of drought-resistant crops and animals, development of new pesticides, yield-sensing technologies, simulation models of crop growth, in-vitro cell culture techniques)
Minimizing the effects of pests (weeds, insects, pathogens, nematodes) on crop or animal production systems.
Transformation of primary products into end-consumer products (e.g., production, preservation, and packaging of dairy products)
Prevention and correction of adverse environmental effects (e.g., soil degradation, waste management, bioremediation)
Theoretical production ecology, relating to crop production modeling
Traditional agricultural systems, sometimes termed subsistence agriculture, which feed most of the poorest people in the world.  These systems are of interest as they sometimes retain a level of integration with natural ecological systems greater than that of industrial agriculture, which may be more sustainable than some modern agricultural systems.
Food production and demand on a global basis, with special attention paid to the major producers, such as China, India, Brazil, the USA and the EU.

Agricultural biotechnology is a specific area of agricultural science involving the use of scientific tools and techniques, including genetic engineering, molecular markers, molecular diagnostics, vaccines, and tissue culture, to modify living organisms: plants, animals, and microorganisms.

One of the most common yield reducers is because of fertilizer not being applied in slightly higher quantities during transition period, the time it takes the soil to rebuild its aggregates and organic matter.  Yields will decrease temporarily because of nitrogen being immobilized in the crop residue, which can take a few months to several years to decompose, depending on the crop's C to N ratio and the local environment


With the exception of theoretical agronomy, research in agronomy, more than in any other field, is strongly related to local areas. It can be considered a science of ecoregions, because it is closely linked to soil properties and climate, which are never exactly the same from one place to another. Many people think an agricultural production system relying on local weather, soil characteristics, and specific crops has to be studied locally. Others feel a need to know and understand production systems in as many areas as possible, and the human dimension of interaction with nature.

Agricultural science began with Gregor Mendel's genetic work, but in modern terms might be better dated from the chemical fertilizer outputs of plant physiological understanding in 18th-century Germany. In the United States, a scientific revolution in agriculture began with the Hatch Act of 1887, which used the term "agricultural science". The Hatch Act was driven by farmers' interest in knowing the constituents of early artificial fertilizer. The Smith-Hughes Act of 1917 shifted agricultural education back to its vocational roots, but the scientific foundation had been built. After 1906, public expenditures on agricultural research in the US exceeded private expenditures for the next 44 years.

Intensification of agriculture since the 1960s in developed and developing countries, often referred to as the Green Revolution, was closely tied to progress made in selecting and improving crops and animals for high productivity, as well as to developing additional inputs such as artificial fertilizers and phytosanitary products.

As the oldest and largest human intervention in nature, the environmental impact of agriculture in general and more recently intensive agriculture, industrial development, and population growth have raised many questions among agricultural scientists and have led to the development and emergence of new fields. These include technological fields that assume the solution to technological problems lies in better technology, such as integrated pest management, waste treatment technologies, landscape architecture, genomics, and agricultural philosophy fields that include references to food production as something essentially different from non-essential economic 'goods'. In fact, the interaction between these two approaches provide a fertile field for deeper understanding in agricultural science.

New technologies, such as biotechnology and computer science (for data processing and storage), and technological advances have made it possible to develop new research fields, including genetic engineering, agrophysics, improved statistical analysis, and precision farming. Balancing these, as above, are the natural and human sciences of agricultural science that seek to understand the human-nature interactions of traditional agriculture, including interaction of religion and agriculture, and the non-material components of agricultural production systems.


Robert Bakewell
Norman Borlaug
Luther Burbank
George Washington Carver
Sir Albert Howard
Kailas Nath Kaul
Justus von Liebig
Jay Lush
Gregor Mendel
Louis Pasteur
M. S. Swaminathan
Jethro Tull
Eli Whitney
Sewall Wright

Agriculture sciences seek to feed the world's population while preventing biosafety problems that may affect human health and the environment. This requires promoting good management of natural resources and respect for the environment, and increasingly concern for the psychological wellbeing of all concerned in the food production and consumption system.

Economic, environmental, and social aspects of agriculture sciences are subjects of ongoing debate. Recent crises (such as avian influenza, mad cow disease and issues such as the use of genetically modified organisms) illustrate the complexity and importance of this debate.

Agricultural biotechnology
Agricultural chemistry
Agricultural diversification
Agricultural economics
Agricultural engineering
Agricultural geography
Agricultural philosophy
Agricultural marketing
Agrophysics
Animal science
Animal breeding
Animal nutrition
Agronomy
Botany
Theoretical production ecology
Horticulture
Plant breeding
Plant fertilization

Aquaculture
Biological engineering
Genetic engineering
Nematology
Microbiology
Plant pathology
Environmental science
Entomology
Food science
Human nutrition
Irrigation and water management
Soil science
Agrology
Waste management
Weed science
Agriculture ministry
Agricultural sciences basic topics
Agroecology
American Society of Agronomy
Genomics of domestication
List of agriculture topics
History of agricultural science
Institute of Food and Agricultural Sciences
International Assessment of Agricultural Science and Technology for Development
International Food Policy Research Institute, IFPRI
Research Institute of Crop Production (RICP) (in the Czech Republic)
University of Agricultural Sciences
National FFA Organization

 Edited by Michelle Adato and Ruth Meinzen-Dick (2007), Johns Hopkins University Press Food Policy Report
Claude Bourguignon, Regenerating the Soil: From Agronomy to Agrology, Other India Press, 2005
Pimentel David, Pimentel Marcia, Computer les kilocalories, CÃ©rÃ¨s, n. 59, sept-oct. 1977
Russell E. Walter, Soil conditions and plant growth, Longman group, London, New York 1973
Salamini Francesco, Oezkan Hakan, Brandolini Andrea, Schaefer-Pregl Ralf, Martin William, Genetics and geography of wild cereal domestication in the Near East, in Nature, vol. 3, ju. 2002
Saltini Antonio,  Agrarian sciences in the west, Firenze, 2015
Vavilov Nicolai I. (Starr Chester K. editor), The Origin, Variation, Immunity and Breeding of Cultivated Plants. Selected Writings, in Chronica botanica, 13: 1-6, Waltham, Mass., 1949â€“50
Vavilov Nicolai I., World Resources of Cereals, Leguminous Seed Crops and Flax, Academy of Sciences of Urss, National Science Foundation, Washington, Israel Program for Scientific Translations, Jerusalem 1960
Winogradsky Serge, Microbiologie du sol. ProblÃ¨mes et methodes. Cinquante ans de recherches, Masson & c.ie, Paris 1949

 - The most comprehensive agricultural library in the world.


Alchemy is an influential tradition whose practitioners have, from antiquity, claimed it to be the precursor to profound powers. As described by Paul-Jacques Malouin in The Encyclopedia of Diederot it is the chemistry of the subtlest kind which allows one to observe extraordinary chemical operations  at a more rapid pace; ones that require a long time for nature to produce. Defining objectives of alchemy are varied but historically have typically included one or more of the following goals: the creation of the fabled philosopher's stone; the ability to transmute base metals into the noble metals (gold or silver); and development of an elixir of life, which would confer youth and longevity.

Though alchemy played a significant role in the development of early modern science, it differs significantly from modern science in its inclusion of Hermetic principles and practices related to mythology, magic, religion, and spirituality. It is recognized as a protoscience that contributed to the development of modern chemistry and medicine. Alchemists developed a structure of basic laboratory techniques, theory, terminology, and experimental method, some of which are still in use today. However, alchemists predated modern foundations of chemistry, such as scientific skepticism, atomic theory, the modern understanding of a chemical element and a chemical substance, the periodic table and conservation of mass and stoichiometry. Instead, they believed in four elements, and cryptic symbolism and mysticism was an integral part of alchemical work.

The ostensible goals of alchemy are often given as the  of common metals into gold (known as chrysopoeia), the creation of a panacea, and the discovery of a universal solvent. However, these only highlight certain aspects of alchemy. Alchemists have historically rewritten and evolved their explanation of alchemy, so it is difficult to define it simply. H.J. Sheppard gives the following as a comprehensive summary:
Modern discussions of alchemy are generally split into an examination of its exoteric practical applications and its esoteric aspects. The former is pursued by historians of the physical sciences who have examined the subject in terms of protochemistry, medicine, and charlatanism. The latter interests psychologists, spiritual and new age communities, hermetic philosophers, and historians of esotericism.  The subject has also made an ongoing impact on literature and the arts. Despite the modern split, numerous sources stress an integration of esoteric and exoteric approaches to alchemy. Holmyard, when writing on exoteric aspects, states that they cannot be properly appreciated if the esoteric is not always kept in mind. The prototype for this model can be found in Bolos of Mendes's 3rd-century BCE work Physika kai Mystika ("On Physical and Mystical Matters"). Marie-Louise von Franz tells us the double approach of Western alchemy was set from the start, when Greek philosophy was mixed with Egyptian and Mesopotamian technology. The technological, operative approach, which she calls extraverted, and the mystic, contemplative, psychological one, which she calls introverted, are not mutually exclusive but complementary since meditation requires practice in the real world and vice versa.

Practical applications of alchemy produced a wide range of contributions to medicine and the physical sciences. The alchemist Robert Boyle is credited as being the father of chemistry. Paracelsian iatrochemistry emphasized the medicinal application of alchemy (continued in plant alchemy, or spagyric). Studies of alchemy also influenced Isaac Newton's theory of gravity.  Academic historical research supports that the alchemists were searching for a material substance using physical methods.

Alchemists made contributions to the "chemical" industries of the dayâ€”ore testing and refining, metalworking, production of gunpowder, ink, dyes, paints, cosmetics, leather tanning, ceramics, glass manufacture, preparation of extracts, liquors, and so on. Alchemists contributed distillation to Western Europe. The attempts of alchemists to arrange information on substances, so as to clarify and anticipate the products of their chemical reactions, resulted in early conceptions of chemical elements and the first rudimentary periodic tables. They learned how to extract metals from ores, and how to compose many types of inorganic acids and bases.

During the 17th century, practical alchemy started to disappear in favor of its younger offshoot chemistry, as it was renamed by Robert Boyle, the "father of modern chemistry". In his book, The Skeptical Chymist, Boyle attacked Paracelsus and the natural philosophy of Aristotle, which was taught at universities. However, Boyle's biographers, in their emphasis that he laid the foundations of modern chemistry, neglect how steadily he clung to the scholastic sciences and to alchemy, in theory, practice and doctrine. The decline of alchemy continued in the 18th century with the birth of modern chemistry, which provided a more precise and reliable framework within a new view of the universe based on rational materialism.

In the eyes of a variety of esoteric and Hermetic practitioners, the heart of alchemy is spiritual. Transmutation of lead into gold is presented as an analogy for personal transmutation, purification, and perfection.  This approach is often termed 'spiritual', 'esoteric', or 'internal' alchemy.

Early alchemists, such as Zosimos of Panopolis (c. AD 300), highlight the spiritual nature of the alchemical quest, symbolic of a religious regeneration of the human soul. This approach continued in the Middle Ages, as metaphysical aspects, substances, physical states, and material processes were used as metaphors for spiritual entities, spiritual states, and, ultimately, transformation. In this sense, the literal meanings of 'Alchemical Formulas' were a blind, hiding their true spiritual philosophy. Practitioners and patrons such as Melchior Cibinensis and Pope Innocent VIII existed within the ranks of the church, while Martin Luther applauded alchemy for its consistency with Christian teachings. Both the transmutation of common metals into gold and the universal panacea symbolized evolution from an imperfect, diseased, corruptible, and ephemeral state toward a perfect, healthy, incorruptible, and everlasting state, so the philosopher's stone then represented a mystic key that would make this evolution possible. Applied to the alchemist himself, the twin goal symbolized his evolution from ignorance to enlightenment, and the stone represented a hidden spiritual truth or power that would lead to that goal. In texts that are written according to this view, the cryptic alchemical symbols, diagrams, and textual imagery of late alchemical works typically contain multiple layers of meanings, allegories, and references to other equally cryptic works; and must be laboriously decoded to discover their true meaning.

In his 1766 Alchemical Catechism, ThÃ©odore Henri de Tschudi denotes that the usage of the metals was a symbol:
During the renaissance, alchemy broke into more distinct schools placing spiritual alchemists in high contrast with those working with literal metals and chemicals. While most spiritual alchemists also incorporate elements of exotericism, examples of a purely spiritual alchemy can be traced back as far as the 16th century, when Jacob Boehme used alchemical terminology in strictly mystical writings.  Another example can be found in the work of Heinrich Khunrath (1560â€“1605) who viewed the process of transmutation as occurring within the alchemist's spirit.

The recent work of L. M. Principe and William R. Newman, seeks to reject the 'spiritual interpretation' of alchemy, especially as applied to medieval, 16th- and 17th-century alchemy, stating it arose as a product of the Victorian occult revival. There is evidence to support that some classical alchemical sources were adulterated during this time to give greater weight to the spiritual aspects of alchemy.  Despite this, other scholars such as Calian and Tilton reject this view as entirely historically inaccurate, drawing examples of historical spiritual alchemy from Boehme, Isaac Newton, and Michael Maier.


The word alchemy was borrowed from Old French alquemie, alkimie, taken from Medieval Latin alchymia, and which is in turn borrowed from Arabic al-kÄ«miyÄâ€™ () â€˜philosopher's stoneâ€™. The Arabic word is borrowed from Late Greek chÄ“meÃ­a (Ï‡Î·Î¼ÎµÎ¯Î±), chÄ“mÃ­a (Ï‡Î·Î¼Î¯Î±) â€˜black magicâ€™ with the agglutination of the Arabic definite article al-  (). This ancient Greek word was derived from the early Greek name for Egypt, ChÄ“mia (Î§Î·Î¼Î¯Î±), based on the Egyptian name for Egypt, kÄ“me (hieroglyphic khmi, lit. â€˜black earthâ€™, as opposed to red desert sand).

The Medieval Latin form was influenced by Greek chymeia (Ï‡Ï…Î¼ÎµÎ¯Î±) meaning â€˜mixtureâ€™ and referring to pharmaceutical chemistry.

Alchemy covers several philosophical traditions spanning some four millennia and three continents. These traditions' general penchant for cryptic and symbolic language makes it hard to trace their mutual influences and "genetic" relationships. One can distinguish at least three major strands, which appear to be largely independent, at least in their earlier stages: Chinese alchemy, centered in China and its zone of cultural influence; Indian alchemy, centered around the Indian subcontinent; and Western alchemy, which occurred around the Mediterranean and whose center has shifted over the millennia from Greco-Roman Egypt, to the Islamic world, and finally medieval Europe. Chinese alchemy was closely connected to Taoism and Indian alchemy with the Dharmic faiths, whereas Western alchemy developed its own philosophical system that was largely independent of, but influenced by, various Western religions. It is still an open question whether these three strands share a common origin, or to what extent they influenced each other.

The start of Western alchemy may generally be traced to Hellenistic Egypt, where the city of Alexandria was a center of alchemical knowledge, and retained its pre-eminence through most of the Greek and Roman periods. Here, elements of technology, religion, mythology, and Hellenistic philosophy, each with their own much longer histories, combined to form the earliest known records of alchemy in the West. Zosimos of Panopolis wrote the oldest known books on alchemy, while Mary the Jewess is credited as being the first non-fictitious Western alchemist. They wrote in Greek and lived in Egypt under Roman rule.

Mythology â€“ Zosimos of Panopolis asserted that alchemy dated back to Pharaonic Egypt where it was the domain of the priestly class, though there is little to no evidence for his assertion. Alchemical writers used Classical figures from Greek, Roman, and Egyptian mythology to illuminate their works and allegorize alchemical transmutation.  These included the pantheon of gods related to the Classical planets, Isis, Osiris, Jason, and many others.

The central figure in the mythology of alchemy is Hermes Trismegistus (or Thrice-Great Hermes). His name is derived from the god Thoth and his Greek counterpart Hermes. Hermes and his caduceus or serpent-staff, were among alchemy's principal symbols. According to Clement of Alexandria, he wrote what were called the "forty-two books of Hermes", covering all fields of knowledge. The Hermetica of Thrice-Great Hermes is generally understood to form the basis for Western alchemical philosophy and practice, called the hermetic philosophy by its early practitioners. These writings were collected in the first centuries of the common era.

Technology â€“ The dawn of Western alchemy is sometimes associated with that of metallurgy, extending back to 3500 BCE. Many writings were lost when the emperor Diocletian ordered the burning of alchemical books after suppressing a revolt in Alexandria (292 CE). Few original Egyptian documents on alchemy have survived, most notable among them the Stockholm papyrus and the Leyden papyrus X. Dating from 300 to 500 CE, they contained recipes for dyeing and making artificial gemstones, cleaning and fabricating pearls, and manufacturing of imitation gold and silver. These writings lack the mystical, philosophical elements of alchemy, but do contain the works of Bolus of Mendes (or Pseudo-Democritus) which aligned these recipes with theoretical knowledge of astrology and the Classical elements. Between the time of Bolus and Zosimos, the change took place that transformed this metallurgy into a Hermetic art.

Philosophy â€“ Alexandria acted as a melting pot for philosophies of Pythagoreanism, Platonism, Stoicism and Gnosticism which formed the origin of alchemy's character. An important example of alchemy's roots in Greek philosophy, originated by Empedocles and developed by Aristotle, was that all things in the universe were formed from only four elements: earth, air, water, and fire. According to Aristotle, each element had a sphere to which it belonged and to which it would return if left undisturbed. The four elements of the Greek were mostly qualitative aspects of matter, not quantitative, as our modern elements are; "...True alchemy never regarded earth, air, water, and fire as corporeal or chemical substances in the present-day sense of the word. The four elements are simply the primary, and most general, qualities by means of which the amorphous and purely quantitative substance of all bodies first reveals itself in differentiated form." The Roman emperor Caligula is said "to have instituted experiments for producing gold out of orpiment (arsenic sulfide)."  Later alchemists extensively developed the mystical aspects of this concept.
Alchemy coexisted alongside emerging Christianity.  Lactantius believed Hermes Trismegistus had prophesied its birth.  Augustine (354â€“430 CE) later affirmed this, but also condemned Trismegistus for idolatry. Examples of Pagan, Christian, and Jewish alchemists can be found during this period.

Most of the Greco-Roman alchemists preceding Zosimos are known only by pseudonyms, such as Moses, Isis, Cleopatra, Democritus, and Ostanes. Others authors such as Komarios, and Chymes, we only know through fragments of text. After 400 CE, Greek alchemical writers occupied themselves solely in commenting on the works of these predecessors. By the middle of the 7th century alchemy was almost an entirely mystical discipline. It was at that time that Khalid Ibn Yazid sparked its migration from Alexandria to the Islamic world, facilitating the translation and preservation of Greek alchemical texts in the 8th and 9th centuries.

After the fall of the Roman Empire, the focus of alchemical development moved to the Islamic World. Much more is known about Islamic alchemy because it was better documented: indeed, most of the earlier writings that have come down through the years were preserved as Arabic translations. The word alchemy itself was derived from the Arabic word al-kÄ«miyÄâ€™ (Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¡). The early Islamic world was a melting pot for alchemy. Platonic and Aristotelian thought, which had already been somewhat appropriated into hermetical science, continued to be assimilated during the late 7th and early 8th centuries through Syriac translations and scholarship.

In the late 8th century, JÄbir ibn HayyÄn (known as "Geber" in Europe) introduced a new approach to alchemy, based on scientific methodology and controlled experimentation in the laboratory, in contrast to the ancient Greek and Egyptian alchemists whose works were often allegorical and unintelligible, with very little concern for laboratory work. Jabir is thus "considered by many to be the father of chemistry", albeit others reserve that title for Robert Boyle or Antoine Lavoisier. The science historian, Paul Kraus, wrote:
Jabir himself clearly recognized and proclaimed the importance of experimentation:
Early Islamic chemists such as Jabir Ibn Hayyan (Ø¬Ø§Ø¨Ø± Ø¨Ù† Ø­ÙŠØ§Ù† in Arabic, Geberus in Latin; usually rendered in English as Geber), Al-Kindi (Alkindus) and Muhammad ibn ZakarÄ«ya RÄzi (Rasis or Rhazes in Latin) contributed a number of key chemical discoveries, such as the muriatic (hydrochloric acid), sulfuric and nitric acids, and more. The discovery that aqua regia, a mixture of nitric and hydrochloric acids, could dissolve the noblest metal, gold, was to fuel the imagination of alchemists for the next millennium.

Islamic philosophers also made great contributions to alchemical hermeticism. The most influential author in this regard was arguably Jabir. Jabir's ultimate goal was Takwin, the artificial creation of life in the alchemical laboratory, up to, and including, human life. He analyzed each Aristotelian element in terms of four basic qualities of hotness, coldness, dryness, and moistness. According to Jabir, in each metal two of these qualities were interior and two were exterior. For example, lead was externally cold and dry, while gold was hot and moist. Thus, Jabir theorized, by rearranging the qualities of one metal, a different metal would result. By this reasoning, the search for the philosopher's stone was introduced to Western alchemy. Jabir developed an elaborate numerology whereby the root letters of a substance's name in Arabic, when treated with various transformations, held correspondences to the element's physical properties.

The elemental system used in medieval alchemy also originated with Jabir. His original system consisted of seven elements, which included the five classical elements (aether, air, earth, fire, and water) in addition to two chemical elements representing the metals: sulphur, "the stone which burns", which characterized the principle of combustibility, and mercury, which contained the idealized principle of metallic properties. Shortly thereafter, this evolved into eight elements, with the Arabic concept of the three metallic principles: sulphur giving flammability or combustion, mercury giving volatility and stability, and salt giving solidity. The atomic theory of corpuscularianism, where all physical bodies possess an inner and outer layer of minute particles or corpuscles, also has its origins in the work of Jabir.

From the 9th to 14th centuries, alchemical theories faced criticism from a variety of practical Muslim chemists, including Alkindus, AbÅ« al-RayhÄn al-BÄ«rÅ«nÄ«, Avicenna and Ibn Khaldun. In particular, they wrote refutations against the idea of the transmutation of metals.

The introduction of alchemy to Latin Europe occurred on 11 February 1144, with the completion of Robert of Chester's translation of the Arabic Book of the Composition of Alchemy. Although European craftsmen and technicians preexisted, Robert notes in his preface that alchemy was unknown in Latin Europe at the time of his writing. The translation of Arabic texts concerning numerous disciplines including alchemy flourished in 12th-century Toledo, Spain, through contributors like Gerard of Cremona and Adelard of Bath. Translations of the time included the Turba Philosophorum, and the works of Avicenna and al-Razi. These brought with them many new words to the European vocabulary for which there was no previous Latin equivalent. Alcohol, carboy, elixir, and athanor are examples.

Meanwhile, theologian contemporaries of the translators made strides towards the reconciliation of faith and experimental rationalism, thereby priming Europe for the influx of alchemical thought. Saint Anselm (1033â€“1109) put forth the opinion that faith and rationalism were compatible and encouraged rationalism in a Christian context.

Peter Abelard (1079â€“1142) followed Anselm's work, laying down the foundation for acceptance of Aristotelian thought before the first works of Aristotle had reached the West. And later, Robert Grosseteste (1170â€“1253) used Abelard's methods of analysis and added the use of observation, experimentation, and conclusions when conducting scientific investigations. Grosseteste also did much work to reconcile Platonic and Aristotelian thinking.

Through much of the 12th and 13th centuries, alchemical knowledge in Europe remained centered around translations, and new Latin contributions were not made. The efforts of the translators were succeeded by that of the encyclopaedists. Albertus Magnus and Roger Bacon are the most notable of these. Their works explained and summarized the newly imported alchemical knowledge in Aristotelian terms. There is little to suggest that Albertus Magnus (1193â€“1280), a Dominican, was himself an alchemist. In his authentic works such as the Book of Minerals, he observed and commented on the operations and theories of alchemical authorities like Hermes and Democritus, and unnamed alchemists of his time. Albertus critically compared these to the writings of Aristotle and Avicenna, where they concerned the transmutation of metals. From the time shortly after his death through to the 15th century, twenty-eight or more alchemical tracts were misattributed to him, a common practice giving rise to his reputation as an accomplished alchemist. Likewise, alchemical texts have been attributed to Albert's student Thomas Aquinas (1225â€“1274).

Roger Bacon (1214â€“1294) was an Oxford Franciscan who studied a wide variety of topics including optics, languages and medicine. After studying the Pseudo-Aristotelian Secretum Secretorum around 1247, he dramatically shifted his studies towards a vision of a universal science which included alchemy and astrology. Bacon maintained that Albertus Magnus' ignorance of the fundamentals of alchemy prevented a complete picture of wisdom. While alchemy was not more important to him than any of the other sciences, and he did not produce symbolic allegorical works, Bacon's contributions advanced alchemy's connections to soteriology and Christian theology. Bacon's writings demonstrated an integration of morality, salvation, alchemy, and the prolongation of life. His correspondence with Pope Clement IV highlighted this integration, calling attention to the importance of alchemy to the papacy. Like the Greeks before him, Bacon acknowledged the division of alchemy into the practical and theoretical. He noted that the theoretical lay outside the scope of Aristotle, the natural philosophers, and all Latin writers of his time. The practical however, confirmed the theoretical through experiment, and Bacon advocated its uses in natural science and medicine.

Soon after Bacon, the influential work of Pseudo-Geber (sometimes identified as Paul of Taranto) appeared. His Summa Perfectionis remained a staple summary of alchemical practice and theory through the medieval and renaissance periods. It was notable for its inclusion of practical chemical operations alongside sulphur-mercury theory, and the unusual clarity with which they were described. By the end of the 13th century, alchemy had developed into a fairly structured system of belief. Adepts believed in the macrocosm-microcosm theories of Hermes, that is to say, they believed that processes that affect minerals and other substances could have an effect on the human body (for example, if one could learn the secret of purifying gold, one could use the technique to purify the human soul). They believed in the four elements and the four qualities as described above, and they had a strong tradition of cloaking their written ideas in a labyrinth of coded jargon set with traps to mislead the uninitiated. Finally, the alchemists practiced their art: they actively experimented with chemicals and made observations and theories about how the universe operated. Their entire philosophy revolved around their belief that man's soul was divided within himself after the fall of Adam. By purifying the two parts of man's soul, man could be reunited with God.

In the 14th century, alchemy became more accessible to Europeans outside the confines of Latin speaking churchmen and scholars. Alchemical discourse shifted from scholarly philosophical debate to an exposed social commentary on the alchemists themselves. Dante, Piers Plowman, and Chaucer all painted unflattering pictures of alchemists as thieves and liars. Pope John XXII's 1317 edict, Spondent quas non exhibent forbade the false promises of transmutation made by pseudo-alchemists. In 1403, Henry IV of England banned the practice of multiplying metals (although it was possible to buy a licence to attempt to make gold alchemically, and a number were granted by Henry VI and Edward IV ). These critiques and regulations centered more around pseudo-alchemical charlatanism than the actual study of alchemy, which continued with an increasingly Christian tone. The 14th century saw the Christian imagery of death and resurrection employed in the alchemical texts of Petrus Bonus, John of Rupescissa and in works written in the name of Raymond Lull and Arnold of Villanova.

Nicolas Flamel is a well known alchemist, but a good example of pseudepigraphy, the practice of giving your works the name of someone else, usually more famous. Though the historical Flamel existed, the writings and legends assigned to him only appeared in 1612.
Flamel was not a religious scholar as were many of his predecessors, and his entire interest in the subject revolved around the pursuit of the philosopher's stone. His work spends a great deal of time describing the processes and reactions, but never actually gives the formula for carrying out the transmutations. Most of 'his' work was aimed at gathering alchemical knowledge that had existed before him, especially as regarded the philosopher's stone.

Through the late Middle Ages (1300â€“1500) alchemists were much like Flamel: they concentrated on looking for the philosophers' stone. Bernard Trevisan and George Ripley made similar contributions in the 14th and 15th centuries. Their cryptic allusions and symbolism led to wide variations in interpretation of the art.

During the Renaissance, Hermetic and Platonic foundations were restored to European alchemy. The dawn of medical, pharmaceutical, occult, and entrepreneurial branches of alchemy followed.

In the late 15th century, Marsilo Ficino translated the Corpus Hermeticum and the works of Plato into Latin. These were previously unavailable to Europeans who for the first time had a full picture of the alchemical theory that Bacon had declared absent. Renaissance Humanism and Renaissance Neoplatonism guided alchemists away from physics to refocus on mankind as the alchemical vessel.

Esoteric systems developed that blended alchemy into a broader occult Hermeticism, fusing it with magic, astrology, and Christian cabala. A key figure in this development was German Heinrich Cornelius Agrippa (1486â€“1535) who received his Hermetic education in Italy in the schools of the humanists. In his De Occulta Philosophia he attempted to merge Kabbalah, Hermetism, and alchemy. He was instrumental in spreading this new blend of Hermeticism outside the borders of Italy.

Philippus Aureolus Paracelsus, (Theophrastus Bombastus von Hohenheim, 1493â€“1541) cast alchemy into a new form, rejecting some of Agrippa's occultism and moving away from chrysopoeia. Paracelsus pioneered the use of chemicals and minerals in medicine and wrote, "Many have said of Alchemy, that it is for the making of gold and silver. For me such is not the aim, but to consider only what virtue and power may lie in medicines."

His hermetical views were that sickness and health in the body relied on the harmony of man the microcosm and Nature the macrocosm. He took an approach different from those before him, using this analogy not in the manner of soul-purification but in the manner that humans must have certain balances of minerals in their bodies, and that certain illnesses of the body had chemical remedies that could cure them. Paracelsian practical alchemy, especially herbal medicine and plant remedies has since been named spagyrics (a synonym for alchemy from the Greek words meaning to separate and to join together, based on the Latin alchemic maxim: solve et coagula). Iatrochemistry also refers to the pharmaceutical applications of alchemy championed by Paracelsus.

John Dee (13 July 1527 â€“ December, 1608) followed Agrippa's occult tradition. Though better known for angel summoning, divination, and his role as astrologer, cryptographer, and consultant to Queen Elizabeth I, Dee's alchemical Monas Hieroglyphica, written in 1564 was his most popular and influential work. His writing portrayed alchemy as a sort of terrestrial astronomy in line with the Hermetic axiom As above so below. During the 17th century, a short-lived "supernatural" interpretation of alchemy became popular, including support by fellows of the Royal Society: Robert Boyle and Elias Ashmole. Proponents of the supernatural interpretation of alchemy believed that the philosopher's stone might be used to summon and communicate with angels.
Entrepreneurial opportunities were not uncommon for the alchemists of Renaissance Europe. Alchemists were contracted by the elite for practical purposes related to mining, medical services, and the production of chemicals, medicines, metals, and gemstones. Rudolf II, Holy Roman Emperor, in the late 16th century, famously received and sponsored various alchemists at his court in Prague, including Dee and his associate Edward Kelley. King James IV of Scotland, Julius, Duke of Brunswick-LÃ¼neburg, Henry V, Duke of Brunswick-LÃ¼neburg, Augustus, Elector of Saxony, Julius Echter von Mespelbrunn, and Maurice, Landgrave of Hesse-Kassel all contracted alchemists. John's son Arthur Dee worked as a court physician to Michael I of Russia and Charles I of England but also compiled the alchemical book Fasciculus Chemicus.

Though most of these appointments were legitimate, the trend of pseudo-alchemical fraud continued through the Renaissance. BetrÃ¼ger would use sleight of hand, or claims of secret knowledge to make money or secure patronage. Legitimate mystical and medical alchemists such as Michael Maier and Heinrich Khunrath wrote about fraudulent transmutations, distinguishing themselves from the con artists.  False alchemists were sometimes prosecuted for fraud.

The terms "chemia" and "alchemia" were used as synonyms in the early modern period, and the differences between alchemy, chemistry and small-scale assaying and metallurgy were not as neat as in the present day. There were important overlaps between practitioners, and trying to classify them into alchemists, chemists and craftsmen is anachronistic. For example, Tycho Brahe (1546â€“1601), an alchemist better known for his astronomical and astrological investigations, had a laboratory built at his Uraniborg observatory/research institute. Michael Sendivogius (MichaÅ‚ SÄ™dziwÃ³j, 1566â€“1636), a Polish alchemist, philosopher, medical doctor and pioneer of chemistry wrote mystical works but is also credited with distilling oxygen in a lab sometime around 1600. Sendivogious taught his technique to Cornelius Drebbel who, in 1621, applied this in a submarine. Isaac Newton devoted considerably more of his writing to the study of alchemy (see Isaac Newton's occult studies) than he did to either optics or physics. Other early modern alchemists who were eminent in their other studies include Robert Boyle, and Jan Baptist van Helmont. Their Hermetism complemented rather than precluded their practical achievements in medicine and science.


The decline of European alchemy was brought about by the rise of modern science with its emphasis on rigorous quantitative experimentation and its disdain for "ancient wisdom". Although the seeds of these events were planted as early as the 17th century, alchemy still flourished for some two hundred years, and in fact may have reached its apogee in the 18th century. As late as 1781 James Price claimed to have produced a powder that could transmute mercury into silver or gold. Early modern European alchemy continued to exhibit a diversity of theories, practices, and purposes: "Scholastic and anti-Aristotelian, Paracelsian and anti-Paracelsian, Hermetic, Neoplatonic, mechanistic, vitalistic, and moreâ€”plus virtually every combination and compromise thereof."

Robert Boyle (1627â€“1691) pioneered the scientific method in chemical investigations. He assumed nothing in his experiments and compiled every piece of relevant data. Boyle would note the place in which the experiment was carried out, the wind characteristics, the position of the Sun and Moon, and the barometer reading, all just in case they proved to be relevant. This approach eventually led to the founding of modern chemistry in the 18th and 19th centuries, based on revolutionary discoveries of Lavoisier and John Dalton.

Beginning around 1720, a rigid distinction was drawn between "alchemy" and "chemistry" for the first time. By the 1740s, "alchemy" was now restricted to the realm of gold making, leading to the popular belief that alchemists were charlatans, and the tradition itself nothing more than a fraud. In order to protect the developing science of modern chemistry from the negative censure of which alchemy was being subjected, academic writers during the scientific Enlightenment attempted, for the sake of survival, to separate and divorce the "new" chemistry from the "old" practices of alchemy. This move was mostly successful, and the consequences of this continued into the 19th and 20th centuries, and even to the present day.

During the occult revival of the early 19th century, alchemy received new attention as an occult science. The esoteric or occultist school, which arose during the 19th century, held (and continues to hold) the view that the substances and operations mentioned in alchemical literature are to be interpreted in a spiritual sense, and it downplays the role of the alchemy as a practical tradition or protoscience. This interpretation further forwarded the view that alchemy is an art primarily concerned with spiritual enlightenment or illumination, as opposed to the physical manipulation of apparatus and chemicals, and claims that the obscure language of the alchemical texts were an allegorical guise for spiritual, moral or mystical processes.

In the 19th-century revival of alchemy, the two most seminal figures were Mary Anne Atwood and Ethan Allen Hitchcock, who independently published similar works regarding spiritual alchemy. Both forwarded a completely esoteric view of alchemy, as Atwood claimed: "No modern art or chemistry, notwithstanding all its surreptitious claims, has any thing in common with Alchemy." Atwood's work influenced subsequent authors of the occult revival including Eliphas Levi, Arthur Edward Waite, and Rudolf Steiner. Hitchcock, in his Remarks Upon Alchymists (1855) attempted to make a case for his spiritual interpretation with his claim that the alchemists wrote about a spiritual discipline under a materialistic guise in order to avoid accusations of blasphemy from the church and state. In 1845, Baron Carl Reichenbach, published his studies on Odic force, a concept with some similarities to alchemy, but his research did not enter the mainstream of scientific discussion.

According to the EncyclopÃ¦dia Britannica, the Vedas describe a connection between eternal life and gold. The use of Mercury for alchemy is first documented in the 3rd - 4th century CE Artha-Å›Ästra. Buddhist texts from the 2nd to 5th centuries CE mention the transmutation of base metals to gold. Greek alchemy may have been introduced to Ancient India through the invasions of Alexander the Great in 325 BCE, and kingdoms that were culturally influenced by the Greeks like GandhÄra, although hard evidence for this is lacking.

The 11th-century Persian chemist and physician AbÅ« RayhÄn BÄ«rÅ«nÄ«, who visited Gujarat as part of the court of Mahmud of Ghazni, reported that they 

The goals of alchemy in India included the creation of a divine body (Sanskrit divya-deham) and immortality while still embodied (Sanskrit jÄ«van-mukti).  Sanskrit alchemical texts include much material on the manipulation of mercury and sulphur, that are homologized with the semen of the god Åšiva and the menstrual blood of the goddess DevÄ«.

Some early alchemical writings seem to have their origins in the Kaula tantric schools associated to the teachings of the personality of Matsyendranath.  Other early writings are found in the Jaina medical treatise KalyÄá¹‡akÄrakam of UgrÄditya, written in South India in the early 9th century.

Two famous early Indian alchemical authors were NÄgÄrjuna Siddha and NityanÄtha Siddha. NÄgÄrjuna Siddha was a Buddhist monk. His book, Rasendramangalam, is an example of Indian alchemy and medicine. NityanÄtha Siddha wrote RasaratnÄkara, also a highly influential work. In Sanskrit, rasa translates to "mercury", and NÄgÄrjuna Siddha was said to have developed a method of converting mercury into gold.

Reliable scholarship on Indian alchemy has been advanced in a major way by the publication of The Alchemical Body by David Gordon White.  Trustworthy scholarship on Indian alchemy must now take the findings of this work into account.

An important modern bibliography on Indian alchemical studies has also been provided by David Gordon White at .


The contents of the following thirty-nine Sanskrit alchemical treatises have been analysed in detail in G. Jan Meulenbeld's History of Indian Medical Literature.:
The discussion of these works in HIML gives a summary of the contents of each work, their special features, and where possible the evidence concerning their dating. Chapter 13 of HIML, Various works on rasaÅ›Ästra and ratnaÅ›Ästra (or Various works on alchemy and gems) gives brief details of a further 655 (six hundred and fifty-five) treatises.  In some cases Meulenbeld gives notes on the contents and authorship of these works; in other cases references are made only to the unpublished manuscripts of these titles.

A great deal remains to be discovered about Indian alchemical literature.  The content of the Sanskrit alchemical corpus has not yet (2014) been adequately integrated into the wider general history of alchemy.

Whereas European alchemy eventually centered on the transmutation of base metals into noble ones, Chinese alchemy had a more obvious connection to medicine. The philosopher's stone of European alchemists can be compared to the Grand Elixir of Immortality sought by Chinese alchemists. However, in the hermetic view, these two goals were not unconnected, and the philosopher's stone was often equated with the universal panacea; therefore, the two traditions may have had more in common than initially appears.

Black powder may have been an important invention of Chinese alchemists. As previously stated above, Chinese alchemy was more related to medicine. It is said that the Chinese invented gunpowder while trying to find a potion for eternal life. Described in 9th-century texts and used in fireworks in China by the 10th century, it was used in cannons by 1290. From China, the use of gunpowder spread to Japan, the Mongols, the Muslim world, and Europe. Gunpowder was used by the Mongols against the Hungarians in 1241, and in Europe by the 14th century.

Chinese alchemy was closely connected to Taoist forms of traditional Chinese medicine, such as Acupuncture and Moxibustion, and to martial arts such as Tai Chi Chuan and Kung Fu (although some Tai Chi schools believe that their art derives from the philosophical or hygienic branches of Taoism, not Alchemical). In fact, in the early Song Dynasty, followers of this Taoist idea (chiefly the elite and upper class) would ingest mercuric sulfide, which, though tolerable in low levels, led many to suicide. Thinking that this consequential death would lead to freedom and access to the Taoist heavens, the ensuing deaths encouraged people to eschew this method of alchemy in favor of external sources (the aforementioned Tai Chi Chuan, mastering of the qi, etc.).

The history of alchemy has become a significant and recognized subject of academic study.   As the language of the alchemists is analyzed, historians are becoming more aware of the intellectual connections between that discipline and other facets of Western cultural history, such as the evolution of science and philosophy, the sociology and psychology of the intellectual communities, kabbalism, spiritualism, Rosicrucianism, and other mystic movements. Institutions involved in this research include The Chymistry of Isaac Newton project at Indiana University, the University of Exeter Centre for the Study of Esotericism (EXESESO), the European Society for the Study of Western Esotericism (ESSWE), and the University of Amsterdam's Sub-department for the History of Hermetic Philosophy and Related Currents. A large collection of books on alchemy is kept in the Bibliotheca Philosophica Hermetica in Amsterdam. 
Journals which publish regularly on the topic of Alchemy include 'Ambix', published by the Society for the History of alchemy and Chemistry, and 'Isis', published by The History of Science Society.

Due to the complexity and obscurity of alchemical literature, and the 18th-century disappearance of remaining alchemical practitioners into the area of chemistry; the general understanding of alchemy has been strongly influenced by several distinct and radically different interpretations. Those focusing on the exoteric, such as historians of science Lawrence M. Principe and William R. Newman, have interpreted the 'decknamen' (or code words) of alchemy as physical substances. These practitioners have reconstructed physicochemical experiments that they say are described in medieval and early modern texts.

At the opposite end of the spectrum, esoteric alchemists interpret these same decknamen as spiritual, religious, or psychological concepts. Today new interpretations of alchemy are still perpetuated, sometimes merging in concepts from New Age or radical environmentalism movements. Groups like the rosicrucians and freemasons have a continued interest in alchemy and its symbolism. Since the Victorian revival of alchemy, "occultists reinterpreted alchemy as a spiritual practice, involving the self-transformation of the practitioner and only incidentally or not at all the transformation of laboratory substances.", which has contributed to a merger of magic and alchemy in popular thought.

Traditional medicine sometimes involves the transmutation of natural substances, using pharmacological or a combination of pharmacological and spiritual techniques. In Ayurveda the samskaras are claimed to transform heavy metals and toxic herbs in a way that removes their toxicity. These processes are actively used to the present day.
 
Spagyrists of the 20th century, Albert Richard Riedel and Jean Dubuis, merged Paracelsian alchemy with occultism, teaching laboratory pharmaceutical methods. The schools they founded, Les Philosophes de la Nature and The Paracelsus Research Society, popularized modern spagyrics including the manufacture of herbal tinctures and products. The courses, books, organizations, and conferences generated by their students continue to influence popular applications of alchemy as a new age medicinal practice.

Alchemical symbolism has been used by psychologists such as Carl Jung who reexamined alchemical symbolism and theory and presented the inner meaning of alchemical work as a spiritual path. Jung was deeply interested in the occult since his youth, participating in seances, which he used as the basis for his doctoral dissertation "On the Psychology and Pathology of So-Called Occult Phenomena." In 1913, Jung had already adopted a "spiritualist and redemptive interpretation of alchemy", likely reflecting his interest in the occult literature of the 19th century. Jung began writing his views on alchemy from the 1920s and continued until the end of his life. His interpretation of Chinese alchemical texts in terms of his analytical psychology also served the function of comparing Eastern and Western alchemical imagery and core concepts and hence its possible inner sources (archetypes).

Jung saw alchemy as a Western proto-psychology dedicated to the achievement of individuation. In his interpretation, alchemy was the vessel by which Gnosticism survived its various purges into the Renaissance, a concept also followed by others such as Stephan A. Hoeller. In this sense, Jung viewed alchemy as comparable to a Yoga of the East, and more adequate to the Western mind than Eastern religions and philosophies. The practice of Alchemy seemed to change the mind and spirit of the Alchemist. Conversely, spontaneous changes on the mind of Western people undergoing any important stage in individuation seems to produce, on occasion, imagery known to Alchemy and relevant to the person's situation. Jung did not completely reject the material experiments of the alchemists, but he massively downplayed it, writing that the transmutation was performed in the mind of the alchemist. He claimed the material substances and procedures were only a projection of the alchemists' internal state, while the real substance to be transformed was the mind itself.

Marie-Louise von Franz, a disciple of Jung, continued Jung's studies on alchemy and its psychological meaning. Jung's work exercised a great influence on the mainstream perception of alchemy, his approach becoming a stock element in many popular texts on the subject to this day. Modern scholars are sometimes critical of the Jungian approach to alchemy as overly reflective of 19th-century occultism.


The Great Work of Alchemy is often described as a series of four stages represented by colors.
nigredo, a blackening or melanosis
albedo, a whitening or leucosis
citrinitas, a yellowing or xanthosis
rubedo, a reddening, purpling, or iosis


Alchemy has had a long-standing relationship with art, seen both in alchemical texts and in mainstream entertainment. Literary alchemy appears throughout the history of English literature from Shakespeare to J. K. Rowling. Here, characters or plot structure follow an alchemical magnum opus. In the 14th century, Chaucer began a trend of alchemical satire that can still be seen in recent fantasy works like those of Terry Pratchett.

Visual artists had a similar relationship with alchemy. While some of them used alchemy as a source of satire, others worked with the alchemists themselves or integrated alchemical thought or symbols in their work. Music was also present in the works of alchemists and continues to influence popular performers. In the last hundred years, alchemists have been portrayed in a magical and spagyric role in fantasy fiction, film, television, novels, comics and video games.

  Adam McLean's online collections and academic discussion.
 Alchemy
  A digital exhibition from the 


Alien may refer to:
Extraterrestrial life, life which does not originate from Earth
Alien (law), a non-citizen inhabitant of a country

Introduced species, a species not native to its environment
Alien (software), a Linux program
AliEn (ALICE Environment), a grid framework
Alien Technology, a manufacturer of RFID technology
Aliens, a newsletter of the IUCN Invasive Species Specialist Group


Alien (film), a 1979 film by Ridley Scott
Aliens (film), the 1986 sequel by James Cameron
Alien (franchise), the film franchise, including other sequels
Alien (creature in Alien franchise)
Alien (soundtrack)
Aliens (soundtrack)
Aliens (novel series)
Aliens (comic book)
The Alien (film), an incomplete 1960s Indian-American film

Alien (band), a 1980s Swedish rock group
The Aliens (Australian band), a 1970s new wave group
The Aliens (Scottish band), a 2005â€“present rock group
Alien (Strapping Young Lad album)
"Alien" (Britney Spears song)
"Alien" (Pennywise song)
"Alien" (Third Day song)
"Alien", a song by Bush on the album Sixteen Stone
"Alien", a song by Erasure on the album Loveboat
"Alien", a song by Japan on the album Quiet Life
"Alien", a song by Lamb on the album Fear of Fours
"Alien", a song by Nerina Pallot on the album Dear Frustrated Superstar
"Alien", a song by P-Model on the album Landsale
"Alien", a song by Thriving Ivory on their self-titled album
"Alien", a song by Tokio Hotel on the album Humanoid
"The Aliens", a song by Warlord

Aliens (Tappan Wright novel), a 1902 novel by Mary Tappan Wright
The Alien (Animorphs), the eighth book in the Animorphs series
Aliens (Kaypro video game), a text-only clone of Space Invaders written for the CP/M operating system
Aliens (novel series), an extension of the Alien franchise

Alien (shipping company), a Russian company
Alien (literary concept)
Alien Sun (born 1974), Singaporean actress
Alien, a perfume by Thierry Mugler

Astrobiology
List of Alien and Predator games
"My Alien", a song by Simple Plan on the album No Pads, No Helmets... Just Balls
Alians, an Islamic order
ATLiens, a 1996 album by OutKast


An astronomer is a scientist who studies celestial bodies such as black holes, moons, planets, stars, asteroids, comets, nebulae and galaxies, as well as Gamma-ray bursts and cosmic microwave background radiation. A related but distinct subject, cosmology, is concerned with studying the universe as a whole. An astronomer researches the world beyond earth.


Historically, astronomy was more concerned with the classification and description of phenomena in the sky, while astrophysics attempted to explain these phenomena and the differences between them using physical laws. Today, that distinction has mostly disappeared and the terms "astronomer" and "astrophysicist" are interchangeable. Professional astronomers are highly educated individuals who typically have a PhD in physics or astronomy and are employed by research institutions or universities. They spend the majority of their time working on research, although they quite often have other duties such as teaching, building instruments, or aiding in the operation of an observatory.

The number of professional astronomers in the United States is actually quite small. The American Astronomical Society, which is the major organization of professional astronomers in North America, has approximately 7,700 members. This number includes scientists from other fields such as physics, geology, and engineering, whose research interests are closely related to astronomy. The International Astronomical Union comprises almost 10,145 members from 70 different countries who are involved in astronomical research at the PhD level and beyond.


Contrary to the classical image of an old astronomer peering through a telescope through the dark hours of the night, it is far more common to use a charge-coupled device camera to record a long, deep exposure, allowing a more sensitive image to be created because the light is added over time. Before CCDs, photographic plates were a common method of observation. Modern astronomers spend relatively little time at telescopes usually just a few weeks per year. Analysis of observed phenomena, along with making predictions as to the causes of what they observe, takes the majority of observational astronomers' time.

Astronomers who serve as faculty spend much of their time teaching undergraduate and graduate classes. Most universities also have outreach programs including public telescope time and sometimes planetariums as a public service to encourage interest in the field.

While there is a relatively low number of professional astronomers, the field is popular among amateurs. Most cities have amateur astronomy clubs that meet on a regular basis and often host star parties. The Astronomical Society of the Pacific is the largest general astronomical society in the world, comprising both professional and amateur astronomers as well as educators from 70 different nations. Like any hobby, most people who think of themselves as amateur astronomers may devote a few hours a month to stargazing and reading the latest developments in research. However, amateurs span the range from so-called "armchair astronomers" to the very ambitious, who own science-grade telescopes and instruments with which they are able to make their own discoveries and assist professional astronomers in research.

Cosmologists
List of astronomers
List of Muslim astronomers
List of Russian astronomers and astrophysicists



ASCII ( ), abbreviated from American Standard Code for Information Interchange, is a character-encoding scheme. Originally based on the English alphabet, it encodes 128 specified characters into 7-bit binary integers as shown by the ASCII chart on the right.  The characters encoded are numbers 0 to 9, lowercase letters a to z, uppercase letters A to Z, basic punctuation symbols, control codes that originated with Teletype machines, and a space. For example, lowercase j would become binary 1101010 and decimal 106.

ASCII codes represent text in computers, communications equipment, and other devices that use text. Most modern character-encoding schemes are based on ASCII, though they support many additional characters.

ASCII developed from telegraphic codes. Its first commercial use was as a 7-bit teleprinter code promoted by Bell data services. Work on the ASCII standard began on October 6, 1960, with the first meeting of the American Standards Association's (ASA) X3.2 subcommittee. The first edition of the standard was published during 1963, a major revision during 1967, and the most recent update during 1986. Compared to earlier telegraph codes, the proposed Bell code and ASCII were both ordered for more convenient sorting (i.e., alphabetization) of lists, and added features for devices other than teleprinters.

ASCII includes definitions for 128 characters: 33 are non-printing control characters (many now obsolete) that affect how text and space are processed and 95 printable characters, including the space (which is considered an invisible graphic).

The IANA prefers the name US-ASCII. ASCII was the most common character encoding on the World Wide Web until December 2007, when it was surpassed by UTF-8, which includes ASCII as a subset.

The American Standard Code for Information Interchange (ASCII) was developed under the auspices of a committee of the American Standards Association, called the X3 committee, by its X3.2 (later X3L2) subcommittee, and later by that subcommittee's X3.2.4 working group. The ASA became the United States of America Standards Institute or USASI and ultimately the American National Standards Institute.

The X3.2 subcommittee designed ASCII based on the earlier teleprinter encoding systems. Like other character encodings, ASCII specifies a correspondence between digital bit patterns and character symbols (i.e. graphemes and control characters). This allows digital devices to communicate with each other and to process, store, and communicate character-oriented information such as written language. Before ASCII was developed, the encodings in use included 26 alphabetic characters, 10 numerical digits, and from 11 to 25 special graphic symbols. To include all these, and control characters compatible with the ComitÃ© Consultatif International TÃ©lÃ©phonique et TÃ©lÃ©graphique (CCITT) International Telegraph Alphabet No. 2 (ITA2) standard, Fieldata, and early EBCDIC, more than 64 codes were required for ASCII.

The committee debated the possibility of a shift function (like in ITA2), which would allow more than 64 codes to be represented by a six-bit code. In a shifted code, some character codes determine choices between options for the following character codes. It allows compact encoding, but is less reliable for data transmission: an error in transmitting the shift code typically makes a long part of the transmission unreadable. The standards committee decided against shifting, and so ASCII required at least a seven-bit code.

The committee considered an eight-bit code, since eight bits (octets) would allow two four-bit patterns to efficiently encode two digits with binary-coded decimal. However, it would require all data transmission to send eight bits when seven could suffice. The committee voted to use a seven-bit code to minimize costs associated with data transmission. Since perforated tape at the time could record eight bits in one position, it also allowed for a parity bit for error checking if desired. Eight-bit machines (with octets as the native data type) that did not use parity checking typically set the eighth bit to 0.

The code itself was patterned so that most control codes were together, and all graphic codes were together, for ease of identification. The first two columns (32 positions) were reserved for control characters. The space character had to come before graphics to make sorting easier, so it became position 20hex; for the same reason, many special signs commonly used as separators were placed before digits. The committee decided it was important to support uppercase 64-character alphabets, and chose to pattern ASCII so it could be reduced easily to a usable 64-character set of graphic codes, as was done in the DEC SIXBIT code. Lowercase letters were therefore not interleaved with uppercase. To keep options available for lowercase letters and other graphics, the special and numeric codes were arranged before the letters, and the letter A was placed in position 41hex to match the draft of the corresponding British standard. The digits 0â€“9 were arranged so they correspond to values in binary prefixed with 011, making conversion with binary-coded decimal straightforward.

Many of the non-alphanumeric characters were positioned to correspond to their shifted position on typewriters. An important subtlety is that these were based on mechanical typewriters, not electric typewriters. Mechanical typewriters followed the standard set by the Remington No. 2 (1878), the first typewriter with a shift key, and the shifted values of 23456789- were "#$%_&'() early typewriters omitted 0 and 1, using O (capital letter o) and l (lowercase letter L) instead, but 1! and 0) pairs became standard once 0 and 1 became common. Thus, in ASCII !"#$% were placed in second column, rows 1â€“5, corresponding to the digits 1â€“5 in the adjacent column. The parentheses could not correspond to 9 and 0, however, because the place corresponding to 0 was taken by the space character. This was accommodated by removing _ (underscore) from 6 and shifting the remaining characters left: this corresponded to many European typewriters, which placed the parentheses with 8 and 9. This discrepancy from typewriters led to bit-paired keyboards, notably the Teletype Model 33, which used the left-shifted layout corresponding to ASCII, not to traditional mechanical typewriters. Electric typewriters, notably the more recently introduced IBM Selectric (1961), used a somewhat different layout that has become standard on computersfollowing the IBM PC (1981), especially Model M (1984)and thus shift values for symbols on modern keyboards do not correspond as closely to the ASCII table as earlier keyboards did. The /? pair also dates to the No. 2, and the ,< .> pairs were used on some keyboards (others, including the No. 2, did not shift , (comma) or . (full stop) so they could be used in uppercase without unshifting). However, ASCII split the ;: pair (dating to No. 2), and rearranged mathematical symbols (varied conventions, commonly -* =+) to :* ;+ -=.

Some common characters were not included, notably Â½Â¼Â¢, while ^`~ were included as diacritics for international use, and <> for mathematical use, together with the simple line characters \| (in addition to common /). The @ symbol was not used in continental Europe and the committee expected it would be replaced by an accented Ã€ in the French variation, so the @ was placed in position 40hex, right before the letter A.

The control codes felt essential for data transmission were the start of message (SOM), end of address (EOA), end of message (EOM), end of transmission (EOT), "who are you?" (WRU), "are you?" (RU), a reserved device control (DC0), synchronous idle (SYNC), and acknowledge (ACK). These were positioned to maximize the Hamming distance between their bit patterns.

With the other special characters and control codes filled in, ASCII was published as ASA X3.4-1963, leaving 28 code positions without any assigned meaning, reserved for future standardization, and one unassigned control code. There was some debate at the time whether there should be more control characters rather than the lowercase alphabet. The indecision did not last long: during May 1963 the CCITT Working Party on the New Telegraph Alphabet proposed to assign lowercase characters to columns 6 and 7, and International Organization for Standardization TC 97 SC 2 voted during October to incorporate the change into its draft standard. The X3.2.4 task group voted its approval for the change to ASCII at its May 1963 meeting.  Locating the lowercase letters in columns 6 and 7 caused the characters to differ in bit pattern from the upper case by a single bit, which simplified case-insensitive character matching and the construction of keyboards and printers.

The X3 committee made other changes, including other new characters (the brace and vertical bar characters), renaming some control characters (SOM became start of header (SOH)) and moving or removing others (RU was removed). ASCII was subsequently updated as USASI X3.4-1967, then USASI X3.4-1968, ANSI X3.4-1977, and finally, ANSI X3.4-1986 (the first two are occasionally retronamed ANSI X3.4-1967, and ANSI X3.4-1968).

The X3 committee also addressed how ASCII should be transmitted (least significant bit first), and how it should be recorded on perforated tape. They proposed a 9-track standard for magnetic tape, and attempted to deal with some forms of punched card formats.

ASCII itself was first used commercially during 1963 as a seven-bit teleprinter code for American Telephone & Telegraph's TWX (TeletypeWriter eXchange) network. TWX originally used the earlier five-bit ITA2, which was also used by the competing Telex teleprinter system. Bob Bemer introduced features such as the escape sequence. His British colleague Hugh McGregor Ross helped to popularize this work â€“ according to Bemer, "so much so that the code that was to become ASCII was first called the Bemer-Ross Code in Europe". Because of his extensive work on ASCII, Bemer has been called "the father of ASCII."
On March 11, 1968, U.S. President Lyndon B. Johnson mandated that all computers purchased by the United States federal government support ASCII, stating:

I have also approved recommendations of the Secretary of Commerce regarding standards for recording the Standard Code for Information Interchange on magnetic tapes and paper tapes when they are used in computer operations.
All computers and related equipment configurations brought into the Federal Government inventory on and after July 1, 1969, must have the capability to use the Standard Code for Information Interchange and the formats prescribed by the magnetic tape and paper tape standards when these media are used.

Other international standards bodies have ratified character encodings such as ISO/IEC 646 that are identical or nearly identical to ASCII, with extensions for characters outside the English alphabet and symbols used outside the United States, such as the symbol for the United Kingdom's pound sterling (Â£). Almost every country needed an adapted version of ASCII, since ASCII suited the needs of only the USA and a few other countries. For example, Canada had its own version that supported French characters. Other adapted encodings include ISCII (India), VISCII (Vietnam), and YUSCII (Yugoslavia). Although these encodings are sometimes referred to as ASCII, true ASCII is defined strictly only by the ANSI standard.

ASCII was incorporated into the Unicode character set as the first 128 symbols, so the 7-bit ASCII characters have the same numeric codes in both sets. This allows UTF-8 to be backward compatible with 7-bit ASCII, as a UTF-8 file containing only ASCII characters is identical to an ASCII file containing the same sequence of characters.  Even more importantly, forward compatibility is ensured as software that recognizes only 7-bit ASCII characters as special and does not alter bytes with the highest bit set (as is often done to support 8-bit ASCII extensions such as ISO-8859-1) will preserve UTF-8 data unchanged.

ASCII reserves the first 32 codes (numbers 0â€“31 decimal) for control characters: codes originally intended not to represent printable information, but rather to control devices (such as printers) that make use of ASCII, or to provide meta-information about data streams such as those stored on magnetic tape.

For example, character 10 represents the line feed function (which causes a printer to advance its paper), and character 8 represents backspace. RFC 2822 refers to control characters that do not include carriage return, line feed or white space as non-whitespace control characters. Except for the control characters that prescribe elementary line-oriented formatting, ASCII does not define any mechanism for describing the structure or appearance of text within a document. Other schemes, such as markup languages, address page and document layout and formatting.

The original ASCII standard used only short descriptive phrases for each control character. The ambiguity this caused was sometimes intentional, for example where a character would be used slightly differently on a terminal link than on a data stream, and sometimes accidental, for example with the meaning of "delete".

Probably the most influential single device on the interpretation of these characters was the Teletype Model 33 ASR, which was a printing terminal with an available paper tape reader/punch option. Paper tape was a very popular medium for long-term program storage until the 1980s, less costly and in some ways less fragile than magnetic tape. In particular, the Teletype Model 33 machine assignments for codes 17 (Control-Q, DC1, also known as XON), 19 (Control-S, DC3, also known as XOFF), and 127 (Delete) became de facto standards. The Model 33 was also notable for taking the description of Control-G (BEL, meaning audibly alert the operator) literally: the unit contained an actual bell which it rang when it received a BEL character.  Because the keytop for the O key also showed a left-arrow symbol (from ASCII-1963, which had this character instead of underscore), a noncompliant use of code 15 (Control-O, Shift In) interpreted as "delete previous character" was also adopted by many early timesharing systems but eventually became neglected.

When a Teletype 33 ASR equipped with the automatic paper tape reader received a Control-S (XOFF, an abbreviation for transmit off), it caused the tape reader to stop; receiving Control-Q (XON, "transmit on") caused the tape reader to resume.  This technique became adopted by several early computer operating systems as a "handshaking" signal warning a sender to stop transmission because of impending overflow; it persists to this day in many systems as a manual output control technique. On some systems Control-S retains its meaning but Control-Q is replaced by a second Control-S to resume output.  The 33 ASR also could be configured to employ Control-R (DC2) and Control-T (DC4) to start and stop the tape punch; on some units equipped with this function, the corresponding control character lettering on the keycap above the letter was TAPE and TAPE respectively.

Code 127 is officially named "delete" but the Teletype label was "rubout". Since the original standard did not give detailed interpretation for most control codes, interpretations of this code varied. The original Teletype meaning, and the intent of the standard, was to make it an ignored character, the same as NUL (all zeroes). This was useful specifically for paper tape, because punching the all-ones bit pattern on top of an existing mark would obliterate it. Tapes designed to be "hand edited" could even be produced with spaces of extra NULs (blank tape) so that a block of characters could be "rubbed out" and then replacements put into the empty space.

Some software assigned special meanings to ASCII characters sent to the software from the terminal. Operating systems from Digital Equipment Corporation, for example, interpreted DEL as an input character as meaning "remove previously-typed input character", and this interpretation also became common in Unix systems. Most other systems used BS for that meaning and used DEL to mean "remove the character at the cursor". That latter interpretation is the most common now.

Many more of the control codes have been given meanings quite different from their original ones. The escape character (ESC, code 27), for example, was intended originally to allow sending other control characters as literals instead of invoking their meaning. This is the same meaning of "escape" encountered in URL encodings, C language strings, and other systems where certain characters have a reserved meaning. Over time this meaning has been co-opted and has eventually been changed. In modern use, an ESC sent to the terminal usually indicates the start of a command sequence, usually in the form of a so-called "ANSI escape code" (or, more properly, a "Control Sequence Introducer") beginning with ESC followed by a "

Austin is the capital of Texas in the United States.

Austin may also refer to:

Austin, Western Australia

Austin, Manitoba
Austin, Ontario
Austin, Quebec
Austin Island, Nunavut

Saint-Austin, hamlet at la Neuville-Chant-d'Oisel, Normandy

Austin, Arkansas
Austin, Colorado
Austin, Illinois:
Austin Township, Macon County, Illinois
Austin, Chicago, Cook County, Illinois
Austin, Indiana
Austin, Kentucky
Austin, Minnesota
Austin, Nevada
Austin, Oregon
Austin County, Texas (note that the city of Austin is located in Travis County)

Austin (name)

Austin College, Sherman, Texas
University of Texas at Austin, flagship institution of the University of Texas System
Austin Peay State University, Clarksville, Tennessee

Augustine of Hippo or Augustine of Canterbury
An adjective for the Augustinians

Austin Automobile Company, short-lived American automobile company
Austin (brand), a brand owned by the Kellogg Company
Austin Motor Company, British car manufacturer
American Austin Car Company, short-lived American automobile maker

"Austin" (song), a single by Blake Shelton
Austin, a kangaroo Beanie Baby produced by Ty, Inc.
Austin the kangaroo from the children's television series The Backyardigans

USS Austin, three ships
Austin Station (disambiguation), various public transportation stations

Austen (disambiguation)
Augustine (disambiguation)


The bouncing ball animation (below) consists of these six frames.

This animation moves at 10 frames per second.
Animation is the process of creating motion and shape change illusion by means of the rapid display of a sequence of static images that minimally differ from each other. The illusionas in motion pictures in generalis thought to rely on the phi phenomenon. Animators are artists who specialize in the creation of animation.

Animations can be recorded on either analogue media, such as a flip book, motion picture film, video tape, or on digital media, including formats such as animated GIF, Flash animation or digital video. To display animation, a digital camera, computer, or projector are used along with new technologies that are produced.

Animation creation methods include the traditional animation creation method and those involving stop motion animation of two and three-dimensional objects, such as paper cutouts, puppets and clay figures. Images are displayed in a rapid succession, usually 24, 25, 30, or 60 frames per second.

From Latin animÄtiÅ, "the act of bringing to life"; from animÅ ("to animate" or "give life to") and -ÄtiÅ ("the act of").

Early examples of attempts to capture the phenomenon of motion into a still drawing can be found in paleolithic cave paintings, where animals are often depicted with multiple legs in superimposed positions, clearly attempting to convey the perception of motion.

An earthen goblet discovered at the site of the 5,200-year-old Burnt City in southeastern Iran, depicts what could possibly be the worldâ€™s oldest example of animation. The artifact bears five sequential images depicting a Persian Desert Ibex jumping up to eat the leaves of a tree.

Ancient Chinese records contain several mentions of devices that were said to "give an impression of movement" to human or animal figures, but these accounts are unclear and may only refer to the actual movement of the figures through space.

In the 19th century, the phenakistoscope (1832), zoetrope (1834) and praxinoscope (1877), as well as the common flip book, were early animation devices that produced an illusion of movement from a series of sequential drawings, but animation did not develop further until the advent of motion picture film and cinematography in the 1890s.

The cinÃ©matographe was a projector, printer, and camera in one machine that allowed moving pictures to be shown successfully on a screen which was invented by history's earliest film makers, Auguste and Louis LumiÃ¨re, in 1894. The first animated projection (screening) was created in France, by Charles-Ã‰mile Reynaud, who was a French science teacher. Reynaud created the Praxinoscope in 1877 and the ThÃ©Ã¢tre Optique in December 1888. On 28 October 1892, he projected the first animation in public, Pauvre Pierrot, at the MusÃ©e GrÃ©vin in Paris. This film is also notable as the first known instance of film perforations being used. His films were not photographed, but drawn directly onto the transparent strip. In 1900, more than 500,000 people had attended these screenings.
The first film that was recorded on standard picture film and included animated sequences was the 1900 Enchanted Drawing, which was followed by the first entirely animated film - the 1906 Humorous Phases of Funny Faces by J. Stuart Blackton, who, because of that, is considered the father of American animation.
In Europe, the French artist, Ã‰mile Cohl, created the first animated film using what came to be known as traditional animation creation methods - the 1908 Fantasmagorie. The film largely consisted of a stick figure moving about and encountering all manner of morphing objects, such as a wine bottle that transforms into a flower. There were also sections of live action in which the animatorâ€™s hands would enter the scene. The film was created by drawing each frame on paper and then shooting each frame onto negative film, which gave the picture a blackboard look.

The author of the first puppet-animated film (The Beautiful Lukanida (1912)) was the Russian-born (ethnically Polish) director Wladyslaw Starewicz, known as Ladislas Starevich.

The more detailed hand-drawn animations, requiring a team of animators drawing each frame manually with detailed backgrounds and characters, were those directed by Winsor McCay, a successful newspaper cartoonist, including the 1911 Little Nemo, the 1914 Gertie the Dinosaur, and the 1918 The Sinking of the Lusitania.

During the 1910s, the production of animated short films, typically referred to as "cartoons", became an industry of its own and cartoon shorts were produced for showing in movie theaters. The most successful producer at the time was John Randolph Bray, who, along with animator Earl Hurd, patented the cel animation process which dominated the animation industry for the rest of the decade.

El ApÃ³stol (Spanish: "The Apostle") was a 1917 Argentine animated film utilizing cutout animation, and the world's first animated feature film. Unfortunately, a fire that destroyed producer Frederico Valle's film studio incinerated the only known copy of El ApÃ³stol, and it is now considered a lost film.

Computer animation has become popular since Toy Story (1995), the first feature-length animated film completely made using this technique.

In 2008, the animation market was worth US$68.4 billion.  Animation as an art and industry continues to thrive as of the mid-2010s, because well-made animated projects can find audiences across borders and in all four quadrants.  Animated feature-length films returned the highest gross margins (around 52%) of all film genres in the 2004-2013 timeframe.


Traditional animation (also called cel animation or hand-drawn animation) was the process used for most animated films of the 20th century. The individual frames of a traditionally animated film are photographs of drawings, first drawn on paper. To create the illusion of movement, each drawing differs slightly from the one before it. The animators' drawings are traced or photocopied onto transparent acetate sheets called cels, which are filled in with paints in assigned colors or tones on the side opposite the line drawings. The completed character cels are photographed one-by-one against a painted background by a rostrum camera onto motion picture film.

The traditional cel animation process became obsolete by the beginning of the 21st century. Today, animators' drawings and the backgrounds are either scanned into or drawn directly into a computer system. Various software programs are used to color the drawings and simulate camera movement and effects. The final animated piece is output to one of several delivery media, including traditional 35mm film and newer media such as digital video. The "look" of traditional cel animation is still preserved, and the character animators' work has remained essentially the same over the past 70 years. Some animation producers have used the term "tradigital" to describe cel animation which makes extensive use of computer technologies.

Examples of traditionally animated feature films include Pinocchio (United States, 1940), Animal Farm (United Kingdom, 1954), and The Illusionist (British-French, 2010). Traditionally animated films which were produced with the aid of computer technology include The Lion King (US, 1994), Akira (Japan, 1988), Spirited Away (Japan, 2001), The Triplets of Belleville (France, 2003), and The Secret of Kells (Irish-French-Belgian, 2009).

Full animation refers to the process of producing high-quality traditionally animated films that regularly use detailed drawings and plausible movement, having a smooth animation. Fully animated films can be made in a variety of styles, from more realistically animated works such as those produced by the Walt Disney studio (Beauty and the Beast, Aladdin, Lion King) to the more 'cartoon' styles of the Warner Bros. animation studio. Many of the Disney animated features are examples of full animation, as are non-Disney works such as The Secret of NIMH (US, 1982), The Iron Giant (US, 1999), and Nocturna (Spain, 2007).
Limited animation involves the use of less detailed or more stylized drawings and methods of movement usually a choppy or "skippy" movement animation. Pioneered by the artists at the American studio United Productions of America, limited animation can be used as a method of stylized artistic expression, as in Gerald McBoing Boing (US, 1951), Yellow Submarine (UK, 1968), and much of the anime produced in Japan. Its primary use, however, has been in producing cost-effective animated content for media such as television (the work of Hanna-Barbera, Filmation, and other TV animation studios) and later the Internet (web cartoons).
Rotoscoping is a technique patented by Max Fleischer in 1917 where animators trace live-action movement, frame by frame. The source film can be directly copied from actors' outlines into animated drawings, as in The Lord of the Rings (US, 1978), or used in a stylized and expressive manner, as in Waking Life (US, 2001) and A Scanner Darkly (US, 2006). Some other examples are: Fire and Ice (US, 1983), Heavy Metal (1981), and Aku no Hana (2013).
Live-action/animation is a technique combining hand-drawn characters into live action shots. One of the earlier uses was in Koko the Clown when Koko was drawn over live action footage. Other examples include Who Framed Roger Rabbit (US, 1988), Space Jam (US, 1996) and Osmosis Jones (US, 2001).

Stop-motion animation is used to describe animation created by physically manipulating real-world objects and photographing them one frame of film at a time to create the illusion of movement. There are many different types of stop-motion animation, usually named after the medium used to create the animation. Computer software is widely available to create this type of animation; however, traditional stop motion animation is usually less expensive and time-consuming to produce than current computer animation.

Puppet animation typically involves stop-motion puppet figures interacting in a constructed environment, in contrast to real-world interaction in model animation. The puppets generally have an armature inside of them to keep them still and steady as well as to constrain their motion to particular joints. Examples include The Tale of the Fox (France, 1937), The Nightmare Before Christmas (US, 1993), Corpse Bride (US, 2005), Coraline (US, 2009), the films of JiÅ™Ã­ Trnka and the TV series Robot Chicken (US, 2005â€“present).
Puppetoon, created using techniques developed by George Pal, are puppet-animated films which typically use a different version of a puppet for different frames, rather than simply manipulating one existing puppet.

Clay animation, or Plasticine animation (often called claymation, which, however, is a trademarked name), uses figures made of clay or a similar malleable material to create stop-motion animation. The figures may have an armature or wire frame inside, similar to the related puppet animation (below), that can be manipulated to pose the figures. Alternatively, the figures may be made entirely of clay, such as in the films of Bruce Bickford, where clay creatures morph into a variety of different shapes. Examples of clay-animated works include The Gumby Show (US, 1957â€“1967) Morph shorts (UK, 1977â€“2000), Wallace and Gromit shorts (UK, as of 1989), Jan Å vankmajer's Dimensions of Dialogue (Czechoslovakia, 1982), The Trap Door (UK, 1984). Films include , Chicken Run and The Adventures of Mark Twain.
Cutout animation is a type of stop-motion animation produced by moving two-dimensional pieces of material such as paper or cloth. Examples include Terry Gilliam's animated sequences from Monty Python's Flying Circus (UK, 1969â€“1974); Fantastic Planet (France/Czechoslovakia, 1973) ; Tale of Tales (Russia, 1979), The pilot episode of the TV series (and sometimes in episodes) of South Park (US, 1997) and the music video Live for the moment, from Verona Riots band (produced by Alberto Serrano and NÃ­vola UyÃ¡, Spain 2014).
Silhouette animation is a variant of cutout animation in which the characters are backlit and only visible as silhouettes. Examples include The Adventures of Prince Achmed (Weimar Republic, 1926) and Princes et princesses (France, 2000).
Model animation refers to stop-motion animation created to interact with and exist as a part of a live-action world. Intercutting, matte effects, and split screens are often employed to blend stop-motion characters or objects with live actors and settings. Examples include the work of Ray Harryhausen, as seen in films such Jason and the Argonauts (1963), and the work of Willis O'Brien on films such as King Kong (1933 film).
Go motion is a variant of model animation that uses various techniques to create  motion blur between frames of film, which is not present in traditional stop-motion. The technique was invented by Industrial Light & Magic and Phil Tippett to create special effects scenes for the film  (1980). Another example is the dragon named "Vermithrax" from Dragonslayer (1981 film).
Object animation refers to the use of regular inanimate objects in stop-motion animation, as opposed to specially created items.
Graphic animation uses non-drawn flat visual graphic material (photographs, newspaper clippings, magazines, etc.), which are sometimes manipulated frame-by-frame to create movement. At other times, the graphics remain stationary, while the stop-motion camera is moved to create on-screen action.
Brickfilm A subgenre of object animation involving using Lego or other similar brick toys to make an animation. These have had a recent boost in popularity with the advent of video sharing sites like YouTube and the availability of cheap cameras and animation software.
Pixilation involves the use of live humans as stop motion characters. This allows for a number of surreal effects, including disappearances and reappearances, allowing people to appear to slide across the ground, and other such effects. Examples of pixilation include The Secret Adventures of Tom Thumb and Angry Kid shorts.


Computer animation encompasses a variety of techniques, the unifying factor being that the animation is created digitally on a computer. 2D animation techniques tend to focus on image manipulation while 3D techniques usually build virtual worlds in which characters and objects move and interact. 3D animation can create images that seem real to the viewer.

2D animation figures are created or edited on the computer using 2D bitmap graphics or created and edited using 2D vector graphics. This includes automated computerized versions of traditional animation techniques such as interpolated morphing, onion skinning and interpolated rotoscoping.
2D animation has many applications, including analog computer animation, Flash animation and PowerPoint animation. Cinemagraphs are still photographs in the form of an animated GIF file of which part is animated.

Final line advection animation, a technique that gives the artists and animators a lot more influence and control over the final product as everything is done within the same department. Examples include Paperman and Feast (2014 film): In Paperman, we didnâ€™t have a cloth department and we didnâ€™t have a hair department. Here, folds in the fabric, hair silhouettes and the like come from of the committed design decision-making that comes with the 2D drawn process. Our animators can change things, actually erase away the CG underlayer if they want, and change the profile of the arm. And they can design all the fabric in that Milt Kahl kind-of way, if they want to.


3D animation is digitally modeled and manipulated by an animator. The animator usually starts by creating a 3D polygon mesh to manipulate. A mesh typically includes many vertices that are connected by edges and faces, to give the visual appearance of form to a 3D object or 3D environment. Sometimes, the mesh is given an internal digital skeletal structure called an armature that can be used to control the mesh by weighting the vertices. This process is called rigging and can be used in conjunction with keyframes to create movement.

Other techniques can be applied, such as mathematical functions (e.g., gravity, particle simulations), simulated fur or hair, and effects such as fire and water simulations. These techniques fall under the category of 3D dynamics.

Cel-shaded animation is used to mimic traditional animation using CG software. Shading looks stark, with less blending of colors. Examples include, Skyland (2007, France), The Iron Giant (1999, United States), Futurama (Fox, 1999) Appleseed Ex Machina (2007, Japan),  (2002, Japan)
Machinima â€“ Films created by screen capturing in video games and virtual worlds.
Motion capture is used when live-action actors wear special suits that allow computers to copy their movements into CG characters. Examples include Polar Express (2004, US), Beowulf (2007, US), A Christmas Carol (2009, US), The Adventures of Tintin (film) (2011, US) kochadiiyan(2014, India)
Photo-realistic animation is used primarily for animation that attempts to resemble real life, using advanced rendering that mimics in detail skin, plants, water, fire, clouds, etc. Examples include Up (2009, US), How to Train Your Dragon (2010, US), Ice Age (2002, US).


Animatronics is the use of mechatronics to create machines which seem animate rather than robotic.
Audio-Animatronics and Autonomatronics is a form of robotics animation, combined with 3-D animation, created by Walt Disney Imagineering for shows and attractions at Disney theme parks move and make noise (generally a recorded speech or song), but are fixed to whatever supports them. They can sit and stand but cannot walk. An Audio-Animatron is different from an android-type robot in that it uses prerecorded movements and sounds, rather than responding to external stimuli. In 2009, Disney created an interactive version of the technology called Autonomatronics.
Linear Animation Generator is a form of animation by using static picture frames installed in a tunnel or a shaft. The animation illusion is created by putting the viewer in a linear motion, parallel to the installed picture frames. The concept and the technical solution, were invented in 2007 by Mihai Girlovan in Romania.
Chuckimation is a type of animation created by the makers of the cartoon Action League Now! in which characters/props are thrown, or chucked from off camera or wiggled around to simulate talking by unseen hands,
Puppetry is a form of theatre or performance animation that involves the manipulation of puppets. It is very ancient, and is believed to have originated 3000 years BC. Puppetry takes many forms but they all share the process of animating inanimate performing objects. Puppetry is used in almost all human societies both as entertainment â€“ in performance â€“ and ceremonially in rituals and celebrations such as carnivals. Most puppetry involves storytelling.
Zoetrope is a device that produces the illusion of motion from a rapid succession of static pictures. The term zoetrope is from the Greek words Î¶Ï‰Î® (zoÄ“), meaning "alive, active", and Ï„ÏÏŒÏ€Î¿Ï‚ (tropos), meaning "turn", with "zoetrope" taken to mean "active turn" or "wheel of life".


Hydrotechnics: a technique that includes lights, water, fire, fog, and lasers, with high-definition projections on mist screens.
Drawn on film animation: a technique where footage is produced by creating the images directly on film stock, for example by Norman McLaren, Len Lye and Stan Brakhage.
Paint-on-glass animation: a technique for making animated films by manipulating slow drying oil paints on sheets of glass, for example by Aleksandr Petrov.
Erasure animation: a technique using traditional 2D media, photographed over time as the artist manipulates the image. For example, William Kentridge is famous for his charcoal erasure films, and Piotr DumaÅ‚a for his auteur technique of animating scratches on plaster.
Pinscreen animation: makes use of a screen filled with movable pins that can be moved in or out by pressing an object onto the screen. The screen is lit from the side so that the pins cast shadows. The technique has been used to create animated films with a range of textural effects difficult to achieve with traditional cel animation.
Sand animation: sand is moved around on a back- or front-lighted piece of glass to create each frame for an animated film. This creates an interesting effect when animated because of the light contrast.
Flip book: a flip book (sometimes, especially in British English, called a flick book) is a book with a series of pictures that vary gradually from one page to the next, so that when the pages are turned rapidly, the pictures appear to animate by simulating motion or some other change. Flip books are often illustrated books for children, but may also be geared towards adults and employ a series of photographs rather than drawings. Flip books are not always separate books, but may appear as an added feature in ordinary books or magazines, often in the page corners. Software packages and websites are also available that convert digital video files into custom-made flip books.
Character animation
Multi-sketching
Special effects animation


The creation of non-trivial animation works (i.e., longer than a few seconds) has developed as a form of filmmaking, but with certain unique aspects.  One thing live-action and animated feature-length films do have in common is that they are both extremely labor-intensive and have high production costs.

The most important difference is that once a film is in the production phase, the marginal cost of one more shot is much, much higher for animated films than for live-action films.  It is relatively easy for a director to ask for one more take during principal photography of a live-action film, but every take on an animated film must be manually rendered by animators (although the task of rendering slightly different takes has been made less tedious by modern computer animation).  It is pointless for a studio to pay the salaries of dozens of animators to spend weeks creating a visually dazzling five-minute scene, if that scene fails to effectively advance the plot of the film.  Thus, animation studios starting with Disney began the practice in the 1930s of maintaining story departments where storyboard artists develop every single scene through storyboards, then handing the film over to the animators only after the production team is satisfied that all the scenes will make sense as a whole. While live-action films are now also storyboarded, they necessarily enjoy much more latitude to depart from storyboards (i.e., real-time improvisation).

Another problem unique to animation is the necessity of ensuring that the style of an animated film is consistent from start to finish, even as films have grown longer and teams have grown larger.  Animators, like all artists, necessarily have their own individual styles, but must subordinate their individuality in a consistent way to whatever style was selected for a particular film.  Since the early 1980s, feature-length animated films have been created by teams of about 500 to 600 people, of whom 50 to 70 are animators.  It is relatively easy for two or three artists to match each other's styles, but it is much harder to keep dozens of artists synchronized with one another.

This problem is usually solved by having a separate group of visual development artists develop an overall look and palette for each film before animation begins.  Character designers on the visual development team draw model sheets to show how each character should look like with different facial expressions, posed in different positions, and viewed from different angles.  On traditionally animated projects, maquettes were often sculpted to further help the animators see how characters would look from different angles.

Unlike live-action films, animated films were traditionally developed beyond the synopsis stage through the storyboard format; the storyboard artists would then receive credit for writing the film.  In the early 1960s, animation studios began hiring professional screenwriters to write screenplays (while also continuing to use story departments) and such screenplays had become commonplace for animated films by the late 1980s.

As with any other form of media, animation too has instituted awards for excellence in the field. The original awards for animation were presented by the Academy of Motion Picture Arts and Sciences for animated shorts from the year 1932, during the 5th Academy Awards function. The first winner of the Academy Award was the short Flowers and Trees, a production by Walt Disney Productions and United Artists. However, the Academy Award for a feature-length animated motion picture was only instituted for the year 2001, and awarded during the 74th Academy Awards in 2002. It was won by the movie Shrek, produced by DreamWorks and Pacific Data Images. Since then, Disney/Pixar have produced the most movies either to win or be nominated for the award. The list of both awards can be obtained here:
Academy Award for Best Animated Feature
Academy Award for Best Animated Short Film

Several other countries have instituted an award for best animated feature film as part of their national film awards: BAFTA Award for Best Animated Film (since 2006), CÃ©sar Award for Best Animated Film (since 2011), Goya Award for Best Animated Film (since 1989), Japan Academy Prize for Animation of the Year (since 2007). Also since 2007, the Asia Pacific Screen Award for Best Animated Feature Film has been awarded at the Asia Pacific Screen Awards. Since 2009, the European Film Awards have awarded the European Film Award for Best Animated Film.
 
The Annie Award is another award presented for excellence in the field of animation. Unlike the Academy Awards, the Annie Awards are only received for achievements in the field of animation and not for any other field of technical and artistic endeavor. They were re-organized in 1992 to create a new field for Best Animated feature. The 1990s winners were dominated by Walt Disney, however newer studios, led by Pixar & DreamWorks, have now begun to consistently vie for this award. The list of awardees is as follows: 
Annie Award for Best Animated Feature
Annie Award for Best Animated Short Subject
Annie Award for Best Animated Television Production


Animation Department

12 basic principles of animation
Animation software
Architectural animation
Avar (animation variable)
Computer generated imagery
International TournÃ©e of Animation
List of motion picture topics
Model sheet
Motion graphic design
Tradigital art
War film#Animated
Wire frame model
Anime

, Vol. 45, No. 1 (Spring 1993): 3-12
Culhane, Shamus, Animation Script to Screen
Laybourne, Kit, The Animation Book
Musa, S; Ziatdinov, R; Griffiths, C. (2013). Introduction to computer animation and its possible educational applications. In M. GallovÃ¡, J. GunÄaga, Z. ChanasovÃ¡, M.M. ChovancovÃ¡ (Eds.), New Challenges in Education. Retrospection of history of education to the future in the interdisciplinary dialogue among didactics of various school subjects (1st ed., pp.177â€“205). RuÅ¾omberok, Slovakia: VERBUM â€“ vydavateÄ¾stvo KatolÃ­ckej univerzity v RuÅ¾omberku.
Ledoux, Trish, Ranney, Doug, & Patten, Fred (Ed.), Complete Anime Guide: Japanese Animation Film Directory and Resource Guide, Tiger Mountain Press 1997
Lowe, Richard & Schnotz, Wolfgang (Eds) Learning with Animation. Research implications for design Cambridge University Press, 2008
Masson, Terrence,  Unique and personal histories of early computer animation production, plus a comprehensive foundation of the industry for all reading levels. ISBN 978-0-9778710-0-1
Serenko, Alexander, , Computers in Human Behavior Vol. 23, No. 1 (2007): 478-495.
Thomas, Frank and Johnston, Ollie, , Abbeville 1981
Walters, Faber and Helen (Ed.), Animation Unlimited: Innovative Short Films Since 1940, HarperCollins Publishers, 2004
Williams, Richard, The Animator's Survival Kit ISBN 978-0-571-20228-7
Bob Godfrey and Anna Jackson, 'The Do-It-Yourself Film Animation Book' BBC Publications 1974 ISBN 978-0-563-10829-0 Now out of print but available s/hand through a range of sources such as Amazon Uk.
Lawson, Tim and Alisa Persons. The Magic Behind the Voices: A Who's Who of Cartoon Voice Actors. University Press of Mississippi. 2004. (A history of cartoon voice-overs and biographies and photographs of many prominent animation voice actors.)
Ball, R., Beck, J., DeMott R., Deneroff, H., Gerstein, D., Gladstone, F., Knott, T., Leal, A., Maestri, G., Mallory, M., Mayerson, M., McCracken, H., McGuire, D., Nagel, J., Pattern, F., Pointer, R., Webb, P., Robinson, C., Ryan, W., Scott, K., Snyder, A. & Webb, G. (2004) Animation Art: From Pencil to Pixel, the History of Cartoon, Anime & CGI. Fulhamm London.: Flame Tree Publishing. ISBN 978-1-84451-140-2
, a 12-minute film demonstrating 10 different animation techniques (and teaching how to use them).


Apollo (Attic, Ionic, and Homeric Greek: , ApollÅn ( ); Doric: , ApellÅn; Arcadocypriot: , ApeilÅn; Aeolic: , Aploun; ) is one of the most important and complex of the Olympian deities in classical Greek and Roman religion and Greek and Roman mythology. The ideal of the kouros (a beardless, athletic youth), Apollo has been variously recognized as a god of music, truth and prophecy, healing, the sun and light, plague, poetry, and more. Apollo is the son of Zeus and Leto, and has a twin sister, the chaste huntress Artemis. Apollo is known in Greek-influenced Etruscan mythology as Apulu.

As the patron of Delphi (Pythian Apollo), Apollo was an oracular godâ€”the prophetic deity of the Delphic Oracle. Medicine and healing are associated with Apollo, whether through the god himself or mediated through his son Asclepius, yet Apollo was also seen as a god who could bring ill-health and deadly plague. Amongst the god's custodial charges, Apollo became associated with dominion over colonists, and as the patron defender of herds and flocks. As the leader of the Muses (Apollon Musegetes) and director of their choir, Apollo functioned as the patron god of music and poetry. Hermes created the lyre for him, and the instrument became a common attribute of Apollo. Hymns sung to Apollo were called paeans.

In Hellenistic times, especially during the 3rd century BCE, as Apollo Helios he became identified among Greeks with Helios, Titan god of the sun, and his sister Artemis similarly equated with Selene, Titan goddess of the moon. In Latin texts, on the other hand, Joseph Fontenrose declared himself unable to find any conflation of Apollo with Sol among the Augustan poets of the 1st century, not even in the conjurations of Aeneas and Latinus in Aeneid XII (161â€“215). Apollo and Helios/Sol remained separate beings in literary and mythological texts until the 3rd century CE.

The name of Apollo itselfâ€”though not Paean, a possible name of a precursor god to or epithet of himâ€”is generally considered to be absent from the Linear B (Mycenean Greek) texts although it is possible that the name is in fact attested in the lacunose form ]pe-rjo--

Andre Kirk Agassi (; born April 29, 1970) is an American retired professional tennis player and former World No. 1, who was one of the game's most dominant players from the early 1990s to the mid-2000s. Generally considered by critics and fellow players to be one of the greatest tennis players of all time, Agassi had been called the best service returner in the history of the game. Described by the BBC upon his retirement as "perhaps the biggest worldwide star in the sport's history", Agassi compiled performances that, along with his unorthodox apparel and attitude, saw him cited as one of the most charismatic players in the history of the game. As a result, he is credited for helping to revive the popularity of tennis during the 1990s.

In singles tennis, Agassi is an eight-time Grand Slam champion and a 1996 Olympic gold medalist, as well as finishing runner-up in seven other Grand Slam tournaments. During the Open Era, Agassi is the first male player to win 4 Australian Open titles and those were an Open Era record until Novak Djokovic won his 5th title on 1 February 2015. Agassi is one of four male singles players to achieve the Career Grand Slam (all four Grand Slam championships) in the Open Era and one of seven in history, the first of two to achieve the Career Golden Slam (Career Grand Slam and Olympic gold medal), and the only man to win the Career Golden Slam and the ATP Tour World Championships (won in 1990): a distinction dubbed as a "Career Super Slam" by Sports Illustrated.

Agassi was the first male player to win all four Grand Slam tournaments on three different surfaces (hard, clay and grass), and the last American male to win the French Open (1999) and the Australian Open (2003). He also won 17 ATP Masters Series titles and was part of a winning Davis Cup team in 1990, 1992 and 1995. Agassi reached the World No. 1 ranking for the first time in 1995 but was troubled by personal issues during the mid-to-late 1990s and sank to World No. 141 in 1997, prompting many to believe that his career was over. Agassi returned to World No. 1 in 1999 and enjoyed the most successful run of his career over the next four years. During his 20-plus year tour career, Agassi was known by the nickname "The Punisher".

After suffering from sciatica caused by two bulging discs in his back, a spondylolisthesis (vertebral displacement) and a bone spur that interfered with the nerve, Agassi retired from professional tennis on September 3, 2006, after losing in the third round of the US Open to Benjamin Becker. He is the founder of the Andre Agassi Charitable Foundation, which has raised over $60million for at-risk children in Southern Nevada. In 2001, the Foundation opened the Andre Agassi College Preparatory Academy in Las Vegas, a K-12 public charter school for at-risk children. He has been married to fellow tennis player Steffi Graf since 2001.

Andre Agassi was born in Las Vegas, Nevada to Emmanuel "Mike" Agassi and Elizabeth "Betty" Agassi (nÃ©e Dudley). His father, a former Olympic boxer for Iran, is Armenian and half Assyrian. Andre Agassi's mother, Betty, is a breast cancer survivor. He has three older siblings â€“ Rita (last wife to Pancho Gonzales), Philip and Tami. One of his ancestors changed his surname from Agassian to Agassi to avoid persecution.

In a passage from the book Open, Agassi details how his father made him play a match for money with football legend Jim Brown, in 1979, when Agassi was 9 years old. Brown was at a Vegas tennis club complaining to the owner about a money match that was canceled. Agassi's father stepped in and told Brown that he could play his son and he would put up his house for the wager. Brown countered with a $10,000 bet, but after he was warned by the club owner not to take the bet because he would lose and be embarrassed, Brown agreed with Mike Agassi that they would set the amount after he and Andre played two sets. Brown lost those sets, 3â€“6, 3â€“6, declined the 10K wager, and offered to play the third set for $500. He lost 2â€“6.

At age 13, Andre was sent to Nick Bollettieri's Tennis Academy in Florida. He was meant to stay for only 3 months because that was all his father could afford. After thirty minutes of watching Agassi play, Bollettieri called Mike and said: "Take your check back. He's here for free," claiming that Agassi had more natural talent than anyone else he had seen. Agassi dropped out of school in the ninth grade.



He turned professional at the age of 16 and competed in his first tournament at La Quinta, California. He won his first match against John Austin, but then lost his second match to Mats Wilander. By the end of the year, Agassi was ranked world no. 91. He won his first top-level singles title in 1987 at the Sul American Open in Itaparica and ended the year ranked world no. 25. He won six additional tournaments in 1988 (Memphis, U.S. Men's Clay Court Championships, Forest Hills WCT, Stuttgart Outdoor, Volvo International and Livingston Open), and, by December of that year, he had surpassed US$1million in career prize money after playing in just 43 tournamentsâ€”the fastest anyone in history had reached that level. During the year, he set the open-era record for most consecutive victories by a male teenager, a record that stood for 17 years until Rafael Nadal broke it in 2005. His year-end ranking was world no. 3, behind second-ranked Ivan Lendl and top-ranked Mats Wilander. Both the Association of Tennis Professionals and Tennis magazine named Agassi the Most Improved Player of the Year for 1988.

In addition to not playing the Australian Open (which later became his best Grand Slam event) for the first eight years of his career, Agassi chose not to play at Wimbledon from 1988 through 1990 and publicly stated that he did not wish to play there because of the event's traditionalism, particularly its "predominantly white" dress code to which players at the event are required to conform.

Strong performances on the tour meant that Agassi was quickly tipped as a future Grand Slam champion. While still a teenager, he reached the semifinals of both the French Open and the US Open in 1988 and made the US Open semifinals in 1989. He began the 1990s with a series of near-misses. He reached his first Grand Slam final in 1990 at the French Open, where he was favored before losing in four sets to AndrÃ©s GÃ³mez, which he attributes to worrying about his wig falling off. He reached his second Grand Slam final of the year at the US Open, defeating defending champion Boris Becker in the semifinals. His opponent in the final was Pete Sampras; a year earlier, Agassi had crushed Sampras, after which he told his coach that he felt bad for Sampras because he was never going to make it as a pro. Agassi lost the US Open final to Sampras in three sets. The rivalry between these two American players became the dominant rivalry in tennis over the rest of the decade. Also in 1990, Agassi helped the United States win its first Davis Cup in 8 years and won his only Tennis Masters Cup, beating reigning Wimbledon champion Stefan Edberg in the final.

In 1991, Agassi reached his second consecutive French Open final, where he faced fellow Bollettieri Academy alumnus Jim Courier. Courier emerged the victor in a five-set final. Agassi decided to play at Wimbledon in 1991, leading to weeks of speculation in the media about the clothes he would wear. He eventually emerged for the first round in a completely white outfit. He reached the quarterfinals on that occasion, losing in five sets to David Wheaton.

Agassi's Grand Slam tournament breakthrough came at Wimbledon, not at the French Open or the US Open, where he had previously enjoyed success. In 1992, he defeated Goran IvaniÅ¡eviÄ‡ in a five-set final. Along the way, Agassi overcame two former Wimbledon champions: Boris Becker and John McEnroe. No other baseliner would triumph at Wimbledon until Lleyton Hewitt ten years later. Agassi was named the BBC Overseas Sports Personality of the Year in 1992. Agassi once again played on the United States' Davis Cup winning team in 1992. It was their second Davis cup title in three years.

1993 saw Agassi win the only doubles title of his career, at the Cincinnati Masters, partnered with Petr Korda. Agassi missed much of the early part of that year with injuries. Although he made the quarterfinals in his Wimbledon title defense, he lost to eventual champion and world no. 1 Pete Sampras in five sets. Agassi lost in the first round at the US Open to Thomas Enqvist and required wrist surgery late in the year.

With new coach Brad Gilbert on board, Agassi began to employ more of a tactical, consistent approach, which fueled his resurgence. He started slowly in 1994, losing in the first week at the French Open and Wimbledon. Nevertheless, he emerged during the hard-court season, winning the Canadian Open. His comeback culminated at the 1994 US Open with a five-set fourth-round victory against compatriot Michael Chang. He then became the first man to capture the US Open as an unseeded player, beating Michael Stich in the final. Along the way, he beat 5 seeded players.

In 1995, Agassi shaved his balding head, breaking with his old "image is everything" style. He competed in the 1995 Australian Open (his first appearance at the event) and won, beating Sampras in a four-set final. Agassi and Sampras met in five tournament finals in 1995, all on hardcourt, with Agassi winning three. Agassi won three Masters Series events in 1995 (Cincinnati, Key Biscayne, and the Canadian Open) and seven titles total. He compiled a career-best 26-match winning streak during the summer hard-court circuit, which ended when he lost the US Open final to Sampras.

Agassi reached the world no. 1 ranking for the first time in April 1995. He held that ranking until November, for a total of 30 weeks. Agassi skipped most of the fall indoor season which allowed Sampras to surpass him and finish ranked no. 1 at the year-ending ranking. In terms of win/loss record, 1995 was Agassi's best year. He won 73 matches while losing 9 and was also once again a key player on the United States' Davis Cup winning teamâ€”the third and final Davis Cup title of Agassi's career.

1996 was a less successful year for Agassi, as he failed to reach any Grand Slam final. He suffered two early-round losses at the hands of compatriots Chris Woodruff and Doug Flach at the French Open and Wimbledon, respectively, and lost to Chang in straight sets in the Australian and US Open semifinals. At the time, Agassi blamed the Australian Open loss on the windy conditions, but later said in his biography that he had lost the match on purpose, as he did not want to play Boris Becker, whom he would have faced in that final. The high point for Agassi was winning the men's singles gold medal at the Olympic Games in Atlanta, beating Sergi Bruguera of Spain in the final.  Agassi also successfully defended his singles titles in Cincinnati and Key Biscayne.

1997 was the low point of Agassi's career. His wrist injury resurfaced, and he played only 24 matches during the year. He later confessed that he started using crystal methamphetamine at that time, allegedly on the urging of a friend. He failed an ATP drug test, but wrote a letter claiming the same friend had spiked a drink. The ATP dropped the failed drug test as a warning. In his autobiography, Agassi admitted that the letter was a lie. He quit the drug soon after. At this time Agassi was also in a failing marriage with actress Brooke Shields and had lost interest in the game. He won no top-level titles, and his ranking sank to world no. 141 on November 10, 1997, prompting many to believe that his run as one of the sport's premier competitors was over and that he would never again win any significant championships.


In 1998, Agassi began a rigorous conditioning program and worked his way back up the rankings by playing in Challenger Series tournaments, a circuit for pro players ranked outside the world's top 50. After returning to top physical and mental shape, Agassi recorded the most successful period of his tennis career and also played classic matches in that period against Pete Sampras and Patrick Rafter.

In 1998, Agassi won five titles and leapt from world no. 110 to no. 6, the highest jump into the top 10 made by any player during a calendar year. At Wimbledon, he had an early loss in the second round to Tommy Haas. He won five titles in ten finals and was runner-up at the Masters Series tournament in Key Biscayne, losing to Marcelo RÃ­os, who became world no. 1 as a result. At the year end he was awarded the ATP Most Improved Player of the Year for the second time in his career (the first being 10 years earlier in 1988).

Agassi entered the history books in 1999 when he came back from two sets to love down to beat Andrei Medvedev in a five-set French Open final, becoming, at the time, only the fifth male player (joining Rod Laver, Fred Perry, Roy Emerson and Don Budgeâ€”these have since been joined by a sixth, Roger Federer and a seventh, Rafael Nadal) to win all four Grand Slam singles titles during his career. Only Laver, Agassi, Federer and Nadal have achieved this feat during the open era. This win also made him the first (of only three, the second and third being Roger Federer and Rafael Nadal respectively) male player in history to have won all four Grand Slam titles on three different surfaces (clay, grass and hard courts), a tribute to his adaptability, as the other four men won their Grand Slam titles on clay and grass courts. Agassi also became the first (of only two, the second being Rafael Nadal) male player to win the Career Golden Slam, consisting of all four Grand Slam tournaments plus an Olympic gold medal in singles.

Agassi followed his 1999 French Open victory by reaching the Wimbledon final, where he lost to Sampras in straight sets. He rebounded from his Wimbledon defeat by winning the US Open, beating Todd Martin in five sets (rallying from a two sets to one deficit) in the final. Overall during the year Agassi won 5 titles including two majors and the ATP Masters Series in Paris, where he beat Marat Safin. Agassi ended 1999 as the world no. 1, ending Sampras's record of six consecutive year-ending top rankings (1993â€“1998).  This was the only time Agassi ended the year at no. 1.

He began the next year by capturing his second Australian Open title, beating Sampras in a five-set semifinal and Yevgeny Kafelnikov in a four-set final. He was the first male player to have reached four consecutive Grand Slam finals since Rod Laver achieved the Grand Slam in 1969.  At the time, Agassi was also only the fourth player since Laver to be the reigning champion of three of four Grand Slam events, missing only the Wimbledon title.. 2000 also saw Agassi reach the semifinals at Wimbledon, where he lost in five sets to Rafter in a match considered by many to be one of the best ever at Wimbledon. At the inaugural Tennis Masters Cup in Lisbon, Agassi reached the final after defeating Marat Safin in the semifinals to end the Russian's hopes to become the youngest world no. 1 in the history of tennis. Agassi then lost to Gustavo Kuerten in the final, allowing Kuerten to be crowned year-end world no. 1.

Agassi opened 2001 by successfully defending his Australian Open title with a straight-sets final win over Arnaud ClÃ©ment. En route, he beat a cramping Rafter in five sets in front of a sell-out crowd in what turned out to be the Aussie's last Australian Open. At Wimbledon, they met again in the semifinals, where Agassi lost another close match to Rafter, 8â€“6 in the fifth set. In the quarterfinals at the US Open, Agassi lost a 3-hour, 33minute epic match with Sampras, 7â€“6, 6â€“7, 6â€“7, 6â€“7, with no breaks of serve during the 52-game match. Despite the setback, Agassi finished 2001 ranked world no. 3, becoming the only male tennis player to finish a year ranked in the top 3 in three different decades (1980s, 1990s, 2000s). He also was the oldest player (age 31) to finish in the top three since 32-year old Connors finished at world no. 2 in 1984.

2002 opened with disappointment for Agassi, as injury forced him to skip the Australian Open, where he was a two-time defending champion. Agassi recovered from the injury and later that year defended his Key Biscayne title beating then rising Roger Federer in a four-set final. The last duel between Agassi and Sampras came in the final of the US Open, which Sampras won in four sets and left Sampras with a 20â€“14 edge in their 34 career meetings. The match was the last of Sampras's career. Agassi's US Open finish, along with his Masters Series victories in Key Biscayne, Rome and Madrid, helped him finish 2002 as the oldest year-end world no. 2 at 32 years and 8 months.

In 2003, Agassi won the eighth (and final) Grand Slam title of his career at the Australian Open, where he beat Rainer SchÃ¼ttler in straight sets in the final. In March, he won his sixth career and third consecutive Key Biscayne title, in the process surpassing his wife, Steffi Graf, who was a five-time winner of the event. The final was his 18th straight win in that tournament, which broke the previous record of 17 set by Sampras from 1993â€“1995. (Agassi's winning streak continued to 20 after winning his first two matches at the 2004 edition of that tournament before bowing to AgustÃ­n Calleri.) With the victory, Agassi became the youngest (19 years old) and oldest (32) winner of the Key Biscayne tournament.

On April 28, 2003, he recaptured the world no. 1 ranking after a quarterfinal victory over Xavier Malisse at the Queen's Club Championships to become the oldest top-ranked male player since the ATP rankings began at 33 years and 13 days. He had held the world no. 1 ranking for two weeks, when Lleyton Hewitt took it back on May 12, 2003. Agassi then recaptured the world no. 1 ranking once again on June 16, 2003, which he held for 12 weeks until September 7, 2003. During his career, Agassi held the world no. 1 ranking for a total of 101 weeks. Agassi's ranking slipped when injuries forced him to withdraw from many events. He did manage to reach the US Open semifinals, where he lost to Juan Carlos Ferrero and surrendered his world no. 1 ranking to Ferrero. At the year-end Tennis Masters Cup, Agassi lost in the final to Federer and finished the year ranked world no. 4. At age 33, he was the oldest player to rank in the top 5 since Connors, at age 35, was world no. 4 in 1987.

In 2004, Agassi began the year with a five-set loss in the semifinals of the Australian Open to Marat Safin; the loss ended Agassi's 26-match winning streak at the event, a record that still stands. He won the Masters series event in Cincinnati to bring his career total to 59 top-level singles titles and a record 17 ATP Masters Series titles, having already won seven of the nine ATP Masters tournamentâ€”all except the tournaments in Monte Carlo and Hamburg. At 34, he became the second-oldest singles champion in Cincinnati tournament history (the tournament began in 1899), surpassed only by Ken Rosewall, who won the title in 1970 at age 35. He finished the year ranked world no. 8, the oldest player to finish in the top 10 since the 36-year-old Connors was world no. 7 in 1988. Agassi also became only the sixth male player during the open era to reach 800 career wins with his first-round victory over Alex Bogomolov in Countrywide Classic in Los Angeles.

Agassi's 2005 began with a quarterfinal loss to Federer at the Australian Open. Agassi had several other deep runs at tournaments, but had to withdraw from several events due to injury. He lost to Jarkko Nieminen in the first round of the French Open. He won his fourth title in Los Angeles and reached the final of the Rogers Cup, before falling to world no. 2 Rafael Nadal.

Agassi's 2005 was defined by an improbable run to the US Open final. After beating RÄƒzvan SabÄƒu and Ivo KarloviÄ‡ in straight sets and TomÃ¡Å¡ Berdych in four sets, Agassi won three consecutive five-set matches to advance to the final. The most notable of these matches was his quarterfinal victory over James Blake, where he rallied from two sets down to win 7â€“6 in the fifth set. His other five-set victims were Xavier Malisse in the fourth round and Robby Ginepri in the semifinals. In the final, Agassi faced Federer, who was seeking his second consecutive US Open title and his sixth Grand Slam title in two years. Federer defeated Agassi in four sets. Agassi finished 2005 ranked world no. 7, his 16th time in the year-end top-10 rankings, which tied Connors for the most times ranked in the top 10 at year's end.

Agassi had a poor start to 2006. He was still recovering from an ankle injury and also suffering from back and leg pain and lack of match play. Agassi withdrew from the Australian Open because of the ankle injury, and his back injury and other pains forced him to withdraw from several other events, eventually skipping the entire clay-court season, including the French Open. This caused his ranking to drop out of the top 10 for the last time. Agassi returned for the grass-court season, playing a tune-up, and then Wimbledon. He was defeated in the third round by world no. 2 (and eventual runner-up) Rafael Nadal. Against conventions, Agassi, the losing player, was interviewed on court after the match. At Wimbledon, Agassi announced his plans to retire following the US Open. Agassi played only two events during the summer hard-court season, with his best result being a quarterfinal loss at the Countrywide Classic in Los Angeles to Fernando GonzÃ¡lez of Chile. As a result, he was unseeded at the US Open.

Agassi had a short, but dramatic, run in his final US Open. Because of extreme back pain, Agassi was forced to receive anti-inflammatory injections after every match. After a tough four-set win against Andrei Pavel, Agassi faced eighth-seeded Marcos Baghdatis in the second round, who had earlier advanced to the 2006 Australian Open final and Wimbledon semifinals. Agassi won in five tough sets as the younger Baghdatis succumbed to muscle cramping in the final set. In his last match, Agassi fell to 112th-ranked big-serving Benjamin Becker of Germany in four sets. Agassi received a four-minute standing ovation from the crowd after the match and delivered a retirement speech.

Agassi earned more than US$30million in prize-money during his career, fifth only to Federer, Nadal, Djokovic and Sampras to date. He also earned more than US$25million a year through endorsements during his career, fourth in all sports at the time.

Since retiring after the 2006 US Open, Agassi has participated in a series of charity tournaments and continues his work with his own charity. On September 5, 2007, he was a surprise guest commentator for the Andy Roddick/Roger Federer US Open quarter-final. He played an exhibition match at Wimbledon, teaming with his wife, Steffi Graf, to play with Tim Henman and Kim Clijsters. He played World Team Tennis for the Philadelphia Freedoms in the summer of 2009. At the 2009 French Open, Agassi was on hand to present Roger Federer, who completed his Career Grand Slam by winning the tournament and joined Agassi as one of six men to complete the Career Grand Slam, with the trophy.

Also in 2009 Agassi played at the Outback Champions Series event for the first time. He played the Cancer Treatment Centers of America Tennis Championships at Surprise, Arizona, where he reached the final before bowing to eventual champion Todd Martin. He also announced that he will not be playing the tour on a full-time basis, and played the tournament as a favor to long-time friend Jim Courier. Agassi returned to the tour renamed for the PowerShares Series in 2011 and participated in a total of seven events while winning two. Agassi beat Courier in the final of the Staples Champions Cup in Boston and later defeated Sampras at the CTCA Championships at his hometown Las Vegas.

In 2012 Agassi took part in five tournaments, winning three of those. In November, at first he won BILT Champions Showdown in San Jose, beating John McEnroe in the final. The following day, he defended his title of the CTCA Championships, while defeating Courier in the decisive match. In the series season finale, he beat Michael Chang for the Acura Champions Cup. The series and Agassi came back to action in 2014. Agassi won both tournaments he participated in. At the Camden Wealth Advisors Cup's final in Houston, Agassi beat James Blake for a rematch of their 2005 US Open quarterfinal. He defeated Blake again in Portland to win the title of the Cancer Treatment Centers of America Championships.

In 2009 in Macau Agassi and Sampras met for the first time on court since the 2002 US Open final. Sampras won the exhibition in three sets. The rivalry between the former champions headlined sports media again in March 2010 after the two participated in the "Hit for Haiti" charity event organized to raise money for the victims of the earthquake. Partnered with Roger Federer and Rafael Nadal, the old rivals began making jokes on each other what ended up with Sampras intentionally striking a serve at Agassi's body. After the event Agassi admitted that he had crossed the line with his jokes and publicly apologized to Sampras. Agassi and Sampras met again one year later for an exhibition match at Madison Square Garden in New York in front of 19 000 spectators as Sampras defeated Agassi in two sets. On March 3, 2014 Agassi and Sampras squared off for an exhibition in London for the annual World Tennis Day. This time it was Agassi who came out on top in two straight sets.

Early in his career, Agassi would look to end points quickly by playing first-strike tennis, typically by inducing a weak return with a deep, hard shot, and then playing a winner at an extreme angle. His groundstrokes, return of serve, baseline game, phenomenal hand-eye coordination and keen sense of anticipation were always among the best in the game. On the rare occasion that he charged the net, Agassi liked to take the ball in the air and hit a swinging volley for the winner. His favored groundstroke was his flat, accurate two-handed backhand, hit well cross-court but in particular down the line. His forehand was nearly as strong, in particular his inside-out forehand to the ad court.

Agassi's strength was in dictating play from the back of the court. While he was growing up, his father and Nick Bollettieri trained him in this way. When in control of a point, Agassi would often pass up an opportunity to attempt a winner and hit a conservative shot to minimize his errors, and to make his opponent run more. This change to more methodical, less aggressive baseline play was largely initiated by his longtime coach, Brad Gilbert, in their first year together in 1994. Gilbert encouraged Agassi to wear out opponents with his deep, flat groundstrokes and to use his fitness to win attrition wars, and noted Agassi's two-handed backhand down the line as his very best shot. A signature play later in his career was a change up drop shot to the deuce court after deep penetrating groundstrokes. This would often be followed by a passing shot or lob if the opponent was fast enough to retrieve it.

Agassi's serve was never the strength of his game, but it improved steadily over the course of his career to being above average. He often used his hard slice serve to the deuce service box to send his opponent off the court, followed by a shot to the opposite corner. Agassi's service speed when hitting a flat first serve would range between . His second serve usually was a heavy "kick" serve in the mid-80s range.

Agassi was raised on hardcourts, but found much of his early major-tournament success on the red clay of Roland Garros, reaching two consecutive finals there early in his career. His first major win was at the slick grass of Wimbledon in 1992, a tournament that he professed to hating at the time. His strongest surface over the course of his career, was indeed hardcourt, where he won six of his eight majors.

Agassi established a limited liability company named Andre Agassi Ventures (formerly named Agassi Enterprises). Agassi, along with five athlete partners (including Wayne Gretzky, Joe Montana, Shaquille O'Neal, Ken Griffey, Jr., and Monica Seles) opened a chain of sports-themed restaurant named Official All Star CafÃ© in April 1996. The restaurant closed down in 2001. In 1999, he paid $1 million for a 10 percent stake in Nevada First Bank and made a $10 million profit when it was sold to Western Alliance Bancorp in 2006. In 2002, he joined the Tennis Channel to promote the channel to consumers and cable and satellite industry, and made an equity investment in the network. After meeting chef Michael Mina at one of his restaurants in San Francisco, Agassi partnered with him in 2002 to start Mina Group Inc. and opened 18 concept restaurants in San Francisco, San Jose, Dana Point, Atlantic City and Las Vegas. Agassi was an equity investor of a group that acquired Golden Nugget Las Vegas and Golden Nugget Laughlin from MGM Mirage for $215 million in 2004. One year later, the group sold the hotel-casino to Landry's, Inc. for $163 million in cash and $182 million in assumed debt. In 2007, he sat on the board of Meadows Bank, an independent bank in Nevada. He has invested in start-up companies backed by Allen & Company.

Agassi and Graf formed a company called Agassi Graf Holdings. They invested in PURE, a nightclub at Caesars Palace, which opened in 2004, and sold it to Angel Management Group in 2010. In August 2006, Agassi and Graf developed a joint venture with high-end furniture maker Kreiss Enterprises. They launched a furniture line called Agassi Graf Collection. In September, Agassi and Graf, through their company Agassi Graf Development LLC, along with Bayview Financial LP, finalized an agreement to develop a condominium hotel, Fairmont Tamarack, at Tamarack Resort in Donnelly, Idaho. Due to difficult market conditions and delays, they withdrew from the project in 2009. The group still own three small chunks of land. In September, they collaborated with Steve Case's Exclusive Resorts to co-develop luxury resorts and design Agassi-Graf Tennis and Fitness Centers.

They also invested in online ticket reseller viagogo in 2009 and both serve as board members and advisors of the company.

In October 2012, Village Roadshow and investors including Agassi and Graf announced plans to build new water park called Wet'n'Wild Las Vegas in Las Vegas. Village Roadshow has a 51% stake in the park while Agassi, Graf, and other private investors hold the remaining 49%. The park opened in May 2013.

IMG managed Agassi from the time he turned pro in 1986 through January 2000, before switching to SFX Sports Group. His business manager, lawyer, and agent was childhood friend Perry Rogers, but they have been estranged since 2008. In 2009, he and Graf signed with CAA.

Agassi used Prince Graphite racket early in his career. He signed a $7 million endorsement contract with Belgian tennis racquet makers Donnay. He later switched to Head Ti Radical racket and Head's LiquidMetal Radical racket, having signed a multi-million dollar endorsement deal with Head in 1993. He renewed his contract in 1999 and in November 2003, he signed a lifetime agreement with Head. He also endorses Penn tennis balls. On July 25, 2005 Agassi left Nike after 17 years and signed an endorsement deal with Adidas.  A major reason for Agassi leaving Nike was because Nike refused to donate to Agassi's charities, and Adidas was more than happy to do so. On May 13, 2013 Agassi rejoined Nike.

Agassi was sponsored by DuPont, Ebel, Mountain Dew in 1993, Mazda in 1997, Kia Motors in 2002, American Express and Deutsche Bank in 2003. In 1990, he appeared in a television commercial for Canon Inc., promoting the Canon EOS Rebel camera. Between 1999 and 2000, he signed a multimillion-dollar, multiyear endorsement deal with Schick and became the worldwide spokesman for the company. Agassi signed a multiyear contract with Twinlab and promoted the company's nutritional supplements. In mid-2003, he was named the spokesman of Aramis Life, a fragrance by Aramis and signed a five-year deal with the company. In March 2004, he signed a ten-year agreement worth $1.5 million a year with 24 Hour Fitness, which will open five Andre Agassi fitness centers by year-end. Prior to the 2012 Australian Open, Agassi and Australian winemaker Jacobs Creek announced a three-year partnership and created the Open Film Series to " personal stories about the life defining moments that shaped his character on and off the court." In 2007, watchmaker Longines named Agassi as their brand ambassador.

Agassi and his mother appeared in a Got Milk? advertisement in 2002.

Agassi has appeared in many advertisements and television commercials with Graf. They both endorsed Deutsche Telekom in 2002, Genworth Financial and Canon Inc. in 2004, LVMH MoÃ«t Hennessy in 2007, and Nintendo Wii and Wii Fit U and Longines in 2013.


In the early 90s Agassi dated American entertainer Barbra Streisand. Writing about the relationship in his 2009 autobiography, he said, "We agree that we're good for each other, and so what if she's twenty-eight years older? We're simpatico, and the public outcry only adds spice to our connection. It makes our friendship feel forbidden, taboo  another piece of my overall rebellion. Dating Barbra Streisand is like wearing Hot Lava."

Agassi was married to Brooke Shields from 1997 to 1999.

He married Steffi Graf on October 22, 2001 at their Las Vegas home, Graf being advanced in her pregnancy. They have two children: son Jaden Gil (born October 26, 2001) and daughter Jaz Elle (born October 3, 2003). Agassi has said that he and Graf are not pushing their children toward becoming tennis players. The Graf-Agassi family resides in Summerlin, a community in the Las Vegas Valley.

Long-time trainer Gil Reyes has been called one of Agassi's closest friends; some have described him as being a "father figure" to Agassi. In 2012, Agassi and Reyes introduced their own line of fitness equipment, BILT By Agassi and Reyes.

In December 2008, Agassi's childhood friend and former business manager, Perry Rogers, sued Graf for $50,000 in management fees he claimed that she owed him.

Agassi's autobiography, Open (written with assistance from J. R. Moehringer), was published in November 2009. In it, Agassi admitted that he used and tested positive for methamphetamine in 1997. In response to this revelation, Roger Federer declared himself shocked and disappointed, while Marat Safin argued that Agassi should return his prize money and be stripped of his titles. In an exclusive interview with CBS, Agassi justified himself and asked for understanding, saying that  "It was a period in my life where I needed help." He also revealed that he had always hated tennis during his career because of the constant pressure it exerted on him. He also revealed he wore a hairpiece earlier in his career and thought Pete Sampras was "robotic". The book reached No. 1 on the New York Times Best Seller list and received favorable reviews. It won the Autobiography category of the 2010 British Sports Book Awards.

Agassi has donated more than $100,000 to Democratic candidates. On September 1, 2010, when he appeared on daily WNYC public radio program "The Brian Lehrer Show," he stated that he is a registered Independent.

Agassi founded the Andre Agassi Charitable Association in 1994, which assists Las Vegas' young people. He was awarded the ATP Arthur Ashe Humanitarian award in 1995 for his efforts to help disadvantaged youth. He is regularly cited as the most charitable and socially involved player in professional tennis. It has also been claimed that he may be the most charitable athlete of his generation.

Agassi's charities help in assisting children reach their athletic potential. His Boys & Girls Club sees 2,000 children throughout the year and boasts a world-class junior tennis team. It also has a basketball program (the Agassi Stars) and a rigorous system that encourages a mix of academics and athletics.

In 2001, Agassi opened the Andre Agassi College Preparatory Academy in Las Vegas, a tuition-free charter school for at-risk children in the area. He personally donated $35 million to the school. In 2009, the graduating class had 100 percent graduation rate and expected a 100 percent college acceptance rate. Among other child-related programs that Agassi supports through his Andre Agassi Charitable Foundation is Clark County's only residential facility for abused and neglected children, Child Haven. In 1997, Agassi donated funding to Child Haven for a six-room classroom building now named the Agassi Center for Education. His foundation also provided $720,000 to assist in the building of the Andre Agassi Cottage for Medically Fragile Children. This 20-bed facility opened in December 2001, and accommodates developmentally delayed or handicapped children and children quarantined for infectious diseases.

In 2007, Agassi, Muhammad Ali, Lance Armstrong, Warrick Dunn, Jeff Gordon, Mia Hamm, Tony Hawk, Andrea Jaeger, Jackie Joyner-Kersee, Mario Lemieux, Alonzo Mourning and Cal Ripken, Jr. founded the charity Athletes for Hope, which helps professional athletes get involved in charitable causes and aims to inspire all people to volunteer and support their communities.

Agassi created the Canyon-Agassi Charter School Facilities Fund. The Fund is an investment initiative for social change, focusing on the "nationwide effort to move charters from stopgap buildings into permanent campuses." It funded the Southwest Detroit Lighthouse Charter Academy, which opened in September 2013.

In September 2013, the Andre Agassi Foundation for Education formed a partnership V20 Foods to launch Box Budd!es, a line of kids' healthy snacks. All proceeds go to the Foundation.

In February 2014, Agassi remodeled the vacant University of Phoenix building as a new school called the Doral Academy West through the Canyon-Agassi Charter School Facilities Fund.  Doral Academy opened in August 2014. The Fund purchased a 4.6-acre plot in Henderson, Nevada to house the Somerset Academy of Las Vegas, which will relocate from its campus inside a church.

ITF World Champion: 1999.
ATP Player of the Year: 1999.
ATP Most Improved Player: 1988, 1998

In 1992, Agassi was named the BBC Overseas Sports Personality of the Year.
In 2010, Sports Illustrated named Agassi the 7th greatest male player of all time. 
On July 9, 2011, Agassi was inducted into the International Tennis Hall of Fame at a ceremony in Newport, Rhode Island.

Wimbledon 2000 Semi-Final â€“ Agassi vs. Rafter (2003) Starring: Andre Agassi, Patrick Rafter; Standing Room Only, DVD Release Date: August 16, 2005, Run Time: 213minutes, .
 Charlie Rose, Inc., DVD Release Date: August 15, 2006, Run Time: 57minutes.
Wimbledon: The Record Breakers (2005) Starring: Andre Agassi, Boris Becker; Standing Room Only, DVD Release Date: August 16, 2005, Run Time: 52minutes, .


Andre Agassi Tennis for the SNES, Sega Genesis, Sega Game Gear, Master System, and Mobile phone
Agassi Tennis Generation for PS2 and GBA
Smash Court Pro Tournament for PS2
Top Spin 4 (On cover of game) for Xbox 360, PlayStation 3 and Wii


List of Grand Slam Men's Singles champions
Agassiâ€“Sampras rivalry
Tennis male players statistics
ATP World Tour records
Tennis records of All Time - Men's Singles
Tennis records of the Open Era â€“ Men's Singles

, 2004


The Austroasiatic languages, in recent classifications synonymous with Monâ€“Khmer, are a large language family of continental Southeast Asia, also scattered throughout India, Bangladesh, and the southern border of China. The name Austroasiatic comes from the Latin words for "south" and "Asia", hence "South Asia". Of these languages, only Khmer, Vietnamese, and Mon have a long-established recorded history, and only Vietnamese and Khmer have official status (in Vietnam and Cambodia, respectively). The rest of the languages are spoken by minority groups. Ethnologue identifies 168 Austroasiatic languages. These form thirteen established families (plus perhaps Shompen, which is poorly attested, as a fourteenth), which have traditionally been grouped into two, as Monâ€“Khmer and Munda. However, one recent classification posits three groups (Munda, Nuclear Mon-Khmer and  Khasi-Khmuic) while another has abandoned Monâ€“Khmer as a taxon altogether, making it synonymous with the larger family.

Austroasiatic languages have a disjunct distribution across India, Bangladesh and Southeast Asia, separated by regions where other languages are spoken. They appear to be the autochthonous languages of Southeast Asia, with the neighboring Indo-Aryan, Taiâ€“Kadai, Dravidian, Austronesian, and Tibeto-Burman languages being the result of later migrations.

The Austroasiatic languages are well known for having a "sesquisyllabic" pattern, with basic nouns and verbs consisting of a reduced minor syllable plus a full syllable. Many of them also have infixes. The Austroasiatic languages are further characterized as having unusually large vowel inventories and employing some sort of register contrast, either between modal (normal) voice and breathy (lax) voice or between modal voice and creaky voice. Languages in the Pearic branch and some in the Vietic branch can have a three- or even four-way voicing contrast. However, some Austroasiatic languages have lost the register contrast by evolving more diphthongs or in a few cases, such as Vietnamese, tonogenesis.

Much work has been done on the reconstruction of Proto-Monâ€“Khmer in Harry L. Shorto's Monâ€“Khmer Comparative Dictionary. Little work has been done on the Munda languages, which are not well documented. With their demotion from a primary branch, Proto-Monâ€“Khmer becomes synonymous with Proto-Austroasiatic.

Paul Sidwell (2005) reconstructs the consonant inventory of Proto-Monâ€“Khmer as follows:
This is identical to earlier reconstructions except for .   is better preserved in the Katuic languages, which Sidwell has specialized in. Sidwell (2011) suggests that the likely homeland of Austroasiatic is the middle Mekong, in the area of the Bahnaric and Katuic languages (approximately where modern Laos, Thailand, and Cambodia come together), and that the family is not as old as frequently assumed, dating to perhaps 2000 BCE.

Linguists traditionally recognize two primary divisions of Austroasiatic: the Monâ€“Khmer languages of Southeast Asia, Northeast India and the Nicobar Islands, and the Munda languages of East and Central India and parts of Bangladesh. However, no evidence for this classification has ever been published.

Each of the families that is written in boldface type below is accepted as a valid clade. By contrast, the relationships between these families within Austroasiatic is debated. In addition to the traditional classification, two recent proposals are given, neither of which accept traditional "Monâ€“Khmer" as a valid unit. However, little of the data used for competing classifications has ever been published, and therefore cannot be evaluated by peer review.

In addition, there are suggestions that additional branches of Austroasiatic might be preserved in substrata of Acehnese in Sumatra (Diffloth), the Chamic languages of Vietnam, and the Land Dayak languages of Borneo (Adelaar 1995).


Paul Sidwell (2009a), in a lexicostatistical comparison of 36 languages which are well-known enough to exclude loan words, finds little evidence for internal branching, though he did find an area of increased contact between the Bahnaric and Katuic languages, such that languages of all branches apart from the geographically distant Munda and Nicobarese show greater similarity to Bahnaric and Katuic the closer they are to those branches, without any noticeable innovations common to Bahnaric and Katuic. He therefore takes the conservative view that the thirteen branches of Austroasiatic should be treated as equidistant on current evidence. Sidwell & Blench (2011) discuss this proposal in more detail, and note that there is good evidence for a Khasiâ€“Palaungic node, which could also possibly be closely related to Khmuic. If this would the case, Sidwell & Blench suggest that Khasic may have been an early offshoot of Palaungic that had spread westward. Sidwell & Blench (2011) suggest Shompen as an additional branch, and believe that a Vieto-Katuic connection is worth investigating. In general, however, the family is thought to have diversified too quickly for a deeply nested structure to have developed, since Proto-Austroasiatic speakers are believed by Sidwell to have radiated out from the central Mekong River valley relatively quickly.
Roger Blench (2009) also proposes that there may have been other primary branches of Austroasiatic that are now extinct, based on substrate evidence in modern-day languages.
Pre-Chamic languages (the languages of coastal Vietnam prior to the Chamic migrations). Chamic has various Austroasiatic loanwords that cannot be clearly traced to existing Austroasiatic branches (Sidwell 2006).
Acehnese substratum (Sidwell 2006). Acehnese has many basic words that are of Austroasiatic origin, suggesting that either Austronesian speakers may have absorbed earlier Austroasiatic residents in northern Sumatra, or that words may have been borrowed from Austroasiatic languages in southern Vietnam â€” or perhaps a combination of both.
Bornean substrate languages (Blench 2010). Blench cites Austroasiatic-origin words in modern-day Bornean branches such as Land Dayak (Bidayuh, Dayak Bakatiq, etc.), Dusunic (Central Dusun, Visayan, etc.), Kayan, and Kenyah, noting especially resemblances with Aslian. As further evidence for his proposal, Blench also cites ethnographic evidence such as musical instruments in Borneo shared in common with Austroasiatic-speaking groups in mainland Southeast Asia.
Lepcha substratum ("Rongic"). Many words of Austroasiatic origin have been noticed in Lepcha, suggesting a Tibeto-Burman superstrate laid over an Austroasiatic substrate. Blench (2013) calls this branch "Rongic" based on the Lepcha autonym RÃ³ng.

Other languages with proposed Austroasiatic substrata are:
Jiamao, based on evidence from the register system of Jiamao, a Hlai language (Thurgood 1992).  Jiamao is known for its highly aberrant vocabulary.

Diffloth compares reconstructions of various clades, and attempts to classify them based on shared innovations, though like other classifications the evidence has not been published. As a schematic, we have:
Or in more detail,

Munda languages (India)
Koraput: 7 languages
Core Munda languages
Kharianâ€“Juang: 2 languages
North Munda languages
Korku
Kherwarian: 12 languages

Khasiâ€“Khmuic languages (Northern Monâ€“Khmer)
Khasian: 3 languages of eastern India and Bangladesh
Palaungo-Khmuic languages
Khmuic: 13 languages of Laos and Thailand

Palaungo-Pakanic languages
Pakanic or Palyu: 4 or 5 languages of southern China and Vietnam
Palaungic: 21 languages of Burma, southern China, and Thailand

Nuclear Monâ€“Khmer languages
Khmero-Vietic languages (Eastern Monâ€“Khmer)

Vieto-Katuic languages ?
Vietic: 10 languages of Vietnam and Laos, including the Vietnamese language, which has the most speakers of any Austroasiatic language. These are the only Austroasiatic languages to have highly developed tone systems.
Katuic: 19 languages of Laos, Vietnam, and Thailand.

Khmero-Bahnaric languages
Bahnaric: 40 languages of Vietnam, Laos, and Cambodia.
Khmeric languages
The Khmer dialects of Cambodia, Thailand, and Vietnam.
Pearic: 6 languages of Cambodia.

Nico-Monic languages (Southern Monâ€“Khmer)
Nicobarese: 6 languages of the Nicobar Islands, a territory of India.

Asli-Monic languages
Aslian: 19 languages of peninsular Malaysia and Thailand.
Monic: 2 languages, the Mon language of Burma and the Nyahkur language of Thailand.

This family tree is consistent with recent studies of migration of Y-Chromosomal haplogroup O2a1-M95. However, the dates obtained from by Zhivotovsky method DNA studies are several times older than that given by linguists. The route map of the people with haplogroup O2a1-M95, speaking this language can be seen in this link. Other geneticists criticise the Zhivotovsky method.

Peiros is a lexicostatistic classification, based on percentages of shared vocabulary. This means that a language may appear to be more distantly related than it actually is due to language contact. Indeed, when Sidwell (2009a) replicated Peiros's study with languages known well enough to account for loans, he did not find the internal (branching) structure below.

Nicobarese
Mundaâ€“Khmer
Munda
Monâ€“Khmer
Khasi
Nuclear Monâ€“Khmer
Mangic (Mang + Palyu) (perhaps in Northern MK)
Vietic (perhaps in Northern MK)
Northern Monâ€“Khmer
Palaungic
Khmuic
Central Monâ€“Khmer
Khmer dialects
Pearic
Asli-Bahnaric
Aslian
Monâ€“Bahnaric
Monic
Katuâ€“Bahnaric
Katuic
Bahnaric
Diffloth's widely cited original classification, now abandoned by Diffloth himself, is used in EncyclopÃ¦dia Britannica andâ€”except for the breakup of Southern Monâ€“Khmerâ€”in Ethnologue.

Munda
North Munda
Korku
Kherwarian
South Munda
Khariaâ€“Juang
Koraput Munda
Monâ€“Khmer
Eastern Monâ€“Khmer
Khmer (Cambodian)
Pearic
Bahnaric
Katuic
Vietic (includes Vietnamese)
Northern Monâ€“Khmer
Khasi (Meghalaya, India)
Palaungic
Khmuic
Southern Monâ€“Khmer
Mon
Aslian (Malaya)
Nicobarese (Nicobar Islands)

Other than Latin-based alphabets, many Austroasiatic languages are written with the ancient Khmer alphabet, Thai alphabet and Lao alphabet. Vietnamese divergently had an indigenous script based on Chinese logographic writing. This has since been supplanted by the Latin alphabet in the 20th century. The following are examples of past-used alphabets or current alphabets of Austroasiatic languages.
Chá»¯ NÃ´m
Khmer alphabet
Ol Chiki alphabet (Santali alphabet)
Sorang Sompeng alphabet (Sora alphabet)
Varang Kshiti (Ho alphabet)
Khom script (used for a short period in the early 20th century for indigenous languages in Laos)

Austric languages


Adams, K. L. (1989). Systems of numeral classification in the Monâ€“Khmer, Nicobarese and Aslian subfamilies of Austroasiatic. Canberra, A.C.T., Australia: Dept. of Linguistics, Research School of Pacific Studies, Australian National University. ISBN 0-85883-373-5
Bradley, David (2012). "", in Rint Sybesma (ed.), Encyclopedia of Chinese Language and Linguistics.
Chakrabarti, Byomkes. (1994). A Comparative Study of Santali and Bengali.
Diffloth, GÃ©rard (2005). "The contribution of linguistic palaeontology and Austro-Asiatic". in Laurent Sagart, Roger Blench and Alicia Sanchez-Mazas, eds. The Peopling of East Asia: Putting Together Archaeology, Linguistics and Genetics. 77â€“80. London: Routledge Curzon. ISBN 0-415-32242-1
Filbeck, D. (1978). T'in: a historical study. Pacific linguistics, no. 49. Canberra: Dept. of Linguistics, Research School of Pacific Studies, Australian National University. ISBN 0-85883-172-4
Hemeling, K. (1907). Die Nanking Kuanhua. (German language)
Jenny, Mathias and Paul Sidwell, eds (2015). . Leiden: Brill.
Peck, B. M., Comp. (1988). An Enumerative Bibliography of South Asian Language Dictionaries.
Peiros, Ilia. 1998. Comparative Linguistics in Southeast Asia. Pacific Linguistics Series C, No. 142. Canberra: Australian National University.
Shorto, Harry L. edited by Sidwell, Paul, Cooper, Doug and Bauer, Christian (2006). A Monâ€“Khmer comparative dictionary. Canberra: Australian National University. Pacific Linguistics. ISBN 0-85883-570-3
Shorto, H. L. Bibliographies of Monâ€“Khmer and Tai Linguistics. London oriental bibliographies, v. 2. London: Oxford University Press, 1963.
Sidwell, Paul (2005). "". In Sidwell, ed., SEALSXV: papers from the 15th meeting of the Southeast Asian Linguistic Society.
Sidwell, Paul (2009a). . Keynote address, SEALS, XIX.
Sidwell, Paul (2009b). Classifying the Austroasiatic languages: history and state of the art. LINCOM studies in Asian linguistics, 76. Munich: Lincom Europa.
Zide, Norman H., and Milton E. Barker. (1966) Studies in Comparative Austroasiatic Linguistics, The Hague: Mouton (Indo-Iranian monographs, v. 5.).

Mann, Noel, Wendy Smith and Eva Ujlakyova. 2009.  Chiang Mai: Payap University.


 (from Wiktionary's )
 at the Linguist List MultiTree Project (not functional as of 2014): Genealogical trees attributed to Sebeok 1942, Pinnow 1959, Diffloth 2005, and Matisoff 2006

 at SEAlang


Afroasiatic (Afro-Asiatic), also known as Afrasian and traditionally as Hamito-Semitic (Chamito-Semitic), is a large language family of several hundred related languages and dialects. It comprises about 300 or so living languages and dialects, according to the 2009 Ethnologue estimate. It includes languages spoken predominantly in the Middle East, North Africa, the Horn of Africa, and parts of the Sahel. The Afroasiatic family is significant to the field of historical linguistics as possessing the longest recorded history of any language family.

Afroasiatic languages have 350+ million native speakers, the fourth largest number of any language family. The most widely spoken Afroasiatic language, Arabic (including literary Arabic and the spoken colloquial varieties), has around 200 to 230 million native speakers, living mostly in the Middle East and in parts of North Africa. Berber (including all its varieties) is spoken in Morocco, Algeria, Libya, Tunisia, northern Mali, and northern Niger by about 25 to 35 million people. Other widely spoken Afroasiatic languages include:

Hausa, the dominant language of northern Nigeria and southern Niger, spoken as a first language by 25 million people and used as a lingua franca by another 20 million across West Africa and the Sahel
Oromo of Ethiopia and Kenya, with about 33 million speakers total
Amharic of Ethiopia, with over 25 million native speakers, not including the millions of other Ethiopians speaking it as a second language
Somali, spoken by 15.5 million people in Greater Somalia 
Modern Hebrew, spoken by about seven million people worldwide
Modern Aramaic, spoken by about 550,000 people worldwide. This isn't just one language â€” It includes a number of subdivisions, with Assyrian Neo-Aramaic being the most spoken variety (232,300).

In addition to languages spoken today, Afroasiatic includes several important ancient languages, such as Ancient Egyptian, Akkadian, Biblical Hebrew, and Old Aramaic.

The Afroasiatic language family was originally referred to as "Hamito-Semitic", a term introduced in the 1860s by the German scholar Karl Richard Lepsius. The name was later popularized by Friedrich MÃ¼ller in his Grundriss der Sprachwissenschaft (Vienna 1876-88).

The term "Afroasiatic" (often now spelled as "Afro-Asiatic") was later coined by Maurice Delafosse (1914). However, it did not come into general use until Joseph Greenberg (1963) formally proposed its adoption. In doing so, Greenberg sought to emphasize the fact that Afroasiatic was represented transcontinentally, in both Africa and Asia.

Individual scholars have also called the family "Erythraean" (Tucker 1966) and "Lisramic" (Hodge 1972). In lieu of "Hamito-Semitic", the Russian linguist Igor Diakonoff later suggested the term "Afrasian", meaning "half African, half Asiatic", in reference to the geographic distribution of the family's constituent languages.

The term "Hamito-Semitic" remains in use in the academic traditions of some European countries. 


The Afroasiatic language family is usually considered to include the following branches:
Berber
Chadic
Cushitic
Egyptian
Omotic
Semitic

Although there is general agreement on these six families, there are some points of disagreement among linguists who study Afroasiatic. In particular:
The Omotic language branch is the most controversial member of Afroasiatic, because the grammatical formatives that most linguists have given greatest weight in classifying languages in the family "are either absent or distinctly wobbly" (Hayward 1995). Greenberg (1963) and others considered it a subgroup of Cushitic, whereas others have raised doubts about it being part of Afroasiatic at all (e.g. Theil 2006).
The Afroasiatic identity of Ongota is also broadly questioned, as is its position within Afroasiatic among those who accept it, due to the "mixed" appearance of the language and a paucity of research and data. Harold Fleming (2006) proposes that Ongota constitutes a separate branch of Afroasiatic. Bonny Sands (2009) believes the most convincing proposal is by SavÃ  and Tosco (2003), namely that Ongota is an East Cushitic language with a Nilo-Saharan substratum. In other words, it would appear that the Ongota people once spoke a Nilo-Saharan language but then shifted to speaking a Cushitic language but retained some characteristics of their earlier Nilo-Saharan language.
Beja is sometimes listed as a separate branch of Afroasiatic but is more often included in the Cushitic branch, which has a high degree of internal diversity.
Whether the various branches of Cushitic actually form a language family is sometimes questioned, but not their inclusion in Afroasiatic itself.
There is no consensus on the interrelationships of the five non-Omotic branches of Afroasiatic (see "Subgrouping" below). This situation is not unusual, even among long-established language families: there are also many disagreements concerning the internal classification of the Indo-European languages, for instance.
Meroitic has been proposed as an unclassified Afroasiatic language, because it shares the phonotactics characteristic of the family, but there is not enough evidence to secure a classification.

In the 9th century, the Hebrew grammarian Judah ibn Quraysh of Tiaret in Algeria was the first to link two branches of Afroasiatic together; he perceived a relationship between Berber and Semitic. He knew of Semitic through his study of Arabic, Hebrew, and Aramaic.

In the course of the 19th century, Europeans also began suggesting such relationships. In 1844, Theodor Benfey suggested a language family consisting of Semitic, Berber, and Cushitic (calling the latter "Ethiopic"). In the same year, T.N. Newman suggested a relationship between Semitic and Hausa, but this would long remain a topic of dispute and uncertainty.

Friedrich MÃ¼ller named the traditional "Hamito-Semitic" family in 1876 in his Grundriss der Sprachwissenschaft. He defined it as consisting of a Semitic group plus a "Hamitic" group containing Egyptian, Berber, and Cushitic; he excluded the Chadic group. These classifications relied in part on non-linguistic anthropological and racial arguments that have largely been discredited (see Hamitic hypothesis).

Leo Reinisch (1909) proposed linking Cushitic and Chadic, while urging a more distant affinity to Egyptian and Semitic, thus foreshadowing Greenberg, but his suggestion found little resonance.

Marcel Cohen (1924) rejected the idea of a distinct Hamitic subgroup and included Hausa (a Chadic language) in his comparative Hamito-Semitic vocabulary.

Joseph Greenberg (1950) strongly confirmed Cohen's rejection of "Hamitic", added (and sub-classified) the Chadic branch, and proposed the new name "Afroasiatic" for the family. Nearly all scholars have accepted Greenberg's classification.

In 1969, Harold Fleming proposed that what had previously been known as Western Cushitic is an independent branch of Afroasiatic, suggesting for it the new name Omotic. This proposal and name have met with widespread acceptance.

Several scholars, including Harold Fleming and Robert Hetzron, have since questioned the traditional inclusion of Beja in Cushitic.

Glottolog does not accept that the inclusion or even unity of Omotic has been established, nor that of Ongota or the unclassified Kujarge, and so splits off the following groups as small families:
South Omotic, Mao, Dizoid, Gongaâ€“Gimojan (North Omotic apart from the preceding), Ongota, Kujarge.

Little agreement exists on the subgrouping of the five or six branches of Afroasiatic: Semitic, Egyptian, Berber, Chadic, Cushitic, and Omotic. However, Christopher Ehret (1979), Harold Fleming (1981), and Joseph Greenberg (1981) all agree that the Omotic branch split from the rest first.

Otherwise:
Paul Newman (1980) groups Berber with Chadic and Egyptian with Semitic, while questioning the inclusion of Omotic in Afroasiatic. Rolf Theil (2006) concurs with the exclusion of Omotic, but does not otherwise address the structure of the family.
Harold Fleming (1981) divides non-Omotic Afroasiatic, or "Erythraean", into three groups, Cushitic, Semitic, and Chadic-Berber-Egyptian. He later added Semitic and Beja to Chadic-Berber-Egyptian and tentatively proposed Ongota as a new third branch of Erythraean. He thus divided Afroasiatic into two major branches, Omotic and Erythraean, with Erythraean consisting of three sub-branches, Cushitic, Chadic-Berber-Egyptian-Semitic-Beja, and Ongota.
Like Harold Fleming, Christopher Ehret (1995: 490) divides Afroasiatic into two branches, Omotic and Erythrean. He divides Omotic into two branches, North Omotic and South Omotic. He divides Erythrean into Cushitic, comprising Beja, Agaw, and East-South Cushitic, and North Erythrean, comprising Chadic and "Boreafrasian." According to his classification, Boreafrasian consists of Egyptian, Berber, and Semitic.
Vladimir Orel and Olga Stolbova (1995) group Berber with Semitic and Chadic with Egyptian. They split up Cushitic into five or more independent branches of Afroasiatic, viewing Cushitic as a Sprachbund rather than a language family.
Igor M. Diakonoff (1996) subdivides Afroasiatic in two, grouping Berber, Cushitic, and Semitic together as East-West Afrasian (ESA), and Chadic with Egyptian as North-South Afrasian (NSA). He excludes Omotic from Afroasiatic.
Lionel Bender (1997) groups Berber, Cushitic, and Semitic together as "Macro-Cushitic". He regards Chadic and Omotic as the branches of Afroasiatic most remote from the others.
Alexander Militarev (2000), on the basis of lexicostatistics, groups Berber with Chadic and both more distantly with Semitic, as against Cushitic and Omotic. He places Ongota in South Omotic.

Afroasiatic is one of the four language families of Africa identified by Joseph Greenberg in his book The Languages of Africa (1963). It is the only one that extends outside of Africa, via the Semitic branch.

There are no generally accepted relations between Afroasiatic and any other language family. However, several proposals grouping Afroasiatic with one or more other language families have been made. The best-known of these are the following:
Hermann MÃ¶ller (1906) argued for a relation between Semitic and the Indo-European languages. This proposal was accepted by some linguists (e.g. Holger Pedersen and Louis Hjelmslev) but has little currency today. (For a fuller account, see Indo-Semitic languages.)
Apparently influenced by MÃ¶ller (a colleague of his at the University of Copenhagen), Holger Pedersen included Hamito-Semitic (the term replaced by Afroasiatic) in his proposed Nostratic macro-family (cf. Pedersen 1931:336â€“338), which also included the Indo-European, Finno-Ugric, Samoyed, Turkish, Mongolian, Manchu, and Yukaghir languages. This inclusion was retained by subsequent Nostraticists, starting with Vladislav Illich-Svitych and Aharon Dolgopolsky.
Joseph Greenberg (2000â€“2002) did not reject a relationship of Afroasiatic to these other languages, but he considered it more distantly related to them than they were to each other, grouping instead these other languages in a separate macro-family, which he called Eurasiatic, and to which he added Chukotian, Gilyak, Korean, Japanese-Ryukyuan, Eskimoâ€“Aleut, and Ainu.
Most recently, Sergei Starostin's school has accepted Eurasiatic as a subgroup of Nostratic, with Afroasiatic, Dravidian, and Kartvelian in Nostratic outside of Eurasiatic. An even larger Borean group would contain Nostratic as well as DenÃ©-Caucasian and Austric.

The earliest written evidence of an Afroasiatic language is an Ancient Egyptian inscription of c. 3400 BC (5,400 years ago). Symbols on Gerzean pottery resembling Egyptian hieroglyphs date back to c. 4000 BC, suggesting a still earlier possible date. This gives us a minimum date for the age of Afroasiatic. However, Ancient Egyptian is highly divergent from Proto-Afroasiatic (Trombetti 1905: 1â€“2), and considerable time must have elapsed in between them. Estimates of the date at which the Proto-Afroasiatic language was spoken vary widely. They fall within a range between approximately 7,500 BC (9,500 years ago) and approximately 16,000 BC (18,000 years ago). According to Igor M. Diakonoff (1988: 33n), Proto-Afroasiatic was spoken c. 10,000 BC. According to Christopher Ehret (2002: 35â€“36), Proto-Afroasiatic was spoken c. 11,000 BC at the latest and possibly as early as c. 16,000 BC. These dates are older than dates associated with most other proto-languages.

The term Afroasiatic Urheimat (Urheimat meaning "original homeland" in German) refers to the 'hypothetical' place where Proto-Afroasiatic speakers lived in a single linguistic community, or complex of communities, before this original language dispersed geographically and divided into distinct languages. Afroasiatic languages are today primarily spoken in the Middle East, North Africa, the Horn of Africa, and parts of the Sahel.  Their distribution seems to have been influenced by the Saharan pump operating over the last 10,000 years.

There is no agreement on when or where this Urheimat existed. Proposed locations include the Horn of Africa, North Africa, the Eastern Sahara, and the Levant.


Widespread (though not universal) features of the Afroasiatic languages include:
A set of emphatic consonants, variously realized as glottalized, pharyngealized, or implosive.
VSO typology with SVO tendencies.
A two-gender system in the singular, with the feminine marked by the sound /t/.
All Afroasiatic subfamilies show evidence of a causative affix s.
Semitic, Berber, Cushitic (including Beja), and Chadic support possessive suffixes.
Morphology in which words inflect by changes within the root (vowel changes or gemination) as well as with prefixes and suffixes.

One of the most remarkable shared features among the Afroasiatic languages is the prefixing verb conjugation (see table above), with a distinctive pattern of prefixes beginning with /Ê” t n y/, and in particular a pattern whereby third-singular masculine /y-/ is opposed to third-singular feminine and second-singular /t-/.

According to Ehret (1996), tonal languages appear in the Omotic and Chadic branches of Afroasiatic, as well as in certain Cushitic languages. The Semitic, Berber and Egyptian branches generally do not use tones phonemically.

The following are some examples of Afroasiatic cognates, including ten pronouns, three nouns, and three verbs.

Source: Christopher Ehret, Reconstructing Proto-Afroasiatic (Berkeley: University of California Press, 1995).

Note: Ehret does not make use of Berber in his etymologies, stating (1995: 12): "the kind of extensive reconstruction of proto-Berber lexicon that might help in sorting through alternative possible etymologies is not yet available." The Berber cognates here are taken from previous version of table in this article and need to be completed and referenced.

Abbreviations: NOm = 'North Omotic', SOm = 'South Omotic'. MSA = 'Modern South Arabian', PSC = 'Proto-Southern Cushitic', PSom-II = 'Proto-Somali, stage 2'. masc. = 'masculine', fem. = 'feminine', sing. = 'singular', pl. = 'plural'. 1s. = 'first person singular', 2s. = 'second person singular'.

Symbols: Following Ehret (1995: 70), a caron Ë‡ over a vowel indicates rising tone, and a circumflex ^ over a vowel indicates falling tone. V indicates a vowel of unknown timbre. É indicates a glottal stop. * indicates reconstructed forms based on comparison of related languages.
There are two etymological dictionaries of Afroasiatic, one by Christopher Ehret, and one by Vladimir Orel and Olga Stolbova. The two dictionaries disagree on almost everything. The following table contains the thirty roots or so (out of thousands) that represent a fragile consensus of present research:
Some of the main sources for Afroasiatic etymologies include:
Cohen, Marcel. 1947. Essai comparatif sur le vocabulaire et la phonÃ©tique du chamito-sÃ©mitique. Paris: Champion.
Diakonoff, Igor M. et al. 1993â€“1997. "Historical-comparative vocabulary of Afrasian", St. Petersburg Journal of African Studies 2â€“6.
Ehret, Christopher. 1995. Reconstructing Proto-Afroasiatic (Proto-Afrasian): Vowels, Tone, Consonants, and Vocabulary (= University of California Publications in Linguistics 126). Berkeley and Los Angeles: University of California Press.
Orel, Vladimir E. and Olga V. Stolbova. 1995. Hamito-Semitic Etymological Dictionary: Materials for a Reconstruction. Leiden: Brill. ISBN 90-04-10051-2.

African languages
Asian languages
Indo-Semitic languages
Nostratic languages
Proto-Afroasiatic language

Anthony, David. 2007. . Princeton: Princeton University Press.
Barnett, William and John Hoopes (editors). 1995. The Emergence of Pottery. Washington, DC: Smithsonian Institution Press. ISBN 1-56098-517-8
Bender, Lionel et al. 2003. Selected Comparative-Historical Afro-Asiatic Studies in Memory of Igor M. Diakonoff. LINCOM.
Bomhard, Alan R. 1996. Indo-European and the Nostratic Hypothesis. Signum.
Diakonoff, Igor M. 1988. Afrasian Languages. Moscow: Nauka.
Diakonoff, Igor M. 1996. "Some reflections on the Afrasian linguistic macrofamily." Journal of Near Eastern Studies 55, 293.
Diakonoff, Igor M. 1998. "The earliest Semitic society: Linguistic data." Journal of Semitic Studies 43, 209.
Dimmendaal, Gerrit, and Erhard Voeltz. 2007. "Africa". In Christopher Moseley, ed., Encyclopedia of the world's endangered languages.
Ehret, Christopher. 1995. Reconstructing Proto-Afroasiatic (Proto-Afrasian): Vowels, Tone, Consonants, and Vocabulary. Berkeley and Los Angeles: University of California Press.
Ehret, Christopher. 1997.  of "The lessons of deep-time historical-comparative reconstruction in Afroasiatic: reflections on Reconstructing Proto-Afroasiatic: Vowels, Tone, Consonants, and Vocabulary (U.C. Press, 1995)", paper delivered at the Twenty-fifth Annual Meeting of the North American Conference on Afro-Asiatic Linguistics, held in Miami, Florida on 21â€“23 March 1997.
Finnegan, Ruth H. 1970. "Afro-Asiatic languages West Africa". Oral Literature in Africa, pg 558.
Fleming, Harold C. 2006. Ongota: A Decisive Language in African Prehistory. Wiesbaden: Otto Harrassowitz.
Greenberg, Joseph H. 1950.  Southwestern Journal of Anthropology 6, 47-63.
Greenberg, Joseph H. 1955. Studies in African Linguistic Classification. New Haven: Compass Publishing Company. (Photo-offset reprint of the SJA articles with minor corrections.)
Greenberg, Joseph H. 1963. The Languages of Africa. Bloomington: Indiana University. (Heavily revised version of Greenberg 1955.)
Greenberg, Joseph H. 1966. The Languages of Africa (2nd ed. with additions and corrections). Bloomington: Indiana University.
Greenberg, Joseph H. 1981. "African linguistic classification." General History of Africa, Volume 1: Methodology and African Prehistory, edited by Joseph Ki-Zerbo, 292â€“308. Berkeley and Los Angeles: University of California Press.
Greenberg, Joseph H. 2000â€“2002. Indo-European and Its Closest Relatives: The Eurasiatic Language Family, Volume 1: Grammar, Volume 2: Lexicon. Stanford: Stanford University Press.
Hayward, R. J. 1995. "The challenge of Omotic: an inaugural lecture delivered on 17 February 1994". London: School of Oriental and African Studies, University of London.
Heine, Bernd and Derek Nurse. 2000. African Languages, Chapter 4. Cambridge University Press.
Hodge, Carleton T. (editor). 1971. Afroasiatic: A Survey. The Hague â€“ Paris: Mouton.
Hodge, Carleton T. 1991. "Indo-European and Afro-Asiatic." In Sydney M. Lamb and E. Douglas Mitchell (editors), Sprung from Some Common Source: Investigations into the Prehistory of Languages, Stanford, California: Stanford University Press, 141â€“165.
Huehnergard, John. 2004. "Afro-Asiatic." In R.D. Woodard (editor), The Cambridge Encyclopedia of the Worldâ€™s Ancient Languages, Cambridge â€“ New York, 2004, 138â€“159.
Militarev, Alexander. "Towards the genetic affiliation of Ongota, a nearly-extinct language of Ethiopia," 60 pp.nbsp&In Orientalia et Classica: Papers of the Institute of Oriental and Classical Studies, Issue 5. Ðœoscow. (Forthcoming.)
Newman, Paul. 1980. The Classification of Chadic within Afroasiatic. Leiden: Universitaire Pers Leiden.
Ruhlen, Merritt. 1991. A Guide to the World's Languages. Stanford, California: Stanford University Press.
Sands, Bonny. 2009. "Africaâ€™s linguistic diversity". In Language and Linguistics Compass 3.2, 559â€“580.
Theil, R. 2006.  Proceedings from the David Dwyer retirement symposium, Michigan State University, East Lansing, 21 October 2006.
Trombetti, Alfredo. 1905. L'UnitÃ  d'origine del linguaggio. Bologna: Luigi Beltrami.

 at the Linguist List MultiTree Project (not functional as of 2014): Genealogical trees attributed to Delafosse 1914, Greenberg 1950â€“1955, Greenberg 1963, Fleming 1976, Hodge 1976, Orel & Stolbova 1995, Diakonoff 1996â€“1998, Ehret 1995â€“2000, Hayward 2000, Militarev 2005, Blench 2006, and Fleming 2006
, presented by Alexander Militarev at his talk "Genealogical classification of Afro-Asiatic languages according to the latest data" at the conference on the 70th anniversary of V.M. Illich-Svitych, Moscow, 2004;  
, by Alexander Militarev in "Examining the Farming/Language Dispersal Hypothesis", eds. P. Bellwood & C. Renfrew. (McDonald Institute Monographs.) Cambridge: McDonald Institute for Archaeological Research, 2002, p.135-50.
, by Alexander Militarev in "Aspects of Comparative Linguistics", v. 1. Moscow: RSUH Publishers, 2005, pp.339â€“408.
, by Alexander Militarev in "Proceedings of the Barcelona Symposium on comparative Semitic", 19-20/11/2004. Aula Orientalis 23/1-2, 2005, pp.83â€“129.
, by Alexander Militarev in "Papers on Semitic and Afroasiatic Linguistics in Honor of Gene B. Gragg." Ed. by Cynthia L. Miller. Studies in Ancient Oriental Civilization 60. Chicago: The Oriental Institute, 2007, p.139-145.

 by Rolf Theil (2006)
 The North American Conference on Afroasiatic Linguistics, now in its 35th year
 of Roger Blench (with ).


Andorra (; , ), officially the Principality of Andorra (), also called the Principality of the Valleys of Andorra (), is a sovereign landlocked microstate in Southwestern Europe, located in the eastern Pyrenees mountains and bordered by Spain and France. It is the sixth-smallest nation in Europe, having an area of 468km2 (181 sq mi) and a population of ca. 85,000. Its capital Andorra la Vella is the highest capital city in Europe, at an elevation of 1,023 metres (3,356ft) above sea level. The official language is Catalan, although Spanish, Portuguese, and French are also commonly spoken.

Created under a charter in A.D. 988, the present Principality was formed in A.D. 1278. It is known as a principality as it is a monarchy headed by two Co-Princes â€“ the Spanish/Roman Catholic Bishop of Urgell and the President of France.

Andorra's tourism services an estimated 10.2million visitors annually. It is not a member of the European Union, but the euro is the de facto currency. It has been a member of the United Nations since 1993. The people of Andorra have one of the highest life expectancies in the world and as of December 2014, according to The Lancet, have the highest in the world - 81 years in 2013.

The origin of the word Andorra is unknown, although several theories have been formulated. The oldest derivation of the word Andorra is from the Greek historian Polybius (The Histories III, 35, 1) who describes the Andosins, an Iberian Pre-Roman tribe, as historically located in the valleys of Andorra and facing the Carthaginian army in its passage through the Pyrenees during the Punic Wars. The word Andosini or Andosins (AÎ½Î´Î¿ÏƒÎ¯Î½Î¿Ï…Ï‚) may derive from the Basque handia whose meaning is "big" or "giant". The Andorran toponymy shows evidence of Basque language in the area. Another theory suggests that the word Andorra may derive from the old word Anorra that contains the Basque word ur (water).

One theory suggests that Andorra may derive from Arabic al-Darra, whose meaning is "The Forest" (Ø§Ù„Ø¯Ø±Ø§). When the Moors invaded the Iberian Peninsula, the valleys of the Pyrenees were formed by large tracts of forest and other regions and towns, also dominated by Muslims, received this designation.

Other theories suggest that the term derives from the Navarro-Aragonese andurrial, which means "land covered with bushes" or "scrubland".

The folk etymology holds that Charlemagne had called the region as a reference to the Biblical Canaanite valley of Endor or Andor (where the Midianites had been defeated), a name also bestowed by their heir and son Louis le Debonnaire after defeating the Moors in the "wild valleys of Hell".

Tradition holds that Charles the Great (Charlemagne) granted a charter to the Andorran people in return for fighting against the Moors. Overlordship of the territory was by the Count of Urgell and eventually by the bishop of the Diocese of Urgell. In 988, Borrell II, Count of Urgell, gave the Andorran valleys to the Diocese of Urgell in exchange for land in Cerdanya. Since then the Bishop of Urgell, based in Seu d'Urgell, has owned Andorra.

Before 1095 Andorra did not have any type of military protection and the Bishop of Urgell, who knew that the Count of Urgell wanted to reclaim the Andorran valleys, asked for help and protection from the Lord of Caboet. In 1095 the Lord of Caboet and the Bishop of Urgell signed under oath a declaration of their co-sovereignty over Andorra. Arnalda, daughter of Arnau of Caboet, married the Viscount of CastellbÃ² and both became Viscounts of CastellbÃ² and Cerdanya. Years later their daughter, Ermessenda, married Roger Bernat II, the French Count of Foix. They became Roger Bernat II and Ermessenda I, Counts of Foix, Viscounts of CastellbÃ² and Cerdanya, and co-sovereigns of Andorra (shared with the Bishop of Urgell).

In the 11th century a dispute arose between the Bishop of Urgell and the Count of Foix. The conflict was resolved in 1278 with the mediation of Aragon by the signing of the first parÃ©age which provided that Andorra's sovereignty be shared between the count of Foix (whose title would ultimately transfer to the French head of state) and the Bishop of Urgell, in Catalonia. This gave the principality its territory and political form.

With the passage of time the co-title to Andorra passed to the kings of Navarre. After Henry of Navarre became King Henry IV of France, he issued an edict in 1607 that established the head of the French state and the Bishop of Urgell as co-princes of Andorra. In 1812â€“13 the First French Empire annexed Catalonia and divided it in four dÃ©partements, with Andorra being made part of the district of PuigcerdÃ  (dÃ©partement of SÃ¨gre).


Andorra declared war on Imperial Germany during World War I, but did not actually take part in the fighting. It remained in an official state of belligerency until 1958 as it was not included in the Treaty of Versailles.

In 1933 France occupied Andorra as a result of social unrest before elections. On 12 July 1934 adventurer Boris Skossyreff issued a proclamation in Urgell, declaring himself "Boris I, King of Andorra", simultaneously declaring war on the Bishop of Urgell. He was arrested by Spanish authorities on 20 July and ultimately expelled from Spain. From 1936 until 1940 a French detachment was garrisoned in Andorra to prevent influences of the Spanish Civil War and Francoist Spain. Francoist troops reached the Andorran border in the later stages of the war. During World War II, Andorra remained neutral and was an important smuggling route between Vichy France and Spain.

Given its relative isolation, Andorra has existed outside the mainstream of European history, with few ties to countries other than France, Spain and Portugal. In recent times, however, its thriving tourist industry along with developments in transport and communications have removed the country from its isolation. Its political system was modernised in 1993, when it became a member of the United Nations and the Council of Europe.

Andorra is a parliamentary co-principality with the President of France and the Bishop of Urgell (Catalonia, Spain), as co-princes. This peculiarity makes the President of France, in his capacity as Prince of Andorra, an elected reigning monarch, although he is not elected by a popular vote of the Andorran people. The politics of Andorra take place in a framework of a parliamentary representative democracy, whereby the Head of Government is the chief executive, and of a  multi-party system.

The current Head of Government is Antoni MartÃ­ of the Democrats for Andorra (DA). Executive power is exercised by the government. Legislative power is vested in both government and parliament.

The Parliament of Andorra is known as the General Council. The General Council consists of between 28 and 42 Councillors, as the members of the legislative branch are called. The Councillors serve for four-year terms and elections are held between the thirtieth and fortieth days following the dissolution of the previous Council. The Councillors can be elected on two equal constituencies.

Half are elected in equal number from each of the seven administrative parishes and the other half of the Councillors are elected from a single national constituency. 15 days after the election, the Councillors hold their inauguration. During this session, the Syndic General, who is the head of the General Council, and the Subsyndic General, his assistant, are elected. Eight days later, the Council convenes once more. During this session the Head of Government is chosen from among the Councillors.
Candidates for the nomination can be proposed by a minimum of one-fifth of the Councillors. The Council then elects the candidate with the absolute majority of votes to be Head of Government. The Syndic General then notifies the Co-princes who in turn appoint the elected candidate as the Head of Government of Andorra. The General Council is also responsible for proposing and passing laws. Bills may be presented to the Council as Private Members' Bills by three of the Local Parish Councils jointly or by at least one tenth of the citizens of Andorra.

The Council also approves the annual budget of the principality. The government must submit the proposed budget for parliamentary approval at least two months before the previous budget expires. If the budget is not approved by the first day of the next year, the previous budget is extended until a new one is approved. Once any bill is approved, the Syndic General is responsible for presenting it to the Co-princes so that they may sign and enact it.

If the Head of Government is not satisfied with the Council, he may request that the Co-princes dissolve the Council and order new elections. In turn, the Councillors have the power to remove the Head of Government from office. After a motion of censure is approved by at least one-fifth of the Councillors, the Council will vote and if it receives the absolute majority of votes, the Head of Government is removed.

The judiciary is composed of the Magistrates Court, the Criminal Law Court, the High Court of Andorra, and the Constitutional Court. The High Court of Justice is composed of five judges: one appointed by the Head of Government, one each by the Co-princes, one by the Syndic General, and one by the Judges and Magistrates. It is presided over by the member appointed by the Syndic General and the judges hold office for six-year terms.

The Magistrates and Judges are appointed by the High Court, as is the President of the Criminal Law Court. The High Court also appoints members of the Office of the Attorney General. The Constitutional Court is responsible for interpreting the Constitution and reviewing all appeals of unconstitutionality against laws and treaties. It is composed of four judges, one appointed by each of the Co-princes and two by the General Council. They serve eight-year terms. The Court is presided over by one of the Judges on a two-year rotation so that each judge at one point will preside over the Court.

Andorra does not have its own armed forces, although there is a small ceremonial Army. Responsibility for defending the nation rests primarily with France and Spain. However, in case of emergencies or natural disasters, the Sometent (an alarm) is called and all able-bodied men between 21 and 60 of Andorran nationality must serve. This is why all Andorrans, and especially the head of each house (usually the eldest able-bodied man of a house) should, by law, keep a rifle, even though the law also states that the police will offer a fire-arm in case of need. Andorra is a full member of the United Nations (UN), the Organization for Security and Co-operation in Europe (OSCE), and has a special agreement with the European Union (EU).

Andorra has a small army, which has historically been raised or reconstituted at various dates, but has never in modern times amounted to a standing army. The basic principle of Andorran defence is that all able-bodied men are available to fight if called upon by the sounding of the Sometent. Being a landlocked country, Andorra has no navy.

In the modern era, the army has consisted of a very small body of volunteers willing to undertake ceremonial duties. Uniforms were handed down from generation to generation within families and communities. Despite not being involved in any fighting, Andorra was technically the longest combatant in the First World War, as the country was left out of the Versailles Peace Conference, and technically remained at war with Germany from 1914 until 1958.

The army's role in internal security was largely taken over by the formation of the Police Corps of Andorra in 1931. Brief civil disorder associated with the elections of 1933 led to assistance being sought from the French National Gendarmerie, with a detachment resident in Andorra for two months under the command of RenÃ©-Jules Baulard. The Andorran Army was reformed in the following year, with eleven soldiers appointed to supervisory roles. The force consisted of six Corporals, one for each parish (although there are currently seven parishes, there were only six until 1978), plus four junior staff officers to co-ordinate action, and a commander with the rank of Major. It was the responsibility of the six corporals, each in his own parish, to be able to raise a fighting force from among the able-bodied men of the parish.

Today a small, twelve-man ceremonial unit remains the only permanent section of the Andorran Army, but all able-bodied men remain technically available for military service, with a requirement for each family to have access to a firearm. The army has not fought for more than 700 years, and its main responsibility is to present the flag of Andorra at official ceremonial functions. According to Marc FornÃ© MolnÃ©, Andorra's military budget is strictly from voluntary donations, and the availability of full-time voluntaries.

The myth that all members of the Andorran Army are ranked as officers is popularly maintained in many works of reference. In reality, all those serving in the permanent ceremonial reserve hold ranks as officers, or non-commissioned officers, because the other ranks are considered to be the rest of the able-bodied male population, who may still be called upon by the Sometent to serve, although such a call has not been made in modern times.

The Grup d'IntervenciÃ³ Policia d'Andorra (GIPA) is a small special forces unit trained in counter-terrorism, and hostage recovery tasks. Although it is the closest in style to an active military force, it is part of the Police Corps, and not the Army. As terrorist and hostage situations are a rare threat to the nation, the GIPA is commonly assigned to prisoner escort duties, and at other times to routine policing.


Andorra maintains a small but modern and well-equipped internal police force, with around 240 police officers supported by civilian assistants. The principal services supplied by the Corps are uniformed community policing, criminal detection, border control, and traffic policing. There are also small specialist units including police dogs, mountain rescue, and bomb disposal.

The Andorran Fire Brigade, with headquarters at Santa Coloma, operates from four modern fire stations, and has a staff of around 120 firefighters. The service is equipped with 16 heavy appliances (fire tenders, turntable ladders, and specialist four-wheel drive vehicles), 4 light support vehicles (cars and vans), and 4 ambulances.

Historically, the families of the six ancient parishes of Andorra maintained local arrangements to assist each other in fighting fires. The first fire pump purchased by the government was acquired in 1943. The serious fires (which lasted for two days) in parts of the principality in December 1959 led to calls for a permanent fire service, and the Andorran Fire Brigade was formed on 21 April 1961.

The fire service maintains full-time cover with five fire crews on duty at any time â€“ two at the brigade's headquarters in Santa Coloma, and one crew at each of the other three fire stations.


Andorra consists of seven parishes:
Andorra la Vella
Canillo
Encamp
Escaldes-Engordany
La Massana
Ordino
Sant JuliÃ  de LÃ²ria

Due to its location in the eastern Pyrenees mountain range, Andorra consists predominantly of rugged mountains, the highest being the Coma Pedrosa at , and the average elevation of Andorra is . These are dissected by three narrow valleys in a Y shape that combine into one as the main stream, the Gran Valira river, leaves the country for Spain (at Andorra's lowest point of ). Andorra's land area is .

Phytogeographically, Andorra belongs to the Atlantic European province of the Circumboreal Region within the Boreal Kingdom. According to the WWF, the territory of Andorra belongs to the ecoregion of Pyrenees conifer and mixed forests.

Andorra has an alpine climate and continental climate. Its higher elevation means there is, on average, more snow in winter, lower humidity, and it is slightly cooler in summer. There are, on average, 300days per year of sunshine.

Tourism, the mainstay of Andorra's tiny, well-to-do economy, accounts for roughly 80% of GDP. An estimated 10.2million tourists visit annually, attracted by Andorra's duty-free status and by its summer and winter resorts. Andorra's relative advantage has recently eroded as the economies of adjoining France and Spain have been opened up, providing broader availability of goods and lower tariffs.

One of the main sources of income in Andorra is as a result of tourism from ski resorts which total to over  of ski ground. The impact of skiing on the Andorra economy has been tremendous, bringing over 7 million visitors a year. It is currently estimated to generate 340 million euros per year, 2000 direct jobs and 10000 indirect jobs.

The banking sector, with its tax haven status, also contributes substantially to the economy (the financial and insurance sector accounts for approximately 19% of GDP). The financial system comprises 5 banking groups, 1 specialised credit entity, 8 investment undertaking management entities, 3 asset management companies and 29 insurance companies, 14 of which are branches of foreign insurance companies authorised to operate in the Principality.

Agricultural production is limitedâ€”only 2% of the land is arableâ€”and most food has to be imported. Some tobacco is grown locally. The principal livestock activity is domestic sheep raising. Manufacturing output consists mainly of cigarettes, cigars, and furniture. Andorra's natural resources include hydroelectric power, mineral water, timber, iron ore, and lead.

Andorra is not a member of the European Union, but enjoys a special relationship with it, such as being treated as an EU member for trade in manufactured goods (no tariffs) and as a non-EU member for agricultural products. Andorra lacked a currency of its own and used both the French franc and the Spanish peseta in banking transactions until 31 December 1999, when both currencies were replaced by the EU's single currency, the euro. Coins and notes of both the franc and the peseta remained legal tender in Andorra until 31 December 2002. Andorra negotiated to issue its own euro coins, beginning in 2014.

Andorra has traditionally had one of the world's lowest unemployment rates. In 2009 it stood at 2.9%.

Andorra has long benefited from its status as a tax haven, with revenues raised exclusively through import tariffs. However, during the European sovereign-debt crisis of the 21st century, its tourist economy suffered a decline, partly caused by a drop in the prices of goods in Spain, which undercut Andorran duty-free shopping. This led to a growth in unemployment. On 1 January 2012, a business tax of 10% was introduced, followed by a sales tax of 2% a year later, which raised just over 14 million euros in its first quarter. On 31 May 2013, it was announced that Andorra intended to legislate for the introduction of income tax by the end of June, against a background of increasing dissatisfaction with the existence of tax havens among EU members. The announcement was made following a meeting in Paris between the Head of Government Antoni Marti and the French President and Prince of Andorra, FranÃ§ois Hollande. Hollande welcomed the move as part of a process of Andorra "bringing its taxation in line with international standards".

The population of Andorra is estimated at 85,458 (2014). The population has grown from 5,000 in 1900.

Andorran nationals form a plurality, but not a majority, in the country (31,363 / 33%); other nationalities include Spaniards (27,300 / 23%), Portuguese (13,794 / 21%), French (5,213 / 17%), British (1,085 / 1%) and Italians.

Two-thirds of the population is made up of citizens without Andorran nationality, who do not have the right to vote (suffrage) in communal elections. Moreover, they are not allowed to be elected as president or to own more than 33% of the capital stock of a privately held company.


The historic and official language is Catalan, a Romance language. The Andorran government  encourages the use of Catalan. It funds a Commission for Catalan Toponymy in Andorra (Catalan: la ComissiÃ³ de ToponÃ­mia d'Andorra), and provides free Catalan classes to assist immigrants. Andorran television and radio stations use Catalan.

Because of immigration, historical links, and close geographic proximity, Spanish, Portuguese and  French are also commonly spoken. Most Andorran residents can speak one or more of these, in addition to Catalan. English is less commonly spoken among the general population, though it is understood to varying degrees in the major tourist resorts. Andorra is one of only four European countries (together with France, Monaco, and Turkey) that have never signed the Council of Europe Framework Convention on National Minorities.

According to the Observatori Social d'Andorra, the linguistic usage in Andorra are as follows:
The population of Andorra is predominantly (90%) Roman Catholic. Their patron saint is Our Lady of Meritxell. Though it is not an official state religion, the constitution acknowledges a special relationship with the Catholic Church, offering some special privileges to that group. Other Christian denominations include the Anglican Church, the Unification Church, the New Apostolic Church, and Jehovah's Witnesses. The Muslim community is primarily made up of the approximately 2,000 North African immigrants who, according to the U.S. State Department, are themselves divided between fundamentalists and non-fundamentalists. There is a small community of Hindus and BahÃ¡'Ã­s and roughly 100 Jews live in Andorra. (See History of the Jews in Andorra.)


Children between the ages of 6 and 16 are required by law to have full-time education. Education up to secondary level is provided free of charge by the government.

There are three systems of schoolsâ€“ Andorran, French and Spanishâ€“ which use Catalan, French and Spanish, respectively, as the main language of instruction. Parents may choose which system their children attend. All schools are built and maintained by Andorran authorities, but teachers in the French and Spanish schools are paid for the most part by France and Spain. About 50% of Andorran children attend the French primary schools, and the rest attend Spanish or Andorran schools.

The Universitat d'Andorra (UdA) is the state public university and is the only university in Andorra. It was established in 1997. The University provides first-level degrees in nursing, computer science, business administration, and educational sciences, in addition to higher professional education courses. The only two graduate schools in Andorra are the Nursing School and the School of Computer Science, the latter having a PhD programme.

The geographical complexity of the country as well as the small number of students prevents the University of Andorra from developing a full academic program, and it serves principally as a centre for virtual studies, connected to Spanish and French universities. The Virtual Studies Centre (Centre dâ€™Estudis Virtuals) at the University runs approximately twenty different academic degrees at both undergraduate and postgraduate levels in fields including tourism, law, Catalan philology, humanities, psychology, political sciences, audiovisual communication, telecommunications engineering, and East Asia studies. The Centre also runs various postgraduate programs and continuing-education courses for professionals.

Healthcare in Andorra is provided to all employed persons and their families by the government-run social security system, CASS (Caixa Andorrana de Seguretat Social), which is funded by employer and employee contributions in respect of salaries. The cost of healthcare is covered by CASS at rates of 75% for out-patient expenses such as medicines and hospital visits, 90% for hospitalisation, and 100% for work-related accidents. The remainder of the costs may be covered by private health insurance. Other residents and tourists require full private health insurance.

The main hospital, Meritxell, is in Escaldes-Engordany. There are also 12 primary health care centres in various locations around the Principality.

Until the 20th century, Andorra had very limited transport links to the outside world, and development of the country was affected by its physical isolation. Even now, the nearest major airports at Toulouse and Barcelona are both three hours' drive from Andorra.

Andorra has a road network of , of which  is unpaved. The two main roads out of Andorra la Vella are the CG-1 to the Spanish border, and the CG-2 to the French border via the Envalira Tunnel near El Pas de la Casa. Bus services cover all metropolitan areas and many rural communities, with services on most major routes running half-hourly or more frequently during peak travel times. There are frequent long-distance bus services from Andorra to Barcelona and Toulouse. Bus services are mostly run by private companies, but some local ones are operated by the government.

There are no railways, ports, or airports for fixed-wing aircraft in Andorra. There are, however, heliports in La Massana (CamÃ­ Heliport), Arinsal and Escaldes-Engordany with commercial helicopter services. Nearby airports are located in Barcelona, Toulouse, Perpignan, Reus, and Girona. The closest public airport is Perpignan - Rivesaltes Airport, which is  away and has short-haul services to several destinations in the United Kingdom and France. La Seu d'Urgell Airport, a small airfield in northern Spain close to the Andorran border and currently used only by private airplanes, is being studied as a possible future airport for public aviation services.

The nearest railway station is L'Hospitalet-prÃ¨s-l'Andorre  east of Andorra which is on the -gauge line from Latour-de-Carol, () southeast of Andorra, to Toulouse and on to Paris by the French high-speed trains. This line is operated by the SNCF. Latour-de-Carol has a scenic  trainline to Villefranche-de-Conflent, as well as the SNCF's  gauge line connecting to Perpignan, and the RENFE's  -gauge line to Barcelona. There are also direct  IntercitÃ©s de Nuit trains between L'Hospitalet-prÃ¨s-l'Andorre and Paris on certain dates.

In Andorra, mobile and fixed telephone and internet services are operated exclusively by the Andorran national telecommunications company, SOM, also known as Servei de Telecomunicacions d'Andorra (STA). The same company also manages the technical infrastructure for national broadcasting of digital television and radio.

By the end of 2010, it was planned that every home in the country would have fibre-to-the-home for internet access at a minimum speed of 100Mbit/s, and the availability was complete in June 2012.

There is only one Andorran television station, RÃ dio i TelevisiÃ³ d'Andorra (RTVA). Radio Nacional dâ€™Andorra operates two radio stations, Radio Andorra and Andorra MÃºsica. There are three national newspapers, Diari d'Andorra, El PeriÃ²dic d'Andorra, and Bondia as well as several local newspapers.

The official and historic language is Catalan. Thus the culture is Catalan, with its own specificity.

Andorra is home to folk dances like the contrapÃ s and marratxa, which survive in Sant JuliÃ  de LÃ²ria especially. Andorran folk music has similarities to the music of its neighbours, but is especially Catalan in character, especially in the presence of dances such as the sardana. Other Andorran folk dances include contrapÃ s in Andorra la Vella and Saint Anne's dance in Escaldes-Engordany. Andorra's national holiday is Our Lady of Meritxell Day, 8 September. American Folk Artist Malvina Reynolds, intrigued by its defence budget of $4.90, wrote a song "Andorra". Pete Seeger added verses, and sang "Andorra" on his 1962 album The Bitter and the Sweet.

Andorra is famous for the practice of Winter Sports. Popular sports played in Andorra include football, rugby union, basketball and roller hockey.

In roller hockey Andorra usually plays in CERH Euro Cup and in FIRS Roller Hockey World Cup. In 2011, Andorra was the host country to the 2011 European League Final Eight.
The country is represented in association football by the Andorra national football team. However, the team has had little success internationally because of Andorra's small population. Football is ruled in Andorra by the Andorran Football Federation founded in 1994, it organizes the national competitions of association football (Primera DivisiÃ³, Copa ConstituciÃ³ and Supercopa) and futsal. FC Andorra, a club based in Andorra la Vella founded in 1942, compete in the Spanish football league system.

Rugby is a traditional sport in Andorra, mainly influenced by the popularity in southern France. The Andorra national rugby union team, nicknamed "Els Isards", has impressed on the international stage in rugby union and rugby sevens. VPC Andorra XV is a rugby team based in Andorra la Vella actually playing in the French championship.

Basketball popularity has increased in the country since the 1990s when the Andorran team BC Andorra played in the top league of Spain (Liga ACB). After 18 years the club returned to the top league in 2014.

Other sports practiced in Andorra include cycling, volleyball, judo, Australian football, handball, swimming, gymnastics, tennis and motorsports. In 2012, Andorra raised its first national cricket team and played a home match against the Dutch Fellowship of Fairly Odd Places Cricket Club, the first match played in the history of Andorra at an altitude of .

Andorra first participated at the Olympic Games in 1976. The country has also appeared in every Winter Olympic Games since 1976. Andorra competes in the Games of the Small States of Europe being twice the host country in 1991 and 2005.

Index of Andorra-related articles
Outline of Andorra
Andorraâ€“European Union relations
Bibliography of Andorra
European microstates
List of Andorrans
List of Co-Princes of Andorra
Lists of ecoregions by country
Recognition of same-sex unions in Andorra
Scouts d'Andorra
Tourism in Andorra

â€“ Official governmental site 

 from the United States Library of Congress
 from UCB Libraries GovPubs

 from the BBC News
 
 from EuroDocs
 â€“ slideshow by The New York Times


In mathematics and statistics, the arithmetic mean (, stress on third syllable of "arithmetic"), or simply the mean or average when the context is clear, is the sum of a collection of numbers divided by the number of numbers in the collection. The collection is often a set of results of an experiment, or a set of results from a survey. The term "arithmetic mean" is preferred in some contexts in mathematics and statistics because it helps distinguish it from other means, such as the geometric mean and the harmonic mean.

In addition to mathematics and statistics, the arithmetic mean is used frequently in fields such as economics, sociology, and history, and it is used in almost every academic field to some extent. For example, per capita income is the arithmetic average income of a nation's population.

While the arithmetic mean is often used to report central tendencies, it is not a robust statistic, meaning that it is greatly influenced by outliers (values that are very much larger or smaller than most of the values). Notably, for skewed distributions, such as the distribution of income for which a few people's incomes are substantially greater than most people's, the arithmetic mean may not accord with one's notion of "middle", and robust statistics, such as the median, may be a better description of central tendency.

In a more obscure usage, any sequence of values that form an arithmetic sequence between two numbers x and y can be called "arithmetic means between x and y."

Suppose we have a data set containing the values  The arithmetic mean  is defined by the formula
.

(See summation for an explanation of the summation operator, Î£). If the data set is a statistical population (i.e., consists of every possible observation and not just a subset of them), then the mean of that population is called the population mean. If the data set is a statistical sample (a subset of the population), we call the statistic resulting from this calculation a sample mean.

The arithmetic mean of a variable is often denoted by a bar, for example as in  (read "x bar"), which is the mean of the  values .


The arithmetic mean has several properties that make it useful, especially as a measure of central tendency. These include:

If numbers  have mean , then . Since  is the distance from a given number to the mean, one way to interpret this property is as saying that the numbers to the left of the mean are balanced by the numbers to the right of the mean. The mean is the only single number for which the residuals (deviations from the estimate) sum to zero.
If it is required to use a single number as a "typical" value for a set of known numbers , then the arithmetic mean of the numbers does this best, in the sense of minimizing the sum of squared deviations from the typical value: the sum of . (It follows that the sample mean is also the best single predictor in the sense of having the lowest root mean squared error.) If the arithmetic mean of a population of numbers is desired, then the estimate of it that is unbiased is the arithmetic mean of a sample drawn from the population.


The arithmetic mean may be contrasted with the median. The median is defined such that half the values are larger than, and half are smaller than, the median. If elements in the sample data increase arithmetically, when placed in some order, then the median and arithmetic average are equal. For example, consider the data sample . The average is , as is the median. However, when we consider a sample that cannot be arranged so as to increase arithmetically, such as , the median and arithmetic average can differ significantly. In this case, the arithmetic average is 6.2 and the median is 4. In general, the average value can vary significantly from most values in the sample, and can be larger or smaller than most of them.

There are applications of this phenomenon in many fields. For example, since the 1980s, the median income in the United States has increased more slowly than the arithmetic average of income.



A weighted average, or weighted mean, is an average in which some data points count more strongly than others, in that they are given more weight in the calculation. For example, the arithmetic mean of  and  is , or equivalently . In contrast, a weighted mean in which the first number receives, for example, twice as much weight as the second (perhaps because it is assumed to appear twice as often in the general population from which these numbers were sampled) would be calculated as . Here the weights, which necessarily sum to the value one, are  and , the former being twice the latter. Note that the arithmetic mean (sometimes called the "unweighted average" or "equally weighted average") can be interpreted as a special case of a weighted average in which all the weights are equal to each other (equal to  in the above example, and equal to  in a situation with  numbers being averaged).

When a population of numbers, and any sample of data from it, could take on any of a continuous range of numbers, instead of for example just integers, then the probability of a number falling into one range of possible values could differ from the probability of falling into a different range of possible values, even if the lengths of both ranges are the same. In such a case, the set of probabilities can be described using a continuous probability distribution. The analog of a weighted average in this context, in which there are an infinitude of possibilities for the precise value of the variable, is called the mean of the probability distribution. The most widely encountered probability distribution is called the normal distribution; it has the property that all measures of its central tendency, including not just the mean but also the aforementioned median and the mode, are equal to each other. This property does not hold however, in the cases of a great many probability distributions, such as the lognormal distribution illustrated here.

Particular care must be taken when using cyclic data, such as phases or angles. NaÃ¯vely taking the arithmetic mean of 1Â° and 359Â° yields a result of 180Â°.
This is incorrect for two reasons:
Firstly, angle measurements are only defined up to an additive constant of 360Â° (or 2Ï€, if measuring in radians). Thus one could as easily call these 1Â° and âˆ’1Â°, or 361Â° and 719Â°, each of which gives a different average.
Secondly, in this situation, 0Â° (equivalently, 360Â°) is geometrically a better average value: there is lower dispersion about it (the points are both 1Â° from it, and 179Â° from 180Â°, the putative average).

In general application, such an oversight will lead to the average value artificially moving towards the middle of the numerical range. A solution to this problem is to use the optimization formulation (viz., define the mean as the central point: the point about which one has the lowest dispersion), and redefine the difference as a modular distance (i.e., the distance on the circle: so the modular distance between 1Â° and 359Â° is 2Â°, not 358Â°).

Average
FrÃ©chet mean
Generalized mean
Geometric mean
Mode
Sample mean and covariance
Summary statistics



The American Football Conference (AFC) is one of the two conferences of the National Football League (NFL). This conference and its counterpart, the National Football Conference (NFC), currently contain 16 teams each, making up the 32 teams of the NFL. The current AFC title holder is the New England Patriots.

Since 2002, the AFC has 16 teams, organized into four divisions each with four teams: East, North, South and West.
Each AFC team plays the other teams in its division twice (home and away) during the regular season, in addition to 10 other games assigned to their schedule by the NFL the previous May. Two of these games are assigned on the basis of the team's final division standing in the previous season. The remaining 8 games are split between the roster of two other NFL divisions. This assignment shifts each year. For instance, in the 2007 regular season, each team in the AFC West played one game against each team in both the AFC South and the NFC North. In this way division competition consists of common opponents, with the exception of the 2 games assigned on the strength of each team's prior division standing. (i.e. the division winner will face the other two division winners in the AFC divisions that they are not scheduled to play) The NFC operates according to the same system.

At the end of each football season, there are playoff games involving the top six teams in the AFC (the four division champions by place standing and the top two remaining non-division-champion teams ("wild cards") by record). The last two teams remaining play in the AFC Championship game with the winner receiving the Lamar Hunt Trophy. The AFC champion plays the NFC champion in the Super Bowl. After Super Bowl XLVII the AFC has won 20 Super Bowls to the 24 won by the NFC. Since losing 13 consecutive Super Bowls in the 1980s and 1990s (XIXXXXI), the AFC has won 10 of the last 16. The coach of the team with the best record that lost in the AFC Divisional round is the coach of the Pro Bowl.

Both the AFC and the NFC were created after the NFL merged with the American Football League (AFL) in 1970. The AFL began play in 1960 with eight teams, and added two more expansion clubs (the Miami Dolphins in 1966 and the Cincinnati Bengals in 1968) before the merger. In order to equalize the number of teams in each conference, three NFL teams that predated the AFL's launch (the Cleveland Browns, Pittsburgh Steelers, and the then-Baltimore Colts) joined the ten former AFL teams to form the AFC. The two AFL divisions AFL East and AFL West were more or less intact, while the Century Division, in which the Browns and the Steelers had played since 1967, was moved from the NFL to become the new AFC Central.

Since the merger, five expansion teams have joined the AFC and two have left, thus making the current total 16. When the Seattle Seahawks and the Tampa Bay Buccaneers joined the league in 1976, they were temporarily placed in the NFC and AFC respectively. This arrangement lasted for one season only before the two teams switched conferences. The Seahawks eventually returned to the NFC as a result of the 2002 realignment. The expansion Jacksonville Jaguars joined the AFC in 1995.

Due to the relocation controversy of the Cleveland Browns, a new AFC franchise called the Baltimore Ravens was officially established in 1996 while the Browns were reactivated in 1999.

The Houston Texans were then added to the league in 2002, joining the AFC.

Between 2000 and 2014, the AFC had sent either the Baltimore Ravens (2 times), the Denver Broncos (1 time), the Indianapolis Colts (2 times), the Oakland Raiders (1 time), the New England Patriots (6 times), and the Pittsburgh Steelers (3 times) to the Super Bowl. By contrast, the NFC has sent 11 different teams during that same time frame.


The merged league created a new logo for the AFC that took elements of the old AFL logo, specifically the "A" and the six stars surrounding it. The AFC logo basically remained unchanged from 1970 to 2009. The 2010 NFL season introduced an updated AFC logo, with the most notable revision being the removal of two stars (leaving four representing the four divisions of the AFC), and moving the stars inside the letter, similar to the NFC logo.



Animal Farm is an allegorical and dystopian novella by George Orwell, first published in England on 17 August 1945. According to Orwell, the book reflects events leading up to the Russian Revolution of 1917 and then on into the Stalin era in the Soviet Union. Orwell, a democratic socialist, was a critic of Joseph Stalin and hostile to Moscow-directed Stalinism, an attitude that was critically shaped by his experiences during the Spanish Civil War. The Soviet Union, he believed, had become a brutal dictatorship, built upon a cult of personality and enforced by a reign of terror. In a letter to Yvonne Davet, Orwell described Animal Farm as a satirical tale against Stalin ("un conte satirique contre Staline"), and in his essay "Why I Write" (1946), he wrote that Animal Farm was the first book in which he had tried, with full consciousness of what he was doing, "to fuse political purpose and artistic purpose into one whole".

The original title was Animal Farm: A Fairy Story, though the subtitle was dropped by U.S. publishers for its 1946 publication and subsequently all but one of the translations during Orwell's lifetime omitted it. Other variations in the title include: A Satire and A Contemporary Satire. Orwell suggested the title Union des rÃ©publiques socialistes animales for the French translation, which abbreviates to URSA, the Latin for "bear", a symbol of Russia, and which recalled the French name of the Soviet Union, Union des rÃ©publiques socialistes soviÃ©tiques.

Orwell wrote the book from November 1943 to February 1944, when the wartime alliance with the Soviet Union was at its height and Stalin was regarded highly by the British people and intelligentsia, a circumstance that Orwell hated. It was initially rejected by a number of British and American publishers, including one of Orwell's own, Victor Gollancz. Its publication was thus delayed, though it became a great commercial success when it did finally appear partly because the Cold War so quickly followed World War II.

Time magazine chose the book as one of the 100 best English-language novels (1923 to 2005); it also featured at number 31 on the Modern Library List of Best 20th-Century Novels. It won a Retrospective Hugo Award in 1996, and is also included in the Great Books of the Western World selection.


Old Major, the old boar on the Manor Farm, summons the animals on the farm together for a meeting, during which he refers to humans as parasites and teaches the animals a revolutionary song called Beasts of England. When Major dies, two young pigs, Snowball and Napoleon, assume command and consider it a duty to prepare for the Rebellion. The animals revolt and drive the drunken and irresponsible farmer Mr. Jones from the farm, renaming it "Animal Farm". They adopt Seven Commandments of Animalism, the most important of which is, "All animals are equal."

Snowball teaches the animals to read and write, while Napoleon educates young puppies on the principles of Animalism. Food is plentiful, and the farm runs smoothly. The pigs elevate themselves to positions of leadership and set aside special food items, ostensibly for their personal health. Napoleon and Snowball struggle for preeminence. When Snowball announces his plans to build a windmill, Napoleon has his dogs chase Snowball away and subsequently declares himself leader of Animal Farm.

Napoleon enacts changes to the governance structure of the farm, replacing meetings with a committee of pigs who will run the farm. Through a young pig named Squealer, Napoleon claims credit for the windmill idea. The animals work harder with the promise of easier lives with the windmill. When the animals find the windmill collapsed after a violent storm, Napoleon and Squealer convince the animals that Snowball is trying to sabotage their project. Once Snowball becomes a scapegoat, Napoleon begins to purge the farm with his dogs, killing animals he accuses of consorting with his old rival. Beasts of England is replaced by an anthem glorifying Napoleon, who appears to be adopting the lifestyle of a man. The animals remain convinced that they are better off than they were under Mr. Jones.

Mr Frederick, one of the neighbouring farmers, attacks the farm, using blasting powder to blow up the restored windmill. Though the animals win the battle, they do so at great cost, as many, including Boxer the workhorse, are wounded. Despite his injuries, Boxer continues working harder and harder, until he collapses while working on the windmill. Napoleon sends for a van to take Boxer to the veterinary surgeon, explaining that better care can be given there. Benjamin, the cynical donkey who "could read as well as any pig", notices that the van belongs to a knacker, and attempts a futile rescue. Squealer reports that the van was purchased by the hospital and the writing from the previous owner had not been repainted. But in reality, Napoleon has sold his most loyal and long-suffering worker for money to buy himself whisky.
Years pass, and the pigs start to resemble humans, as they walk upright, carry whips, and wear clothes. The Seven Commandments are abridged to a single phrase: "All animals are equal, but some animals are more equal than others". Napoleon holds a dinner party for the pigs and local farmers, with whom he celebrates a new alliance. He abolishes the practice of the revolutionary traditions and restores the name "The Manor Farm". As the animals look from pigs to humans, they realise they can no longer distinguish between the two.



Old Major â€“ An aged prize Middle White boar provides the inspiration that fuels the Rebellion in the book. He is an allegorical combination of Karl Marx, one of the creators of communism, and Lenin, the communist leader of the Russian Revolution and the early Soviet nation, in that he draws up the principles of the revolution. His skull being put on revered public display recalls Lenin, whose embalmed body was put on display. 
Napoleon â€“ "A large, rather fierce-looking Berkshire boar, the only Berkshire on the farm, not much of a talker, but with a reputation for getting his own way". An allegory of Joseph Stalin, Napoleon is the main villain of Animal Farm. In the first French version of Animal Farm, Napoleon is called , the French form of Caesar, although another translation has him as .
Snowball â€“ Napoleon's rival and original head of the farm after Jones' overthrow. He is mainly based on Leon Trotsky, but also combines elements from Lenin.
Squealer â€“ A small, white, fat porker who serves as Napoleon's right-trotter pig and minister of propaganda, holding a position similar to that of Vyacheslav Molotov.
Minimus â€“ A poetic pig who writes the second and third national anthems of Animal Farm after the singing of "Beasts of England" is banned.
The piglets â€“ Hinted to be the children of Napoleon and are the first generation of animals subjugated to his idea of animal inequality.
The young pigs â€“ Four pigs who complain about Napoleon's takeover of the farm but are quickly silenced and later executed. Based on the Great Purge of Grigori Zinoviev, Lev Kamenev, Nikolai Bukharin, and Alexei Rykov.
Pinkeye â€“ A minor pig who is mentioned only once; he is the pig that tastes Napoleon's food to make sure it is not poisoned, in response to rumours about an assassination attempt on Napoleon.


Mr Jones â€“ The former owner of the farm,  Jones is a very heavy drinker. The animals revolt against him after he drinks so much that he does not feed or take care of them. He is an allegory of Russian Tsar Nicholas II, who abdicated following the February Revolution of 1917 and was murdered, along with the rest of his family, by the Bolsheviks on 17 July 1918.
Mr Frederick â€“ The tough owner of Pinchfield, a small but well-kept neighbouring farm, who briefly enters into an alliance with Napoleon. He is an allegory of Adolf Hitler, who enters into a neutrality pact with Joseph Stalin's USSR only to later break it by invading the Soviet Union. 
Mr Pilkington â€“ The easy-going but crafty and well-to-do owner of Foxwood, a large neighbouring farm overgrown with weeds. 
Mr Whymper â€“ A man hired by Napoleon to act as the liaison between Animal Farm and human society. At first he is used to acquire goods needed for the  farm, such as dog biscuits and paraffin, but later he procures luxuries like alcohol for the pigs.


Boxer â€“ A loyal, kind, dedicated, hard working, and respectable cart-horse, although quite naive and gullible. Boxer does a large share of the physical labor on the farm, adhering to the simplistic belief that working harder will solve all the animals problems. He has been described as "faithful and strong"; he believes any problem can be solved if he works harder. However, when Boxer is injured, Napoleon sells him to a local knacker to buy himself whisky.
Mollie â€“ A self-centered, self-indulgent and vain young white mare who quickly leaves for another farm after the revolution. She is only once mentioned again, in a manner similar to those who left Russia after the fall of the Tsar.
Clover - A gentle, caring female horse, who shows concern especially for Boxer, who often pushes herself too hard. She seems to catch on to the sly tricks and schemes set up by Napoleon and Squealer.
Benjamin â€“ A donkey, one of the oldest, wisest animals on the farm, and one of the few who can read properly. He is skeptical, temperamental and cynical: his most frequent remark is, "Life will go on as it has always gone onâ€”that is, badly." The academic Morris Dickstein has suggested there is "a touch of Orwell himself in this creature's timeless skepticism" and indeed, friends called Orwell "Donkey George", "after his grumbling donkey Benjamin, in Animal Farm."

Muriel â€“ A wise old goat who is friends with all of the animals on the farm. She, like Benjamin and Snowball, is one of the few animals on the farm who can read.
The puppies â€“ Offspring of Jessie and Bluebell, they were taken away at birth by Napoleon and reared by him to be his security force.
Moses â€“ The raven, "Mr. Jones's especial pet, was a spy and a tale-bearer, but he was also a clever talker." Initially following Mrs. Jones into exile, he reappears several years later and resumes his role of talking but not working. He regales Animal Farm's denizens with tales of a wondrous place beyond the clouds called "Sugarcandy Mountain, that happy country where we poor animals shall rest forever from our labours!" Orwell portrays established religion as "the black raven of priestcraftâ€”promising pie in the sky when you die, and faithfully serving whoever happens to be in power." Napoleon brings the raven back (Ch. IX), as Stalin brought back the Russian Orthodox Church. 
The sheep â€“ They show limited understanding of the Animalism and the political atmosphere of the farm; yet nonetheless they blindly support Napoleon's ideals with vocal jingles during his speeches and meetings with Snowball.
The hens â€“ The hens are among the first to rebel against Napoleon.
The cows â€“ Their milk is stolen by the pigs, who learn to milk them. The milk is stirred into the pigs' mash every day, while the other animals are denied such luxuries. 
The cat â€“ Never seen to carry out any work, the cat is absent for long periods and is forgiven; because her excuses are so convincing and she "purred so affectionately that it was impossible not to believe in her good intentions." She has no interest in the politics of the farm, and the only time she is recorded as having participated in an election, she is found to have actually "voted on both sides."


George Orwell wrote the manuscript in 1943 and 1944 subsequent to his experiences during the Spanish Civil War, which he described in Homage to Catalonia (1938). In the preface of a 1947 Ukrainian edition of Animal Farm, he explained how escaping the communist purges in Spain taught him "how easily totalitarian propaganda can control the opinion of enlightened people in democratic countries". This motivated Orwell to expose and strongly condemn what he saw as the Stalinist corruption of the original socialist ideals.

Immediately prior to his writing, Orwell had quit the BBC. He was also upset about a booklet for propagandists the Ministry of Information had put out. The booklet included instructions on how to quell ideological fears of the Soviet Union, such as directions to claim that the Red Terror was a figment of Nazi imagination.

In the preface, Orwell also described the source of the idea of setting the book on a farm:
Orwell initially encountered difficulty getting the manuscript published, largely due to fears that the book might upset the alliance between Britain, the United States, and the Soviet Union. Four publishers refused; one had initially accepted the work but declined it after consulting the Ministry of Information. Eventually, Secker and Warburg published the first edition in 1945.

During the Second World War, it became clear to Orwell that anti-Soviet literature was not something which most major publishing houses would touch â€” including his regular publisher Gollancz. He also submitted the manuscript to Faber and Faber, where the poet T. S. Eliot (who was a director of the firm) rejected it; Eliot wrote back to Orwell praising the book's "good writing" and "fundamental integrity", but declared that they would only accept it for publication if they had some sympathy for the viewpoint "which I take to be generally Trotskyite". Eliot said he found the view "not convincing", and contended that the pigs were made out to be the best to run the farm; he posited that someone might argue "what was needed.. was not more communism but more public-spirited pigs". Orwell let AndrÃ© Deutsch, who was working for Nicholson & Watson in 1944, read the typescript, and Deutsch was convinced that Nicholson & Watson would want to publish it; however, they did not, and "lectured Orwell on what they perceived to be errors in Animal Farm." In his London Letter on 17 April 1944 for Partisan Review, Orwell wrote that it was "now next door to impossible to get anything overtly anti-Russian printed. Anti-Russian books do appear, but mostly from Catholic publishing firms and always from a religious or frankly reactionary angle."

The publisher Jonathan Cape, who had initially accepted Animal Farm, subsequently rejected the book after an official at the British Ministry of Information warned him off â€” although the civil servant who it is assumed gave the order was later found to be a Soviet spy. Writing to Leonard Moore, a partner in the literary agency of Christy & Moore, publisher Jonathan Cape explained that the decision had been taken on the advice of a senior official in the Ministry of Information. Such flagrant anti-Soviet bias was unacceptable, and the choice of pigs as the dominant class was thought to be especially offensive. It may reasonably be assumed that the 'important official' was a man named Peter Smollett, who was later unmasked as a Soviet agent. Orwell was suspicious of Smollett/Smolka, and he would be one of the names Orwell included in his list of Crypto-Communists and Fellow-Travellers sent to the Information Research Department in 1949. Born Hans Peter Smolka in Vienna in 1912, he came to Britain in 1933 as an NKVD agent with the codename 'Abo', became a naturalised British subject in 1938, changed his name, and after the outbreak of World War II joined the Ministry of Information where he organised pro-Soviet propaganda, working with Kim Philby in 1943-45. Smollett's family have rejected the accusation that he was a spy. The publisher wrote to Orwell, saying:
Frederic Warburg also faced pressures against publication, even from people in his own office and from his wife Pamela, who felt that it was not the moment for ingratitude towards Stalin and the heroic Red Army, which had played a major part in defeating Hitler. A Russian translation was printed in the paper Posev, and in giving permission for a Russian translation of Animal Farm, Orwell refused in advance all royalties. A translation in Ukrainian, which was produced in Germany, was confiscated in large part by the American wartime authorities and handed over to the Soviet repatriation commission.

In October 1945, Orwell wrote to Frederic Warburg expressing interest in pursuing the possibility that the political cartoonist David Low might illustrate Animal Farm. Low had written a letter saying that he had had "a good time with ANIMAL FARM - an excellent bit of satire - it would illustrate perfectly." Nothing came of this, and a trial issue produced by Secker & Warburg in 1956 illustrated by John Driver was abandoned, but the Folio Society published an edition in 1984 illustrated by Quentin Blake and an edition illustrated by the cartoonist Ralph Steadman was published by Secker & Warburg in 1995 to celebrate the fiftieth anniversary of the first edition of Animal Farm.

Orwell originally wrote a preface complaining about British self-censorship and how the British people were suppressing criticism of the USSR, their World War II ally. "The sinister fact about literary censorship in England is that it is largely voluntary.... Things are kept right out of the British press, not because the Government intervenes but because of a general tacit agreement that 'it wouldn't do' to mention that particular fact." Although the first edition allowed space for the preface, it was not included, and as of June 2009 it has not been published with most editions of the book.

Secker and Warburg published the first edition of Animal Farm in 1945 without any introduction. However, the publisher had provided space for a preface in the author's proof composited from the manuscript. For reasons unknown, no preface was supplied, and all the page numbers needed to be redone at the last minute.

Years later, in 1972, Ian Angus found the original typescript titled "The Freedom of the Press", and Bernard Crick published it, together with his own introduction, in The Times Literary Supplement on 15 September 1972 as "How the essay came to be written". Orwell's essay criticised British self-censorship by the press, specifically the suppression of unflattering descriptions of Stalin and the Soviet government. The same essay also appeared in the Italian 1976 Animal Farm edition, with another introduction by Crick, claiming to be the first edition with the preface. Other publishers were still declining to publish it.

Contemporary reviews of the work were not universally positive. Writing in the American New Republic magazine, George Soule expressed his disappointment in the book, writing that it "puzzled and saddened me. It seemed on the whole dull. The allegory turned out to be a creaking machine for saying in a clumsy way things that have been said better directly." Soule believed that the animals were not consistent enough with their real world inspirations, and said, "It seems to me that the failure of this book (commercially it is already assured of tremendous success) arises from the fact that the satire deals not with something the author has experienced, but rather with stereotyped ideas about a country which he probably does not know very well".

Tosco Fyvel, writing in Tribune, 24 August 1945, called the book "a gentle satire on a certain State and on the illusions of an age which may already be behind us." Julian Symons responded, on 7 September, "Should we not expect, in Tribune at least, acknowledgement of the fact that it is a satire not at all gentle upon a particular State - Soviet Russia? It seems to me that a reviewer should have the courage to identify Napoleon with Stalin, and Snowball with Trotsky, and express an opinion favourable or unfavourable to the author, upon a political ground. In a hundred years time perhaps, Animal Farm may be simply a fairy story, today it is a political satire with a good deal of point."

Animal Farm has been subject to much comment in the decades since these early remarks.


The pigs Snowball, Napoleon, and Squealer adapt Old Major's ideas into "a complete system of thought", which they formally name Animalism, an allegoric reference to Communism. Soon after, Napoleon and Squealer partake in activities associated with the humans (drinking alcohol, sleeping in beds, trading), which were explicitly prohibited by the Seven Commandments. Squealer is employed to alter the Seven Commandments to account for this humanisation, an allusion to the Soviet government's revising of history in order to exercise control of the people's beliefs about themselves and their society.
The original commandments are:
Whatever goes upon two legs is an enemy.
Whatever goes upon four legs, or has wings, is a friend.
No animal shall wear clothes.
No animal shall sleep in a bed.
No animal shall drink alcohol.
No animal shall kill any other animal.
All animals are equal.

Later, Napoleon and his pigs secretly revise some commandments to clear themselves of accusations of law-breaking. The changed commandments are as follows, with the changes bolded:

No animal shall sleep in a bed with sheets.
No animal shall drink alcohol to excess.
No animal shall kill any other animal without cause.

Eventually, these are replaced with the maxims, "All animals are equal, but some animals are more equal than others", and "Four legs good, two legs better!" as the pigs become more human. This is an ironic twist to the original purpose of the Seven Commandments, which were supposed to keep order within Animal Farm by uniting the animals together against the humans and preventing animals from following the humans' evil habits. Through the revision of the commandments, Orwell demonstrates how simply political dogma can be turned into malleable propaganda.

In the Eastern Bloc, both Animal Farm and later Nineteen Eighty-Four were on the list of forbidden books until the end of communism in 1989, and were only available via clandestine Samizdat networks.

Orwell biographer Jeffrey Meyers has written, "virtually every detail has political significance in this allegory." Orwell himself wrote in 1946, "Of course I intended it primarily as a satire on the Russian revolution.. that kind of revolution (violent conspiratorial revolution, led by unconsciously power hungry people) can only lead to a change of masters  revolutions only effect a radical improvement when the masses are alert." In a preface for a 1947 Ukrainian edition, he stated, "...for the past ten years I have been convinced that the destruction of the Soviet myth was essential if we wanted a revival of the socialist movement. On my return from Spain  I thought of exposing the Soviet myth in a story that could be easily understood by almost anyone and which could be easily translated into other languages."

The revolt of the animals against Farmer Jones is Orwell's analogy with the October 1917 Bolshevik Revolution, and Jones's attempt to regain control, with the aid of neighbouring farmers, parallels the Western powers' efforts of 1918-21 to crush the Bolsheviks. The pigs' rise to pre-eminence mirrors the rise of a Stalinist bureaucracy in the USSR, just as Napoleon's emergence as the farm's sole leader reflects Stalin's emergence. The pigs' appropriation of milk and apples for their own use, "the turning point of the story" as Orwell termed it in a letter to Dwight Macdonald, stands as an analogy for the crushing of the left-wing 1921 Kronstadt revolt against the Bolsheviks, and the difficult efforts of the animals to build the windmill suggest the various Five Year Plans. The puppies controlled by Napoleon parallel the nurture of the secret police in the Stalinist structure, and the pigs' treatment of the other animals on the farm recalls the internal terror faced by the populace in the 1930s. In chapter seven, when the animals confess their nonexistent crimes and are killed, Orwell directly alludes to the purges, confessions and show trials of the late 1930s. These contributed to Orwell's conviction that the Bolshevik revolution had been corrupted and the Soviet system become rotten.

Peter Edgerly Firchow and Peter Davison consider that in real life, with events in Animal Farm mirroring those in the Soviet Union, the Battle of the Windmill represents the Great Patriotic War (World War II), especially the Battle of Stalingrad and the Battle of Moscow. During the battle, Frederick drills a hole and places explosives inside, and then "All the animals, including Napoleon" took cover; Orwell had the publisher alter this to "All the animals except Napoleon" in recognition of Joseph Stalin's decision to remain in Moscow during the German advance. This very particular alteration had been occasioned by Orwell having been in Paris in March 1945, working as a war correspondent for the Observer and the Manchester Evening News. In Paris he met Joseph Czapski, a survivor of the Katyn Massacre. In spite of Czapski's opposition to the Soviet regime, he told Orwell, as Orwell wrote to Arthur Koestler, that it had been "the character  greatness of Stalin" that saved Russia from the German invasion.

The Battle of the Cowshed has been said to represent the allied invasion of Soviet Russia in 1918, and the defeat of the White Russians in the Russian Civil War.
Other connections that writers have suggested illustrate Orwell's telescoping of Russian history from 1917 to 1943 include the wave of rebelliousness that ran through the countryside after the Rebellion, which stands for the abortive revolutions in Hungary and in Germany (Ch IV); the conflict between Napoleon and Snowball (Ch V), paralleling "the two rival and quasi-Messianic beliefs that seemed pitted against one another: Trotskyism, with its faith in the revolutionary vocation of the proletariat of the West; and Stalinism with its glorification of Russia's socialist destiny"; Napoleon's dealings with Whymper and the Willingdon markets (Ch VI), paralleling the Treaty of Rapallo; and Frederick's bank notes, paralleling the Hitler-Stalin non-aggression pact of August 1939, which are forgeries. Frederick attacks Animal Farm without warning and destroys the windmill.

The book's close, with the pigs and men in a kind of rapprochement, reflected Orwell's view of the 1943 Teheran Conference that seemed to display the establishment of "the best possible relations between the USSR and the West"â€”but in reality were destined, as Orwell presciently predicted, to continue to unravel. The disagreement between the allies and the start of the Cold War is suggested when Napoleon and Pilkington, both suspicious, "played an ace of spades simultaneously".

A BBC radio version, produced by Rayner Heppenstall, was broadcast in January 1947. Orwell listened to the production at his home in Canonbury Square in London, with Hugh Gordon Porteous, amongst others. Orwell later wrote to Heppenstall that Porteous, "who had not read the book, grasped what was happening after a few minutes." A further radio production, again using Orwell's own dramatisation of the book, was broadcast in January 2013 on BBC Radio Four. Tamsin Greig narrated and the cast included Nicky Henson as Napoleon, Toby Jones as the propagandist Squealer, and Ralph Ineson as Boxer.

Animal Farm has been adapted to film twice. The 1954 Animal Farm film was an animated feature and the 1999 Animal Farm film was a TV live action version. Both differ from the novel, and have been accused of taking significant liberties, including sanitising some aspects. In the 1954 version, Napoleon is apparently overthrown in a second revolution. The 1999 film shows Napoleon's regime collapsing in on itself, with the farm having new human owners, reflecting the collapse of Soviet communism, appropriating the new political reality to the story. In 2012, a HFR-3D version of Animal Farm potentially directed by Andy Serkis was announced.

A theatrical version, with music by Richard Peaslee and lyrics by Adrian Mitchell, was staged at the National Theatre London on 25 April 1984, directed by Peter Hall. It toured nine cities in 1985. A solo version, adapted and performed by Guy Masterson, premiÃ¨red at the Traverse Theatre Edinburgh in January 1995 and has toured worldwide since.


Pink Floyd's 1977 record album Animals was partially inspired by Animal Farm. It categorises people as pigs, dogs, or sheep.
R.E.M.'s song "Disturbance at the Heron House" is based on Animal Farm.
The Clash used an image from the 1954 animated movie Animal Farm on their 45-RPM single "English Civil War".
Canadian-based band Boxer the Horse takes its name from a character in the novel.
Dead prez based a song on their 2000 album Let's Get Free called "Animal in Man" based on the novella, putting emphasis on how the other animals should not trust the pigs during a revolution.
The lyrics of the song â€³Arthur's Farmâ€³ from the Half Man Half Biscuit album Back Again in the DHSS tell the story of Douglas Bader and Arthur Askey visiting Animal Farm. The song features the line "Four legs good, but no legs best" in apparent tribute to the two famous amputees.
Radiohead's song "Optimistic" contains a lyric mentioning Animal Farm.
The Boston Crusaders Drum and Bugle Corps 2014 show was titled Animal Farm, which was based on the novel.

In The Daleks' Master Plan, a 1966 episode of the long-running British science fiction show Doctor Who, a character references the modified seventh commandment of Animal Farm, saying: "Though we are all equal partners with the Daleks on this great conquest, some of us are more equal than others."

In the seventh episode of the second season of the HBO series Oz was titled Animal Farm, in reference to the conniving and manipulation of the characters vying for control, similar to the characters of the novella.

In the third episode of the first season of the X-Men animated series, "Enter Magneto," Beast is seen reading a copy of Animal Farm, and is mocked by the prison guards for "reading a picture book" and is asked if he "sees any relatives in there" because they assume he is an illiterate animal.

In the tenth episode of  of Johnny Bravo, "Aunt Katie's Farm", Johnny, while dressed in a pig costume, yells, "Four feet good! Two feet bad!".

The Lost episode "ExposÃ©", in season three, involves flashbacks with Nikki and Paulo involving an argument with Kate about the handgun case. During this scene, Dr. Leslie Arzt yells at Kate that "The pigs are walking," a reference to Animal Farm where Napoleon and his generals begin to adapt human characteristics and change their oath from "Four legs good, two legs bad" to "Four legs good, two legs better."

 (hardcover, 1946, First American Edition)
ISBN 0-451-51679-6 (paperback, 1956, Signet Classic)
ISBN 0-582-02173-1 (paper text, 1989)
ISBN 0-15-107255-8 (hardcover, 1990)
ISBN 0-582-06010-9 (paper text, 1991)
ISBN 0-679-42039-8 (hardcover, 1993)
ISBN 0-606-00102-6 (prebound, 1996)
ISBN 0-15-100217-7 (hardcover, 1996, Anniversary Edition)
ISBN 0-452-27750-7 (paperback, 1996, Anniversary Edition)
ISBN 0-451-52634-1 (mass market paperback, 1996, Anniversary Edition)
ISBN 0-582-53008-3 (1996)
ISBN 1-56000-520-3 (cloth text, 1998, Large Type Edition)
ISBN 0-7910-4774-1 (hardcover, 1999)
ISBN 0-451-52536-1 (paperback, 1999)
ISBN 0-7641-0819-0 (paperback, 1999)
ISBN 0-8220-7009-X (e-book, 1999)
ISBN 0-7587-7843-0 (hardcover, 2002)
ISBN 0-15-101026-9 (hardcover, 2003, with Nineteen Eighty-Four)
ISBN 0-452-28424-4 (paperback, 2003, Centennial Edition)
ISBN 0-8488-0120-2 (hardcover)
ISBN 0-03-055434-9 (hardcover) Animal Farm with Connections
ISBN 0-395-79677-6 (hardcover) Animal Farm & Related Readings, 1997
ISBN 0-582-43447-5 (hardcover, 2007)
ISBN 0-14-103349-5 (paperback, 2007)

On 17 July 2009, Amazon.com withdrew certain Amazon Kindle titles, including Animal Farm and Nineteen Eighty-Four by George Orwell, from sale, refunded buyers, and remotely deleted items from purchasers' devices after discovering that the publisher lacked rights to publish the titles in question. Notes and annotations for the books made by users on their devices were also deleted. After the move prompted outcry and comparisons to Nineteen Eighty-Four itself, Amazon spokesman Drew Herdener stated that the company is "hanging our systems so that in the future we will not remove books from customers' devices in these circumstances."

History of Soviet Russia and the Soviet Union (1917â€“1927)
History of the Soviet Union (1927â€“1953)
New class
Polish Nobel laureate WÅ‚adysÅ‚aw Reymont, with his Revolt, anticipated by two decades Orwell's Animal Farm.

Books:
Gulliver's Travels, a favourite book of Orwell'sâ€”Swift reverses the role of horses and human beings in the fourth bookâ€”Orwell brought also to Animal Farm "a dose of Swiftian misanthropy, looking ahead to a time 'when the human race had finally been overthrown.'"
Bunt (Revolt), published in 1924, is a book by Polish Nobel laureate WÅ‚adysÅ‚aw Reymont with a theme similar to Animal Farms.
White Acre vs. Black Acre, published in 1856 and written by William M. Burwell, is a satirical novel that features allegories for slavery in the United States similar to Animal Farm's portrayal of Soviet history.
George Orwell's own Nineteen Eighty-Four, the classic dystopian novel about totalitarianism.

 (Bernard Crick's preface quotes Orwell writing to T. S. Eliot about Cape's suggestion to find another animal than pigs to represent the Bolsheviks)
 (web archive)


Amphibians are ectothermic, tetrapod vertebrates of the class Amphibia. Modern amphibians are all Lissamphibia. They inhabit a wide variety of habitats with most species living within terrestrial, fossorial, arboreal or freshwater aquatic ecosystems. Amphibians typically start out as larvae living in water, but some species have developed behavioural adaptations to bypass this. The young generally undergo metamorphosis from larva with gills to an adult air-breathing form with lungs. Amphibians use their skin as a secondary respiratory surface and some small terrestrial salamanders and frogs lack lungs and rely entirely on their skin. They are superficially similar to reptiles but, along with mammals and birds, reptiles are amniotes and do not require water bodies in which to breed. With their complex reproductive needs and permeable skins, amphibians are often ecological indicators and in recent decades there has been a dramatic decline in amphibian populations for many species around the globe.

The earliest amphibians evolved in the Devonian period from sarcopterygian fish with lungs and bony-limbed fins, features that were helpful in adapting to dry land. They diversified and became dominant during the Carboniferous and Permian periods, but were later displaced by reptiles and other vertebrates. Over time, amphibians shrank in size and decreased in diversity, leaving only the modern subclass Lissamphibia. The three modern orders of amphibians are Anura (the frogs and toads), Caudata/Urodela (the salamanders), and Gymnophiona/Apoda (the caecilians). The number of known amphibian species is approximately 7,000, of which nearly 90% are frogs. The smallest amphibian (and vertebrate) in the world is a frog from New Guinea (Paedophryne amauensis) with a length of just . The largest living amphibian is the  Chinese giant salamander (Andrias davidianus), but this is dwarfed by the extinct  Prionosuchus from the middle Permian of Brazil. The study of amphibians is called batrachology, while the study of both reptiles and amphibians is called herpetology.

The word "amphibian" is derived from the Ancient Greek term á¼€Î¼Ï†Î¯Î²Î¹Î¿Ï‚ (amphÃ­bios), which means "both kinds of life", á¼€Î¼Ï†Î¯ meaning "of both kinds" and Î²Î¹Î¿Ï‚ meaning "life". The term was initially used as a general adjective for animals that could live on land or in water, including seals and otters. Traditionally, the class Amphibia includes all tetrapod vertebrates that are not amniotes. Amphibia in its widest sense (sensu lato) was divided into three subclasses, two of which are extinct:
Subclass Labyrinthodontiaâ€  (diverse Paleozoic and early Mesozoic group)
Subclass Lepospondyliâ€  (small Paleozoic group, sometimes included in the Labyrinthodontia, which may actually be more closely related to amniotes than Lissamphibia)
Subclass Lissamphibia (all modern amphibians, including frogs, toads, salamanders, newts and caecilians)
Order Anura (frogs and toads): Jurassic to presentâ€”6,200 current species in 53 families
Order Caudata or Urodela (salamanders, newts): Jurassic to presentâ€”652 current species in 9 families
Order Gymnophiona or Apoda (caecilians): Jurassic to presentâ€”192 current species in 10 families
The actual number of species in each group depends on the taxonomic classification followed. The two most common systems are the classification adopted by the website AmphibiaWeb, University of California, Berkeley and the classification by herpetologist Darrel Frost and the American Museum of Natural History, available as the online reference database "Amphibian Species of the World". The numbers of species cited above follow Frost and the total number of known amphibian species is over 7,000, of which nearly 90% are frogs.

With the phylogenetic classification, the taxon Labyrinthodontia has been discarded as it is a paraphyletic group without unique defining features apart from shared primitive characteristics. Classification varies according to the preferred phylogeny of the author and whether they use a stem-based or a node-based classification. Traditionally, amphibians as a class are defined as all tetrapods with a larval stage, while the group that includes the common ancestors of all living amphibians (frogs, salamanders and caecilians) and all their descendants is called Lissamphibia. The phylogeny of Paleozoic amphibians is uncertain, and Lissamphibia may possibly fall within extinct groups, like the Temnospondyli (traditionally placed in the subclass Labyrinthodontia) or the Lepospondyli, and in some analyses even in the amniotes. This means that advocates of phylogenetic nomenclature have removed a large number of basal Devonian and Carboniferous amphibian-type tetrapod groups that were formerly placed in Amphibia in Linnaean taxonomy, and included them elsewhere under cladistic taxonomy. If the common ancestor of amphibians and amniotes is included in Amphibia, it becomes a paraphyletic group.

All modern amphibians are included in the subclass Lissamphibia, which is usually considered a clade, a group of species that have evolved from a common ancestor. The three modern orders are Anura (the frogs and toads), Caudata (or Urodela, the salamanders), and Gymnophiona (or Apoda, the caecilians). It has been suggested that salamanders arose separately from a Temnospondyl-like ancestor, and even that caecilians are the sister group of the advanced reptiliomorph amphibians, and thus of amniotes. Although the fossils of several older proto-frogs with primitive characteristics are known, the oldest "true frog" is Prosalirus bitis, from the Early Jurassic Kayenta Formation of Arizona. It is anatomically very similar to modern frogs. The oldest known caecilian is another Early Jurassic species, Eocaecilia micropodia and is also from Arizona. The earliest salamander is Beiyanerpeton jianpingensis from the Late Jurassic of northeastern China.

Authorities disagree as to whether Salientia is a superorder that includes the order Anura, or whether Anura is a sub-order of the order Salientia. The Lissamphibia are traditionally divided into three orders, but an extinct salamander-like family, the Albanerpetontidae, is now considered part of Lissamphibia alongside the superorder Salientia. Furthermore, Salientia includes all three recent orders plus the Triassic proto-frog, Triadobatrachus.

The first major groups of amphibians developed in the Devonian period, around 370 million years ago, from lobe-finned fish which were similar to the modern coelacanth and lungfish. These ancient lobe-finned fish had evolved multi-jointed leg-like fins with digits that enabled them to crawl along the sea bottom. Some fish had developed primitive lungs to help them breathe air when the stagnant pools of the Devonian swamps were low in oxygen. They could also use their strong fins to hoist themselves out of the water and onto dry land if circumstances so required. Eventually, their bony fins would evolve into limbs and they would become the ancestors to all tetrapods, including modern amphibians, reptiles, birds, and mammals. Despite being able to crawl on land, many of these prehistoric tetrapodomorph fish still spent most of their time in the water. They had started to develop lungs, but still breathed predominantly with gills.

Many examples of species showing transitional features have been discovered. Ichthyostega was one of the first primitive amphibians, with nostrils and more efficient lungs. It had four sturdy limbs, a neck, a tail with fins and a skull very similar to that of the lobe-finned fish, Eusthenopteron. Amphibians evolved adaptations that allowed them to stay out of the water for longer periods. Their lungs improved and their skeletons became heavier and stronger, better able to support the weight of their bodies on land.. They developed "hands" and "feet" with five or more digits; the skin became more capable of retaining body fluids and resisting desiccation. The fish's hyomandibula bone in the hyoid region behind the gills diminished in size and became the stapes of the amphibian ear, an adaptation necessary for hearing on dry land. An affinity between the amphibians and the teleost fish is the multi-folded structure of the teeth and the paired supra-occipital bones at the back of the head, neither of these features being found elsewhere in the animal kingdom.
At the end of the Devonian period (360 million years ago), the seas, rivers and lakes were teeming with life while the land was the realm of early plants and devoid of vertebrates, though some, such as Ichthyostega, may have sometimes hauled themselves out of the water. It is thought they may have propelled themselves with their forelimbs, dragging their hindquarters in a similar manner to that used by the elephant seal. In the early Carboniferous (360 to 345 million years ago), the climate became wet and warm. Extensive swamps developed with mosses, ferns, horsetails and calamites. Air-breathing arthropods evolved and invaded the land where they provided food for the carnivorous amphibians that began to adapt to the terrestrial environment. There were no other tetrapods on the land and the amphibians were at the top of the food chain, occupying the ecological position currently held by the crocodile. Though equipped with limbs and the ability to breathe air, most still had a long tapering body and strong tail. They were the top land predators, sometimes reaching several metres in length, preying on the large insects of the period and the many types of fish in the water. They still needed to return to water to lay their shell-less eggs, and even most modern amphibians have a fully aquatic larval stage with gills like their fish ancestors. It was the development of the amniotic egg, which prevents the developing embryo from drying out, that enabled the reptiles to reproduce on land and which led to their dominance in the period that followed.

After the Carboniferous rainforest collapse amphibian dominance gave way to reptiles, and amphibians were further devastated by the Permianâ€“Triassic extinction event. During the Triassic Period (250 to 200 million years ago), the reptiles continued to out-compete the amphibians, leading to a reduction in both the amphibians' size and their importance in the biosphere. According to the fossil record, Lissamphibia, which includes all modern amphibians and is the only surviving lineage, may have branched off from the extinct groups Temnospondyli and Lepospondyli at some period between the Late Carboniferous and the Early Triassic. The relative scarcity of fossil evidence precludes precise dating, but the most recent molecular study, based on multilocus sequence typing, suggests a Late Carboniferous/Early Permian origin for extant amphibians.
The origins and evolutionary relationships between the three main groups of amphibians is a matter of debate. A 2005 molecular phylogeny, based on rDNA analysis, suggests that salamanders and caecilians are more closely related to each other than they are to frogs. It also appears that the divergence of the three groups took place in the Paleozoic or early Mesozoic (around 250 million years ago), before the breakup of the supercontinent Pangaea and soon after their divergence from the lobe-finned fish. The briefness of this period, and the swiftness with which radiation took place, would help account for the relative scarcity of primitive amphibian fossils. There are large gaps in the fossil record, but the discovery of a proto-frog from the Early Permian in Texas in 2008 provided a missing link with many of the characteristics of modern frogs. Molecular analysis suggests that the frogâ€“salamander divergence took place considerably earlier than the palaeontological evidence indicates.

As they evolved from lunged fish, amphibians had to make certain adaptations for living on land including the need to develop new means of locomotion. In the water, the sideways thrusts of their tails had propelled them forward, but on land, quite different mechanisms were required. Their vertebral columns, limbs, limb girdles and musculature needed to be strong enough to raise them off the ground for locomotion and feeding. Terrestrial adults discarded their lateral line systems and adapted their sensory systems to receive stimuli via the medium of the air. They needed to develop new methods to regulate their body heat to cope with fluctuations in ambient temperature. They developed behaviours suitable for reproduction in a terrestrial environment. Their skins were exposed to harmful ultraviolet rays that had previously been absorbed by the water. The skin changed to become more protective and prevent excessive water loss.

The superclass Tetrapoda is divided into four classes of vertebrate animals with four limbs. Reptiles, birds and mammals are amniotes, the eggs of which are either laid or carried by the female and are surrounded by several membranes, some of which are impervious. Lacking these membranes, amphibians require water bodies for reproduction, although some species have developed various strategies for protecting or bypassing the vulnerable aquatic larval stage. They are not found in the sea with the exception of one or two frogs that live in brackish water in mangrove swamps. On land, amphibians are restricted to moist habitats because of the need to keep their skin damp.

The smallest amphibian (and vertebrate) in the world is a microhylid frog from New Guinea (Paedophryne amauensis) first discovered in 2012. It has an average length of  and is part of a genus that contains four of the world's ten smallest frog species. The largest living amphibian is the  Chinese giant salamander (Andrias davidianus) but this is a great deal smaller than the largest amphibian that ever existedâ€”the extinct  Prionosuchus, a crocodile-like temnospondyl dating to 270 million years ago from the middle Permian of Brazil. The largest frog is the African Goliath frog (Conraua goliath), which can reach  and weigh .

Amphibians are ectothermic (cold-blooded) vertebrates that do not maintain their body temperature through internal physiological processes. Their metabolic rate is low and as a result, their food and energy requirements are limited. In the adult state, they have tear ducts and movable eyelids, and most species have ears that can detect airborne or ground vibrations. They have muscular tongues, which in many species can be protruded. Modern amphibians have fully ossified vertebrae with articular processes. Their ribs are usually short and may be fused to the vertebrae. Their skulls are mostly broad and short, and are often incompletely ossified. Their skin contains little keratin and lacks scales, apart from a few fish-like scales in certain caecilians. The skin contains many mucous glands and in some species, poison glands. The hearts of amphibians have three chambers, two atria and one ventricle. They have a urinary bladder and nitrogenous waste products are excreted primarily as urea. Most amphibians lay their eggs in water and have aquatic larvae that undergo metamorphosis to become terrestrial adults. Amphibians breathe by means of a pump action in which air is first drawn into the buccopharyngeal region through the nostrils. These are then closed and the air is forced into the lungs by contraction of the throat. They supplement this with gas exchange through the skin.


The order Anura (from the Ancient Greek a(n)- meaning "without" and oura meaning "tail") comprises the frogs and toads. They usually have long hind limbs that fold underneath them, shorter forelimbs, webbed toes with no claws, no tails, large eyes and glandular moist skin. Members of this order with smooth skins are commonly referred to as frogs, while those with  skins are known as toads. The difference is not a formal one taxonomically and there are numerous exceptions to this rule. Members of the family Bufonidae are known as the "true toads". Frogs range in size from the  goliath frog (Conraua goliath) of West Africa to the  Paedophryne amauensis, first described in Papua New Guinea in 2012, which is also the smallest known vertebrate. Although most species are associated with water and damp habitats, some are specialised to live in trees or in deserts. They are found worldwide except for polar areas.

Anura is divided into three suborders that are broadly accepted by the scientific community, but the relationships between some families remain unclear. Future molecular studies should provide further insights into their evolutionary relationships. The suborder Archaeobatrachia contains four families of primitive frogs. These are Ascaphidae, Bombinatoridae, Discoglossidae and Leiopelmatidae which have few derived features and are probably paraphyletic with regard to other frog lineages. The six families in the more evolutionarily advanced suborder Mesobatrachia are the fossorial Megophryidae, Pelobatidae, Pelodytidae, Scaphiopodidae and Rhinophrynidae and the obligatorily aquatic Pipidae. These have certain characteristics that are intermediate between the two other suborders. Neobatrachia is by far the largest suborder and includes the remaining families of modern frogs, including most common species. 96% of the over 5,000 extant species of frog are neobatrachians.

The order Caudata (from the Latin cauda meaning "tail") consists of the salamandersâ€”elongated, low-slung animals that mostly resemble lizards in form. This is a symplesiomorphic trait and they are no more closely related to lizards than they are to mammals. Salamanders lack claws, have scale-free skins, either smooth or covered with tubercles, and tails that are usually flattened from side to side and often finned. They range in size from the Chinese giant salamander (Andrias davidianus), which has been reported to grow to a length of , to the diminutive Thorius pennatulus from Mexico which seldom exceeds  in length. Salamanders have a mostly Laurasian distribution, being present in much of the Holarctic region of the northern hemisphere. The family Plethodontidae is also found in Central America and South America north of the Amazon basin; South America was apparently invaded from Central America by about the start of the Miocene, 23 million years ago. Urodela is a name sometimes used for all the extant species of salamanders. Members of several families of salamanders have become paedomorphic and either fail to complete their metamorphosis or retain some larval characteristics as adults. Most salamanders are under  long. They may be terrestrial or aquatic and many spend part of the year in each habitat. When on land, they mostly spend the day hidden under stones or logs or in dense vegetation, emerging in the evening and night to forage for worms, insects and other invertebrates.
The suborder Cryptobranchoidea contains the primitive salamanders. A number of fossil cryptobranchids have been found, but there are only three living species, the Chinese giant salamander (Andrias davidianus), the Japanese giant salamander (Andrias japonicus) and the hellbender (Cryptobranchus alleganiensis) from North America. These large amphibians retain several larval characteristics in their adult state; gills slits are present and the eyes are unlidded. A unique feature is their ability to feed by suction, depressing either the left side of their lower jaw or the right. The males excavate nests, persuade females to lay their egg strings inside them, and guard them. As well as breathing with lungs, they respire through the many folds in their thin skin, which has capillaries close to the surface.

The suborder Salamandroidea contains the advanced salamanders. They differ from the cryptobranchids by having fused prearticular bones in the lower jaw, and by using internal fertilisation. In salamandrids, the male deposits a bundle of sperm, the spermatophore, and the female picks it up and inserts it into her cloaca where the sperm is stored until the eggs are laid. The largest family in this group is Plethodontidae, the lungless salamanders, which includes 60% of all salamander species. The family Salamandridae includes the true salamanders and the name "newt" is given to members of its subfamily Pleurodelinae.

The third suborder, Sirenoidea, contains the four species of sirens, which are in a single family, Sirenidae. Members of this order are eel-like aquatic salamanders with much reduced forelimbs and no hind limbs. Some of their features are primitive while others are derived. Fertilisation is likely to be external as sirenids lack the cloacal glands used by male salamandrids to produce spermatophores and the females lack spermathecae for sperm storage. Despite this, the eggs are laid singly, a behaviour not conducive for external fertilisation.


The order Gymnophiona (from the Greek gymnos meaning "naked" and ophis meaning "serpent") or Apoda (from the Latin an- meaning "without" and the Greek poda meaning "legs") comprises the caecilians. These are long, cylindrical, limbless animals with a snake- or worm-like form. The adults vary in length from 8 to 75 centimetres (3 to 30 inches) with the exception of Thomson's caecilian (Caecilia thompsoni), which can reach . A caecilian's skin has a large number of transverse folds and in some species contains tiny embedded dermal scales. It has rudimentary eyes covered in skin, which are probably limited to discerning differences in light intensity. It also has a pair of short tentacles near the eye that can be extended and which have tactile and olfactory functions. Most caecilians live underground in burrows in damp soil, in rotten wood and under plant debris, but some are aquatic. Most species lay their eggs underground and when the larvae hatch, they make their way to adjacent bodies of water. Others brood their eggs and the larvae undergo metamorphosis before the eggs hatch. A few species give birth to live young, nourishing them with glandular secretions while they are in the oviduct. Caecilians have a mostly Gondwanan distribution, being found in tropical regions of Africa, Asia and Central and South America.



The  structure contains some typical characteristics common to terrestrial vertebrates, such as the presence of highly cornified outer layers, renewed periodically through a moulting process controlled by the pituitary and thyroid glands. Local thickenings (often called warts) are common, such as those found on toads. The outside of the skin is shed periodically more or less in one piece, in contrast to mammals and birds where it is shed in flakes. Amphibians often eat the sloughed skin. Caecilians are unique among amphibians in having mineralized dermal scales embedded in the dermis between the furrows in the skin. The similarity of these to the scales of bony fish is largely superficial. Lizards and some frogs have somewhat similar osteoderms forming bony deposits in the dermis, but this is an example of convergent evolution with similar structures having arisen independently in diverse vertebrate lineages.

Amphibian skin is permeable to water. Gas exchange can take place through the skin (cutaneous respiration) and this allows adult amphibians to respire without rising to the surface of water and to hibernate at the bottom of ponds. To compensate for their thin and delicate skin, amphibians have evolved mucous glands, principally on their heads, backs and tails. The secretions produced by these help keep the skin moist. In addition, most species of amphibian have granular glands that secrete distasteful or poisonous substances. Some amphibian toxins can be lethal to humans while others have little effect. The main poison-producing glands, the paratoids, produce the neurotoxin bufotoxin and are located behind the ears of toads, along the backs of frogs, behind the eyes of salamanders and on the upper surface of caecilians.

The skin colour of amphibians is produced by three layers of pigment cells called chromatophores. These three cell layers consist of the melanophores (occupying the deepest layer), the guanophores (forming an intermediate layer and containing many granules, producing a blue-green colour) and the lipophores (yellow, the most superficial layer). The colour change displayed by many species is initiated by hormones secreted by the pituitary gland. Unlike bony fish, there is no direct control of the pigment cells by the nervous system, and this results in the colour change taking place more slowly than happens in fish. A vividly coloured skin usually indicates that the species is toxic and is a warning sign to predators.

Amphibians have a skeletal system that is structurally homologous to other tetrapods, though with a number of variations. They all have four limbs except for the legless caecilians and a few species of salamander with reduced or no limbs. The bones are hollow and lightweight. The musculoskeletal system is strong to enable it to support the head and body. The bones are fully ossified and the vertebrae interlock with each other by means of overlapping processes. The pectoral girdle is supported by muscle, and the well-developed pelvic girdle is attached to the backbone by a pair of sacral ribs. The ilium slopes forward and the body is held closer to the ground than is the case in mammals.
In most amphibians, there are four digits on the fore foot and five on the hind foot, but no claws on either. Some salamanders have fewer digits and the amphiumas are eel-like in appearance with tiny, stubby legs. The sirens are aquatic salamanders with stumpy forelimbs and no hind limbs. The caecilians are limbless. They burrow in the manner of earthworms with zones of muscle contractions moving along the body. On the surface of the ground or in water they move by undulating their body from side to side.

In frogs, the hind legs are larger than the fore legs, especially so in those species that principally move by jumping or swimming. In the walkers and runners the hind limbs are not so large, and the burrowers mostly have short limbs and broad bodies. The feet have adaptations for the way of life, with webbing between the toes for swimming, broad adhesive toe pads for climbing, and keratinised tubercles on the hind feet for digging (frogs usually dig backwards into the soil). In most salamanders, the limbs are short and more or less the same length and project at right angles from the body. Locomotion on land is by walking and the tail often swings from side to side or is used as a prop, particularly when climbing. In their normal gait, only one leg is advanced at a time in the manner adopted by their ancestors, the lobe-finned fish. Some salamanders in the genus Aneides and certain plethodontids climb trees and have long limbs, large toepads and prehensile tails. In aquatic salamanders and in frog tadpoles, the tail has dorsal and ventral fins and is moved from side to side as a means of propulsion. Adult frogs do not have tails and caecilians have only very short ones.

Salamanders use their tails in defence and some are prepared to jettison them to save their lives in a process known as autotomy. Certain species in the Plethodontidae have a weak zone at the base of the tail and use this strategy readily. The tail often continues to twitch after separation which may distract the attacker and allow the salamander to escape. Both tails and limbs can be regenerated. Adult frogs are unable to regrow limbs but tadpoles can do so.


Amphibians have a juvenile stage and an adult stage, and the circulatory systems of the two are distinct. In the juvenile (or tadpole) stage, the circulation is similar to that of a fish; the two-chambered heart pumps the blood through the gills where it is oxygenated, around the body and back to the heart in a single loop. In the adult stage, amphibians (especially frogs) lose their gills and develop lungs. They have a heart that consists of a single ventricle and two atria. When the ventricle starts contracting, deoxygenated blood is pumped through the pulmonary artery to the lungs. Continued contraction then pumps oxygenated blood around the rest of the body. Mixing of the two bloodstreams is minimized by the anatomy of the chambers.

The nervous system is basically the same as in other vertebrates, with a central brain, a spinal cord, and nerves throughout the body. The amphibian brain is less well developed than that of reptiles, birds and mammals but is similar in morphology and function to that of a fish. It consists of equal parts cerebrum, midbrain and cerebellum. Various parts of the cerebrum process sensory input, such as smell in the olfactory lobe and sight in the optic lobe, and it is additionally the centre of behaviour and learning. The cerebellum is the centre of muscular coordination and the medulla oblongata controls some organ functions including heartbeat and respiration. The brain sends signals through the spinal cord and nerves to regulate activity in the rest of the body. The pineal body, known to regulate sleep patterns in humans, is thought to produce the hormones involved in hibernation and aestivation in amphibians.

Tadpoles retain the lateral line system of their ancestral fishes, but this is lost in terrestrial adult amphibians. Some caecilians possess electroreceptors that allow them to locate objects around them when submerged in water. The ears are well developed in frogs. There is no external ear, but the large circular eardrum lies on the surface of the head just behind the eye. This vibrates and sound is transmitted through a single bone, the stapes, to the inner ear. Only high-frequency sounds like mating calls are heard in this way, but low-frequency noises can be detected through another mechanism. There is a patch of specialized haircells, called papilla amphibiorum, in the inner ear capable of detecting deeper sounds. Another feature, unique to frogs and salamanders, is the columella-operculum complex adjoining the auditory capsule which is involved in the transmission of both airborne and seismic signals. The ears of salamanders and caecilians are less highly developed than those of frogs as they do not normally communicate with each other through the medium of sound.

The eyes of tadpoles lack lids, but at metamorphosis, the cornea becomes more dome-shaped, the lens becomes flatter, and eyelids and associated glands and ducts develop. The adult eyes are an improvement on invertebrate eyes and were a first step in the development of more advanced vertebrate eyes. They allow colour vision and depth of focus. In the retinas are green rods, which are receptive to a wide range of wavelengths.


Many amphibians catch their prey by flicking out an elongated tongue with a sticky tip and drawing it back into the mouth before seizing the item with their jaws. Some use inertial feeding to help them swallow the prey, repeatedly thrusting their head forward sharply causing the food to move backwards in their mouth by inertia. Most amphibians swallow their prey whole without much chewing so they possess voluminous stomachs. The short oesophagus is lined with cilia that help to move the food to the stomach and mucus produced by glands in the mouth and pharynx eases its passage. The enzyme chitinase produced in the stomach helps digest the chitinous cuticle of arthropod prey.

Amphibians possess a pancreas, liver and gall bladder. The liver is usually large with two lobes. Its size is determined by its function as a glycogen and fat storage unit, and may change with the seasons as these reserves are built or used up. Adipose tissue is another important means of storing energy and this occurs in the abdomen, under the skin and, in some salamanders, in the tail.
There are two kidneys located dorsally, near the roof of the body cavity. Their job is to filter the blood of metabolic waste and transport the urine via ureters to the urinary bladder where it is stored before being passed out periodically through the cloacal vent. Larvae and most aquatic adult amphibians excrete the nitrogen as ammonia in large quantities of dilute urine, while terrestrial species, with a greater need to conserve water, excrete the less toxic product urea. Some tree frogs with limited access to water excrete most of their metabolic waste as uric acid.


The lungs in amphibians are primitive compared to those of amniotes, possessing few internal septa and large alveoli, and consequently having a comparatively slow diffusion rate for oxygen entering the blood. Ventilation is accomplished by buccal pumping. Most amphibians, however, are able to exchange gases with the water or air via their skin. To enable sufficient cutaneous respiration, the surface of their highly vascularised skin must remain moist to allow the oxygen to diffuse at a sufficiently high rate. Because oxygen concentration in the water increases at both low temperatures and high flow rates, aquatic amphibians in these situations can rely primarily on cutaneous respiration, as in the Titicaca water frog and the hellbender salamander. In air, where oxygen is more concentrated, some small species can rely solely on cutaneous gas exchange, most famously the plethodontid salamanders, which have neither lungs nor gills. Many aquatic salamanders and all tadpoles have gills in their larval stage, with some (such as the axolotl) retaining gills as aquatic adults.


For the purpose of reproduction most amphibians require fresh water although some lay their eggs on land and have developed various means of keeping them moist. A few (e.g. Fejervarya raja) can inhabit brackish water, but there are no true marine amphibians. There are reports, however, of particular amphibian populations unexpectedly invading marine waters. Such was the case with the Black Sea invasion of the natural hybrid Pelophylax esculentus reported in 2010.

Several hundred frog species in adaptive radiations (e.g., Eleutherodactylus, the Pacific Platymantines, the Australo-Papuan microhylids, and many other tropical frogs), however, do not need any water for breeding in the wild. They reproduce via direct development, an ecological and evolutionary adaptation that has allowed them to be completely independent from free-standing water. Almost all of these frogs live in wet tropical rainforests and their eggs hatch directly into miniature versions of the adult, passing through the tadpole stage within the egg. Reproductive success of many amphibians is dependent not only on the quantity of rainfall, but the seasonal timing.

In the tropics, many amphibians breed continuously or at any time of year. In temperate regions, breeding is mostly seasonal, usually in the spring, and is triggered by increasing day length, rising temperatures or rainfall. Experiments have shown the importance of temperature, but the trigger event, especially in arid regions, is often a storm. In anurans, males usually arrive at the breeding sites before females and the vocal chorus they produce may stimulate ovulation in females and the endocrine activity of males that are not yet reproductively active.

In caecilians, fertilisation is internal, the male extruding an intromittent organ, the phallodeum, and inserting it into the female cloaca. The paired MÃ¼llerian glands inside the male cloaca secrete a fluid which resembles that produced by mammalian prostate glands and which may transport and nourish the sperm. Fertilisation probably takes place in the oviduct.

The majority of salamanders also engage in internal fertilisation. In most of these, the male deposits a spermatophore, a small packet of sperm on top of a gelatinous cone, on the substrate either on land or in the water. The female takes up the sperm packet by grasping it with the lips of the cloaca and pushing it into the vent. The spermatozoa move to the spermatheca in the roof of the cloaca where they remain until ovulation which may be many months later. Courtship rituals and methods of transfer of the spermatophore vary between species. In some, the spermatophore may be placed directly into the female cloaca while in others, the female may be guided to the spermatophore or restrained with an embrace called amplexus. Certain primitive salamanders in the families Sirenidae, Hynobiidae and Cryptobranchidae practice external fertilisation in a similar manner to frogs, with the female laying the eggs in water and the male releasing sperm onto the egg mass.

With a few exceptions, frogs use external fertilisation. The male grasps the female tightly with his forelimbs either behind the arms or in front of the back legs, or in the case of Epipedobates tricolor, around the neck. They remain in amplexus with their cloacae positioned close together while the female lays the eggs and the male covers them with sperm. Roughened nuptial pads on the male's hands aid in retaining grip. Often the male collects and retains the egg mass, forming a sort of basket with the hind feet. An exception is the granular poison frog (Oophaga granulifera) where the male and female place their cloacae in close proximity while facing in opposite directions and then release eggs and sperm simultaneously. The tailed frog (Ascaphus truei) exhibits internal fertilisation. The "tail" is only possessed by the male and is an extension of the cloaca and used to inseminate the female. This frog lives in fast-flowing streams and internal fertilisation prevents the sperm from being washed away before fertilisation occurs. The sperm may be retained in storage tubes attached to the oviduct until the following spring.

Most frogs can be classified as either prolonged or explosive breeders. Typically, prolonged breeders congregate at a breeding site, the males usually arriving first, calling and setting up territories. Other satellite males remain quietly nearby, waiting for their opportunity to take over a territory. The females arrive sporadically, mate selection takes place and eggs are laid. The females depart and territories may change hands. More females appear and in due course, the breeding season comes to an end. Explosive breeders on the other hand are found where temporary pools appear in dry regions after rainfall. These frogs are typically  species that emerge after heavy rains and congregate at a breeding site. They are attracted there by the calling of the first male to find a suitable place, perhaps a pool that forms in the same place each rainy season. The assembled frogs may call in unison and frenzied activity ensues, the males scrambling to mate with the usually smaller number of females.

Most amphibians go through metamorphosis, a process of significant morphological change after birth. In typical amphibian development, eggs are laid in water and larvae are adapted to an aquatic lifestyle. Frogs, toads and salamanders all hatch from the egg as larvae with external gills. Metamorphosis in amphibians is regulated by thyroxine concentration in the blood, which stimulates metamorphosis, and prolactin, which counteracts thyroxine's effect. Specific events are dependent on threshold values for different tissues. Because most embryonic development is outside the parental body, it is subject to many adaptations due to specific environmental circumstances. For this reason tadpoles can have horny ridges instead of teeth, whisker-like skin extensions or fins. They also make use of a sensory lateral line organ similar to that of fish. After metamorphosis, these organs become redundant and will be reabsorbed by controlled cell death, called apoptosis. The variety of adaptations to specific environmental circumstances among amphibians is wide, with many discoveries still being made.

The egg of an amphibian is typically surrounded by a transparent gelatinous covering secreted by the oviducts and containing mucoproteins and mucopolysaccharides. This capsule is permeable to water and gases, and swells considerably as it absorbs water. The ovum is at first rigidly held, but in fertilised eggs the innermost layer liquefies and allows the embryo to move freely. This also happens in salamander eggs, even when they are unfertilised. Eggs of some salamanders and frogs contain unicellular green algae. These penetrate the jelly envelope after the eggs are laid and may increase the supply of oxygen to the embryo through photosynthesis. They seem to both speed up the development of the larvae and reduce mortality. Most eggs contain the pigment melanin which raises their temperature through the absorption of light and also protects them against ultraviolet radiation. Caecilians, some plethodontid salamanders and certain frogs that lay eggs underground have unpigmented eggs. In the wood frog (Rana sylvatica), the interior of the globular egg cluster has been found to be up to  warmer than its surroundings which is an advantage in its cool northern habitat.

The eggs may be deposited singly or in small groups, or may take the form of spherical egg masses, rafts or long strings. In terrestrial caecilians, the eggs are laid in grape-like clusters in burrows near streams. The amphibious salamander Ensatina attaches its similar clusters by stalks to underwater stems and roots. The greenhouse frog (Eleutherodactylus planirostris) lays eggs in small groups in the soil where they develop in about two weeks directly into juvenile frogs without an intervening larval stage. The tungara frog (Physalaemus pustulosus) builds a floating nest from foam to protect its eggs. First a raft is built, then eggs are laid in the centre, and finally a foam cap is overlaid. The foam has anti-microbial properties. It contains no detergents but is created by whipping up proteins and lectins secreted by the female.


The eggs of amphibians are typically laid in water and hatch into free-living larvae that complete their development in water and later transform into either aquatic or terrestrial adults. In many species of frog and in most lungless salamanders (Plethodontidae), direct development takes place, the larvae growing within the eggs and emerging as miniature adults. Many caecilians and some other amphibians lay their eggs on land, and the newly hatched larvae wriggle or are transported to water bodies. Some caecilians, the Alpine salamander (Salamandra atra) and some of the African live-bearing toads (Nectophrynoides spp.) are viviparous. Their larvae feed on glandular secretions and develop within the female's oviduct, often for long periods. Other amphibians, but not caecilians, are ovoviviparous. The eggs are retained in or on the parent's body, but the larvae subsist on the yolks of their eggs and receive no nourishment from the adult. The larvae emerge at varying stages of their growth, either before or after metamorphosis, according to their species. The toad genus Nectophrynoides exhibits all of these developmental patterns among its dozen or so members.

Frog larvae are known as tadpoles and typically have oval bodies and long, vertically flattened tails with fins. The free-living larvae are normally fully aquatic, but the tadpoles of some species such as (Nannophrys ceylonensis) are semi-terrestrial and live among wet rocks. Tadpoles have cartilaginous skeletons, gills for respiration (external gills at first, internal gills later), lateral line systems and large tails that they use for swimming. Newly hatched tadpoles soon develop gill pouches that cover the gills. The lungs develop early and are used as accessory breathing organs, the tadpoles rising to the water surface to gulp air. Some species complete their development inside the egg and hatch directly into small frogs. These larvae do not have gills but instead have specialised areas of skin through which respiration takes place. While tadpoles do not have true teeth, in most species, the jaws have long, parallel rows of small keratinized structures called keradonts surrounded by a horny beak. Front legs are formed under the gill sac and hind legs become visible a few days later. Tadpoles are typically herbivorous, feeding mostly on algae, including diatoms filtered from the water through the gills. They are also detritivores, stirring up the sediment at the pond bottom and ingesting edible fragments. They have a relatively long, spiral-shaped gut to enable them to digest this diet. Some species are carnivorous at the tadpole stage, eating insects, smaller tadpoles and fish. Young of the Cuban tree frog (Osteopilus septentrionalis) can occasionally be cannibalistic, the younger tadpoles attacking a larger, more developed tadpole when it is undergoing metamorphosis.
At metamorphosis, rapid changes in the body take place as the lifestyle of the frog changes completely. The spiralâ€shaped mouth with horny tooth ridges is reabsorbed together with the spiral gut. The animal develops a large jaw, and its gills disappear along with its gill sac. Eyes and legs grow quickly, and a tongue is formed. There are associated changes in the neural networks such as development of stereoscopic vision and loss of the lateral line system. All this can happen in about a day. A few days later, the tail is reabsorbed, due to the higher thyroxine concentration required for this to take place.

At hatching, a typical salamander larva has eyes without lids, teeth in both upper and lower jaws, three pairs of feathery external gills, a somewhat laterally flattened body and a long tail with dorsal and ventral fins. The forelimbs may be partially developed and the hind limbs are rudimentary in pond-living species but may be rather more developed in species that reproduce in moving water. Pond-type larvae often have a pair of balancers, rod-like structures on either side of the head that may prevent the gills from becoming clogged up with sediment. Some members of the genera Ambystoma and Dicamptodon have larvae that never fully develop into the adult form, but this varies with species and with populations. The northwestern salamander (Ambystoma gracile) is one of these and, depending on environmental factors, either remains permanently in the larval state, a condition known as neoteny, or transforms into an adult. Both of these are able to breed. Neoteny occurs when the animal's growth rate is very low and is usually linked to adverse conditions such as low water temperatures that may change the response of the tissues to the hormone thyroxine. Other factors that may inhibit metamorphosis include lack of food, lack of trace elements and competition from conspecifics. The tiger salamander (Ambystoma tigrinum) also sometimes behaves in this way and may grow particularly large in the process. The adult tiger salamander is terrestrial, but the larva is aquatic and able to breed while still in the larval state. When conditions are particularly inhospitable on land, larval breeding may allow continuation of a population that would otherwise die out. There are fifteen species of  neotenic salamanders, including species of Necturus, Proteus and Amphiuma, and many examples of  ones that adopt this strategy under appropriate environmental circumstances.

Lungless salamanders in the family Plethodontidae are terrestrial and lay a small number of unpigmented eggs in a cluster among damp leaf litter. Each egg has a large yolk sac and the larva feeds on this while it develops inside the egg, emerging fully formed as a juvenile salamander. The female salamander often broods the eggs. In the genus Ensatinas, the female has been observed to coil around them and press her throat area against them, effectively massaging them with a mucous secretion.

In newts and salamanders, metamorphosis is less dramatic than in frogs. This is because the larvae are already carnivorous and continue to feed as predators when they are adults so few changes are needed to their digestive systems. Their lungs are functional early, but the larvae don't make as much use of them as do tadpoles. Their gills are never covered by gill sacs and are reabsorbed just before the animals leave the water. Other changes include the reduction in size or loss of tail fins, the closure of gill slits, thickening of the skin, the development of eyelids, and certain changes in dentition and tongue structure. Salamanders are at their most vulnerable at metamorphosis as swimming speeds are reduced and transforming tails are encumbrances on land. Adult salamanders often have an aquatic phase in spring and summer, and a land phase in winter. For adaptation to a water phase, prolactin is the required hormone, and for adaptation to the land phase, thyroxine. External gills do not return in subsequent aquatic phases because these are completely absorbed upon leaving the water for the first time.


Most terrestrial caecilians that lay eggs do so in burrows or moist places on land near bodies of water. The development of the young of Ichthyophis glutinosus, a species from Sri Lanka, has been much studied. The eel-like larvae hatch out of the eggs and make their way to water. They have three pairs of external red feathery gills, a blunt head with two rudimentary eyes, a lateral line system and a short tail with fins. They swim by undulating their body from side to side. They are mostly active at night, soon lose their gills and make sorties onto land. Metamorphosis is gradual. By the age of about ten months they have developed a pointed head with sensory tentacles near the mouth and lost their eyes, lateral line systems and tails. The skin thickens, embedded scales develop and the body divides into segments. By this time, the caecilian has constructed a burrow and is living on land.
In the majority of species of caecilians, the young are produced by vivipary. Typhlonectes compressicauda, a species from South America, is typical of these. Up to nine larvae can develop in the oviduct at any one time. They are elongated and have paired sac-like gills, small eyes and specialised scraping teeth. At first, they feed on the yolks of the eggs, but as this source of nourishment declines they begin to rasp at the ciliated epithelial cells that line the oviduct. This stimulates the secretion of fluids rich in lipids and mucoproteins on which they feed along with scrapings from the oviduct wall. They may increase their length sixfold and be two-fifths as long as their mother before being born. By this time they have undergone metamorphosis, lost their eyes and gills, developed a thicker skin and mouth tentacles, and reabsorbed their teeth. A permanent set of teeth grow through soon after birth.

The ringed caecilian (Siphonops annulatus) has developed a unique adaptation for the purposes of reproduction. The progeny feed on a skin layer that is specially developed by the adult in a phenomenon known as maternal dermatophagy. The brood feed as a batch for about seven minutes at intervals of approximately three days which gives the skin an opportunity to regenerate. Meanwhile they have been observed to ingest fluid exuded from the maternal cloaca.

The care of offspring among amphibians has been little studied but, in general, the larger the number of eggs in a batch, the less likely it is that any degree of parental care takes place. Nevertheless, it is estimated that in up to 20% of amphibian species, one or both adults play some role in the care of the young. Those species that breed in smaller water bodies or other specialised habitats tend to have complex patterns of behaviour in the care of their young.

Many woodland salamanders lay clutches of eggs under dead logs or stones on land. The black mountain salamander (Desmognathus welteri) does this, the mother brooding the eggs and guarding them from predation as the embryos feed on the yolks of their eggs. When fully developed, they break their way out of the egg capsules and disperse as juvenile salamanders. The male hellbender, a primitive salamander, excavates an underwater nest and encourages females to lay there. The male then guards the site for the two or three months before the eggs hatch, using body undulations to fan the eggs and increase their supply of oxygen.
The male Colostethus subpunctatus, a tiny frog, protects the egg cluster which is hidden under a stone or log. When the eggs hatch, the male transports the tadpoles on his back, stuck there by a mucous secretion, to a temporary pool where he dips himself into the water and the tadpoles drop off. The male midwife toad (Alytes obstetricans) winds egg strings round his thighs and carries the eggs around for up to eight weeks. He keeps them moist and when they are ready to hatch, he visits a pond or ditch and releases the tadpoles. The female gastric-brooding frog (Rheobatrachus spp.) reared larvae in her stomach after swallowing either the eggs or hatchlings; however, this stage was never observed before the species became extinct. The tadpoles secrete a hormone that inhibits digestion in the mother whilst they develop by consuming their very large yolk supply. The pouched frog (Assa darlingtoni) lays eggs on the ground. When they hatch, the male carries the tadpoles around in brood pouches on his hind legs. The aquatic Surinam toad (Pipa pipa) raises its young in pores on its back where they remain until metamorphosis. The granular poison frog (Oophaga granulifera) is typical of a number of tree frogs in the poison dart frog family Dendrobatidae. Its eggs are laid on the forest floor and when they hatch, the tadpoles are carried one by one on the back of an adult to a suitable water-filled crevice such as the  of a leaf or the rosette of a bromeliad. The female visits the nursery sites regularly and deposits unfertilised eggs in the water and these are consumed by the tadpoles.


With a few exceptions, adult amphibians are predators, feeding on virtually anything that moves that they can swallow. The diet mostly consists of small prey that do not move too fast such as beetles, caterpillars, earthworms and spiders. The sirens (Siren spp.) often ingest aquatic plant material with the invertebrates on which they feed and a Brazilian tree frog Xenohyla truncata includes a large quantity of fruit in its diet. The Mexican burrowing toad (Rhinophrynus dorsalis) has a specially adapted tongue for picking up ants and termites. It projects it with the tip foremost whereas other frogs flick out the rear part first, their tongues being hinged at the front.

Food is mostly selected by sight, even in conditions of dim light. Movement of the prey triggers a feeding response. Frogs have been caught on fish hooks baited with red flannel and green frogs (Rana clamitans) have been found with stomachs full of elm seeds that they had seen floating past. Toads, salamanders and caecilians also use smell to detect prey. This response is mostly secondary because salamanders have been observed to remain stationary near odoriferous prey but only feed if it moves. Cave-dwelling amphibians normally hunt by smell. Some salamanders seem to have learned to recognize immobile prey when it has no smell, even in complete darkness.

Amphibians usually swallow food whole but may chew it lightly first to subdue it. They typically have small hinged pedicellate teeth, a feature unique to amphibians. The base and crown of these are composed of dentine separated by an uncalcified layer and they are replaced at intervals. Salamanders, caecilians and some frogs have one or two rows of teeth in both jaws, but some frogs (Rana spp.) lack teeth in the lower jaw, and toads (Bufo spp.) have no teeth. In many amphibians there are also vomerine teeth attached to a facial bone in the roof of the mouth.
The tiger salamander (Ambystoma tigrinum) is typical of the frogs and salamanders that hide under cover ready to ambush unwary invertebrates. Others amphibians, such as the Bufo spp. toads, actively search for prey, while the Argentine horned frog (Ceratophrys ornata) lures inquisitive prey closer by raising its hind feet over its back and vibrating its yellow toes. Among leaf litter frogs in Panama, frogs that actively hunt prey have narrow mouths and are slim, often brightly coloured and toxic, while ambushers have wide mouths and are broad and well-camouflaged. Caecilians do not flick their tongues, but catch their prey by grabbing it with their slightly backward-pointing teeth. The struggles of the prey and further jaw movements work it inwards and the caecilian usually retreats into its burrow. The subdued prey is gulped down whole.

When they are newly hatched, frog larvae feed on the yolk of the egg. When this is exhausted some move on to feed on bacteria, algal crusts, detritus and raspings from submerged plants. Water is drawn in through their mouths, which are usually at the bottom of their heads, and passes through branchial food traps between their mouths and their gills where fine particles are trapped in mucus and filtered out. Others have specialised mouthparts consisting of a horny beak edged by several rows of labial teeth. They scrape and bite food of many kinds as well as stirring up the bottom sediment, filtering out larger particles with the papillae around their mouths. Some, such as the spadefoot toads, have strong biting jaws and are carnivorous or even cannibalistic.


The calls made by caecilians and salamanders are limited to occasional soft squeaks, grunts or hisses and have not been much studied. A clicking sound sometimes produced by caecilians may be a means of orientation, as in bats, or a form of communication. Most salamanders are considered voiceless, but the California giant salamander (Dicamptodon ensatus) has vocal cords and can produce a rattling or barking sound. Some species of salamander emit a quiet squeak or yelp if attacked.
Frogs are much more vocal, especially during the breeding season when they use their voices to attract mates. The presence of a particular species in an area may be more easily discerned by its characteristic call than by a fleeting glimpse of the animal itself. In most species, the sound is produced by expelling air from the lungs over the vocal cords into an air sac or sacs in the throat or at the corner of the mouth. This may distend like a balloon and acts as a resonator, helping to transfer the sound to the atmosphere, or the water at times when the animal is submerged. The main vocalisation is the male's loud advertisement call which seeks to both encourage a female to approach and discourage other males from intruding on its territory. This call is modified to a quieter courtship call on the approach of a female or to a more aggressive version if a male intruder draws near. Calling carries the risk of attracting predators and involves the expenditure of much energy. Other calls include those given by a female in response to the advertisement call and a release call given by a male or female during unwanted attempts at amplexus. When a frog is attacked, a distress or fright call is emitted, often resembling a scream. The usually nocturnal Cuban tree frog (Osteopilus septentrionalis) produces a rain call when there is rainfall during daylight hours.

Little is known of the territorial behaviour of caecilians, but some frogs and salamanders defend home ranges. These are usually feeding, breeding or sheltering sites. Males normally exhibit such behaviour though in some species, females and even juveniles are also involved. Although in many frog species, females are larger than males, this is not the case in most species where males are actively involved in territorial defence. Some of these have specific adaptations such as enlarged teeth for biting or spines on the chest, arms or thumbs.
In salamanders, defence of a territory involves adopting an aggressive posture and if necessary attacking the intruder. This may involve snapping, chasing and sometimes biting, occasionally causing the loss of a tail. The behaviour of red back salamanders (Plethodon cinereus) has been much studied. 91% of marked individuals that were later recaptured were within a metre (yard) of their original daytime retreat under a log or rock. A similar proportion, when moved experimentally a distance of , found their way back to their home base. The salamanders left odour marks around their territories which averaged  in size and were sometimes inhabited by a male and female pair. These deterred the intrusion of others and delineated the boundaries between neighbouring areas. Much of their behaviour seemed stereotyped and did not involve any actual contact between individuals. An aggressive posture involved raising the body off the ground and glaring at the opponent who often turned away submissively. If the intruder persisted, a biting lunge was usually launched at either the tail region or the naso-labial grooves. Damage to either of these areas can reduce the fitness of the rival, either because of the need to regenerate tissue or because it impairs its ability to detect food.

In frogs, male territorial behaviour is often observed at breeding locations; calling is both an announcement of ownership of part of this resource and an advertisement call to potential mates. In general, a deeper voice represents a heavier and more powerful individual, and this may be sufficient to prevent intrusion by smaller males. Much energy is used in the vocalization and it takes a toll on the territory holder who may be displaced by a fitter rival if he tires. There is a tendency for males to tolerate the holders of neighbouring territories while vigorously attacking unknown intruders. Holders of territories have a "home advantage" and usually come off better in an encounter between two similar-sized frogs. If threats are insufficient, chest to chest tussles may take place. Fighting methods include pushing and shoving, deflating the opponent's vocal sac, seizing him by the head, jumping on his back, biting, chasing, splashing, and ducking him under the water.

Amphibians have soft bodies with thin skins, and lack claws, defensive armour, or spines. Nevertheless they have evolved various defence mechanisms to keep themselves alive. The first line of defence in salamanders and frogs is the mucous secretion that they produce. This keeps their skin moist and makes them slippery and difficult to grip. The secretion is often sticky and distasteful or toxic. Snakes have been observed yawning and gaping when trying to swallow African clawed frogs (Xenopus laevis), which gives the frogs an opportunity to escape. Caecilians have been little studied in this respect, but the Cayenne caecilian (Typhlonectes compressicauda) produces toxic mucus that has killed predatory fish in a feeding experiment in Brazil. In some salamanders, the skin is poisonous. The rough-skinned newt (Taricha granulosa) from North America and other members of its genus contain the neurotoxin tetrodotoxin (TTX), the most toxic non-protein substance known and almost identical to that produced by pufferfish. Handling the newts does not cause harm, but ingestion of even the most minute amounts of the skin is deadly. In feeding trials, fish, frogs, reptiles, birds and mammals were all found to be susceptible. The only predators with some tolerance to the poison are certain populations of common garter snake (Thamnophis sirtalis).
In locations where both snake and salamander co-exist, the snakes have developed immunity through genetic changes and they feed on the amphibians with impunity. Coevolution occurs with the newt increasing its toxic capabilities at the same rate as the snake further develops its immunity. Some frogs and toads are toxic, the main poison glands being at the side of the neck and under the warts on the back. These regions are presented to the attacking animal and their secretions may be foul-tasting or cause various physical or neurological symptoms. Altogether, over 200 toxins have been isolated from the limited number of amphibian species that have been investigated.
Poisonous species often use bright colouring to warn potential predators of their toxicity. These warning colours tend to be red or yellow combined with black, with the fire salamander (Salamandra salamandra) being an example. Once a predator has sampled one of these, it is likely to remember the colouration next time it encounters a similar animal. In some species, such as the fire-bellied toad (Bombina spp.), the warning colouration is on the belly and these animals adopt a defensive pose when attacked, exhibiting their bright colours to the predator. The frog Allobates zaparo is not poisonous, but mimics the appearance of other toxic species in its locality, a strategy that may deceive predators.

Many amphibians are nocturnal and hide during the day, thereby avoiding diurnal predators that hunt by sight. Other amphibians use camouflage to avoid being detected. They have various colourings such as mottled browns, greys and olives to blend into the background. Some salamanders adopt defensive poses when faced by a potential predator such as the North American northern short-tailed shrew (Blarina brevicauda). Their bodies writhe and they raise and lash their tails which makes it difficult for the predator to avoid contact with their poison-producing granular glands. A few salamanders will autotomise their tails when attacked, sacrificing this part of their anatomy to enable them to escape. The tail may have a constriction at its base to allow it to be easily detached. The tail is regenerated later, but the energy cost to the animal of replacing it is significant.

Some frogs and toads inflate themselves to make themselves look large and fierce, and some spadefoot toads (Pelobates spp) scream and leap towards the attacker. Giant salamanders of the genus Andrias, as well as Ceratophrine and Pyxicephalus frogs possess sharp teeth and are capable of drawing blood with a defensive bite. The blackbelly salamander (Desmognathus quadramaculatus) can bite an attacking common garter snake (Thamnophis sirtalis) two or three times its size on the head and often manages to escape.

Dramatic declines in amphibian populations, including population crashes and mass localized extinction, have been noted since the late 1980s from locations all over the world, and amphibian declines are thus perceived to be one of the most critical threats to global biodiversity. In 2004, the International Union for Conservation of Nature (IUCN) reported stating that currently birds, mammals, and amphibians extinction rates were at maximum 48 times greater than natural extinction ratesâ€”possibly 1,024 times higher.
In 2006 there were believed to be 4,035 species of amphibians that depended on water at some stage during their life cycle. Of these, 1,356 (33.6%) were considered to be threatened and this figure is likely to be an underestimate because it excludes 1,427 species for which there was insufficient data to assess their status. A number of causes are believed to be involved, including habitat destruction and modification, over-exploitation, pollution, introduced species, climate change, endocrine-disrupting pollutants, destruction of the ozone layer (ultraviolet radiation has shown to be especially damaging to the skin, eyes, and eggs of amphibians), and diseases like chytridiomycosis. However, many of the causes of amphibian declines are still poorly understood, and are a topic of ongoing discussion.
With their complex reproductive needs and permeable skins, amphibians are often considered to be ecological indicators. In many terrestrial ecosystems, they constitute one of the largest parts of the vertebrate biomass. Any decline in amphibian numbers will have an impact on the patterns of predation. The loss of carnivorous species near the top of the food chain will upset the delicate ecosystem balance and may cause dramatic increases in opportunistic species. In the Middle East, a growing appetite for eating frog legs and the consequent gathering of them for food was linked to an increase in mosquitoes. Predators that feed on amphibians are affected by their decline. The western terrestrial garter snake (Thamnophis elegans) in California is largely aquatic and depends heavily on two species of frog that are diminishing in numbers, the Yosemite toad (Bufo canorus) and the mountain yellow-legged frog (Rana muscosa), putting the snake's future at risk. If the snake were to become scarce, this would affect birds of prey and other predators that feed on it. Meanwhile, in the ponds and lakes, fewer frogs means fewer tadpoles. These normally play an important role in controlling the growth of algae and also forage on detritus that accumulates as sediment on the bottom. A reduction in the number of tadpoles may lead to an overgrowth of algae, resulting in depletion of oxygen in the water when the algae later die and decompose. Aquatic invertebrates and fish might then die and there would be unpredictable ecological consequences.

A global strategy to stem the crisis was released in 2005 in the form of the Amphibian Conservation Action Plan. Developed by over eighty leading experts in the field, this call to action details what would be required to curtail amphibian declines and extinctions over the following five years and how much this would cost. The Amphibian Specialist Group of the World Conservation Union (IUCN) is spearheading efforts to implement a comprehensive global strategy for amphibian conservation. Amphibian Ark is an organization that was formed to implement the ex-situ conservation recommendations of this plan, and they have been working with zoos and aquaria around the world, encouraging them to create assurance colonies of threatened amphibians. One such project is the Panama Amphibian Rescue and Conservation Project that built on existing conservation efforts in Panama to create a country-wide response to the threat of chytridiomycosis.

List of threatened reptiles and amphibians of the United States
List of amphibians

 â€“ AnimalSpot.net
 : available in vector, image and PDF formats


Alaska () is a U.S. state situated in the northwest extremity of the North American continent. Bordering the state to the east is the Canadian territory of Yukon and the Canadian province of British Columbia, the Arctic Ocean to the north, and the Pacific Ocean to the west and south, with Russia (specifically, Chukotka Autonomous Okrug and Kamchatka Krai) further west across the Bering Strait. Alaska is the largest state in the United States by area, the 4th least populous and the least densely populated of the 50 United States.  Approximately half of Alaska's 735,132 residents live within the Anchorage metropolitan area. Alaska's economy is dominated by the oil, natural gas, and fishing industries, resources which it has in abundance. Tourism is also a significant part of the economy.

Although it had been occupied for thousands of years by indigenous peoples, from the 18th century onward, European powers considered the territory of Alaska ripe for exploitation and trade. The United States purchased Alaska from the Russian Empire on March 30, 1867, for 7.2million U.S. dollars at approximately two cents per acre ($4.74/km2). The area went through several administrative changes before becoming organized as a territory on May 11, 1912. It was admitted as the 49th state of the U.S. on January 3, 1959.

The name "Alaska" (ÐÐ»ÑÑÐºÐ°) had been introduced in the Russian colonial period, when it was used to refer to the peninsula. It was derived from an Aleut, or Unangam idiom, which figuratively refers to the mainland of Alaska.  Literally, it means object to which the action of the sea is directed. It is also known as Alyeska, the "great land", an Aleut word derived from the same root.


Alaska has a longer coastline than all the other U.S. states combined. It is the only non-contiguous U.S. state on continental North America; about  of British Columbia (Canada) separates Alaska from Washington. Alaska is thus an exclave of the United States, possibly the largest exclave in the world. It is technically part of the continental U.S., but is often not included in colloquial use; Alaska is not part of the contiguous U.S., often called "the Lower 48". The capital city, Juneau, is situated on the mainland of the North American continent, but is not connected by road to the rest of the North American highway system.

The state is bordered by Yukon and British Columbia in Canada, to the east, the Gulf of Alaska and the Pacific Ocean to the south, the Bering Sea, Bering Strait, and Chukchi Sea to the west and the Arctic Ocean to the north. Alaska's territorial waters touch Russia's territorial waters in the Bering Strait, as the Russian Big Diomede Island and Alaskan Little Diomede Island are only  apart. With the extension of the Aleutian Islands into the eastern hemisphere, it is technically both the westernmost and easternmost state in the United States, as well as also being the northernmost.
Alaska is the largest state in the United States in land area at , over twice the size of Texas, the next largest state. Alaska is larger than all but 18 sovereign countries. Counting territorial waters, Alaska is larger than the combined area of the next three largest states: Texas, California, and Montana. It is also larger than the combined area of the 22 smallest U.S. states.

There are no officially defined borders demarcating the various regions of Alaska, but there are six widely accepted regions:


The most populous region of Alaska, containing Anchorage, the Matanuska-Susitna Valley and the Kenai Peninsula.  Rural, mostly unpopulated areas south of the Alaska Range and west of the Wrangell Mountains also fall within the definition of Southcentral, as well as the Prince William Sound area and the communities of Cordova and Valdez.


Also referred to as the Panhandle or Inside Passage, this is the region of Alaska closest to the rest of the United States.  As such, this was where most of the initial non-indigenous settlement occurred in the years following the Alaska Purchase.  The region is dominated by the Alexander Archipelago as well as the Tongass National Forest, the largest national forest in the United States.  It contains the state capital Juneau, the former capital Sitka, and Ketchikan, at one time Alaska's largest city. The Alaska Marine Highway provides a vital surface transportation link throughout the area, as only three communities (Haines, Hyder and Skagway) enjoy direct connections to the contiguous North American road system.

The largest region of Alaska, much of the interior is uninhabited wilderness. Fairbanks is the only large city in the region.  Small towns and Alaska Native villages are scattered throughout, mostly along the highway and river systems. Denali National Park and Preserve is located here, home to Mount McKinley (also widely known by its local name of Denali), the highest point in North America.

Southwest Alaska is a sparsely inhabited region stretching some  inland from the Bering Sea. Most of the population lives along the coast. Kodiak Island is also located in Southwest. The massive Yukonâ€“Kuskokwim Delta, one of the largest river deltas in the world, is here.  Portions of the Alaska Peninsula are considered part of Southwest, with the remaining portions included with the Aleutian Islands (see below).


The North Slope is mostly tundra peppered with small villages. The area is known for its massive reserves of crude oil, and contains both the National Petroleum Reserveâ€“Alaska and the Prudhoe Bay Oil Field. Barrow, the northernmost city in the United States, is located here. The Northwest Arctic area, anchored by Kotzebue and also containing the Kobuk River valley, is often regarded as being part of this region.  However, the respective Inupiat of the North Slope and of the Northwest Arctic seldom consider themselves to be one people.


More than 300 small, volcanic islands make up this chain, which stretches over  into the Pacific Ocean. The International Date Line was drawn west of 180Â° to keep the whole state, and thus the entire North American continent, within the same legal day.  However, because some of these islands fall in the Eastern Hemisphere, this makes Alaska the northernmost, easternmost and westernmost state in the union, with the southernmost state being Hawaii. Two of the islands, Attu and Kiska, were occupied by Japanese forces during World War II.


With its myriad islands, Alaska has nearly  of tidal shoreline. The Aleutian Islands chain extends west from the southern tip of the Alaska Peninsula. Many active volcanoes are found in the Aleutians and in coastal regions. Unimak Island, for example, is home to Mount Shishaldin, which is an occasionally smoldering volcano that rises to  above the North Pacific. It is the most perfect volcanic cone on Earth, even more symmetrical than Japan's Mount Fuji. The chain of volcanoes extends to Mount Spurr, west of Anchorage on the mainland. Geologists have identified Alaska as part of Wrangellia, a large region consisting of multiple states and Canadian provinces in the Pacific Northwest, which is actively undergoing continent building.

One of the world's largest tides occurs in Turnagain Arm, just south of Anchorageâ€“ tidal differences can be more than .
Alaska has more than three million lakes.Marshlands and wetland permafrost cover  (mostly in northern, western and southwest flatlands). Glacier ice covers some  of land and  of tidal zone. The Bering Glacier complex near the southeastern border with Yukon covers  alone. With over 100,000 glaciers, Alaska has half of all in the world.


According to an October 1998 report by the United States Bureau of Land Management, approximately 65% of Alaska is owned and managed by the U.S. federal government as public lands, including a multitude of national forests, national parks, and national wildlife refuges. Of these, the Bureau of Land Management manages , or 23.8% of the state. The Arctic National Wildlife Refuge is managed by the United States Fish and Wildlife Service. It is the world's largest wildlife refuge, comprising .

Of the remaining land area, the state of Alaska owns , its entitlement under the Alaska Statehood Act.  A portion of that acreage is occasionally ceded to organized boroughs, under the statutory provisions pertaining to newly formed boroughs.  Smaller portions are set aside for rural subdivisions and other homesteading-related opportunities. These are not very popular due to the often remote and roadless locations.  The University of Alaska, as a land grant university, also owns substantial acreage which it manages independently.

Another  are owned by 12 regional, and scores of local, Native corporations created under the Alaska Native Claims Settlement Act (ANCSA) of 1971. Regional Native corporation Doyon, Limited often promotes itself as the largest private landowner in Alaska in advertisements and other communications.  Provisions of ANCSA allowing the corporations' land holdings to be sold on the open market starting in 1991 were repealed before they could take effect. Effectively, the corporations hold title (including subsurface title in many cases, a privilege denied to individual Alaskans) but cannot sell the land. Individual Native allotments can be and are sold on the open market, however.

Various private interests own the remaining land, totaling about one percent of the state.  Alaska is, by a large margin, the state with the smallest percentage of private land ownership when Native corporation holdings are excluded.

The climate in Southeast Alaska is a mid-latitude oceanic climate (KÃ¶ppen climate classification: Cfb) in the southern sections and a subarctic oceanic climate (KÃ¶ppen Cfc) in the northern parts. On an annual basis, Southeast is both the wettest and warmest part of Alaska with milder temperatures in the winter and high precipitation throughout the year. Juneau averages over  of precipitation a year, and Ketchikan averages over . This is also the only region in Alaska in which the average daytime high temperature is above freezing during the winter months.

The climate of Anchorage and south central Alaska is mild by Alaskan standards due to the region's proximity to the seacoast. While the area gets less rain than southeast Alaska, it gets more snow, and days tend to be clearer. On average, Anchorage receives  of precipitation a year, with around  of snow, although there are areas in the south central which receive far more snow. It is a subarctic climate () due to its brief, cool summers.

The climate of Western Alaska is determined in large part by the Bering Sea and the Gulf of Alaska. It is a subarctic oceanic climate in the southwest and a continental subarctic climate farther north. The temperature is somewhat moderate considering how far north the area is. This region has a tremendous amount of variety in precipitation. An area stretching from the northern side of the Seward Peninsula to the Kobuk River valley (i.e., the region around Kotzebue Sound) is technically a desert, with portions receiving less than  of precipitation annually. On the other extreme, some locations between Dillingham and Bethel average around  of precipitation.

The climate of the interior of Alaska is subarctic. Some of the highest and lowest temperatures in Alaska occur around the area near Fairbanks. The summers may have temperatures reaching into the 90s Â°F (the low-to-mid 30s Â°C), while in the winter, the temperature can fall below . Precipitation is sparse in the Interior, often less than  a year, but what precipitation falls in the winter tends to stay the entire winter.

The highest and lowest recorded temperatures in Alaska are both in the Interior. The highest is  in Fort Yukon (which is just  inside the arctic circle) on June 27, 1915, making Alaska tied with Hawaii as the state with the lowest high temperature in the United States. The lowest official Alaska temperature is  in Prospect Creek on January 23, 1971, one degree above the lowest temperature recorded in continental North America (in Snag, Yukon, Canada).

The climate in the extreme north of Alaska is Arctic () with long, very cold winters and short, cool summers. Even in July, the average low temperature in Barrow is . Precipitation is light in this part of Alaska, with many places averaging less than  per year, mostly as snow which stays on the ground almost the entire year.

Numerous indigenous peoples occupied Alaska for thousands of years before the arrival of European peoples to the area. Linguistic and DNA studies done here have provided evidence for the settlement of North America by way of the Bering land bridge. The Tlingit people developed a society with a matrilineal kinship system of property inheritance and descent in what is today Southeast Alaska, along with parts of British Columbia and the Yukon. Also in Southeast were the Haida, now well known for their unique arts. The Tsimshian people came to Alaska from British Columbia in 1887, when President Grover Cleveland, and later the U.S. Congress, granted them permission to settle on Annette Island and found the town of Metlakatla. All three of these peoples, as well as other indigenous peoples of the Pacific Northwest Coast, experienced smallpox outbreaks from the late 18th through the mid-19th century, with the most devastating epidemics occurring in the 1830s and 1860s, resulting in high fatalities and social disruption.

The Aleutian Islands are still home to the Aleut people's seafaring society, although they were the first Native Alaskans to be exploited by Russians. Western and Southwestern Alaska are home to the Yup'ik, while their cousins the Alutiiq ~ Sugpiaq lived in what is now Southcentral Alaska. The Gwich'in people of the northern Interior region are Athabaskan and primarily known today for their dependence on the caribou within the much-contested Arctic National Wildlife Refuge. The North Slope and Little Diomede Island are occupied by the widespread Inupiat people.


Some researchers believe that the first Russian settlement in Alaska was established in the 17th century. According to this hypothesis, in 1648 several koches of Semyon Dezhnyov's expedition came ashore in Alaska by storm and founded this settlement. This hypothesis is based on the testimony of Chukchi geographer Nikolai Daurkin, who had visited Alaska in 1764â€“1765 and who had reported on a village on the Kheuveren River, populated by "bearded men" who "pray to the icons". Some modern researchers associate Kheuveren with Koyuk River.
The first European vessel to reach Alaska is generally held to be the St. Gabriel under the authority of the surveyor M. S. Gvozdev and assistant navigator I. Fyodorov on August 21, 1732 during an expedition of Siberian cossak A. F. Shestakov and Belorussian explorer Dmitry Pavlutsky (1729â€”1735).

Another European contact with Alaska occurred in 1741, when Vitus Bering led an expedition for the Russian Navy aboard the St. Peter. After his crew returned to Russia with sea otter pelts judged to be the finest fur in the world, small associations of fur traders began to sail from the shores of Siberia toward the Aleutian Islands. The first permanent European settlement was founded in 1784.

Between 1774 and 1800, Spain sent several expeditions to Alaska in order to assert its claim over the Pacific Northwest. In 1789 a Spanish settlement and fort were built in Nootka Sound. These expeditions gave names to places such as Valdez, Bucareli Sound, and Cordova. Later, the Russian-American Company carried out an expanded colonization program during the early-to-mid-19th century.

Sitka, renamed New Archangel from 1804 to 1867, on Baranof Island in the Alexander Archipelago in what is now Southeast Alaska, became the capital of Russian America. It remained the capital after the colony was transferred to the United States. The Russians never fully colonized Alaska, and the colony was never very profitable. Evidence of Russian settlement in names and churches survive throughout southeast Alaska.

William H. Seward, the United States Secretary of State, negotiated the Alaska Purchase (also known as Seward's Folly) with the Russians in 1867 for $7.2million. Alaska was loosely governed by the military initially, and was administered as a district starting in 1884, with a governor appointed by the President of the United States. A federal district court was headquartered in Sitka.

For most of Alaska's first decade under the United States flag, Sitka was the only community inhabited by American settlers. They organized a "provisional city government," which was Alaska's first municipal government, but not in a legal sense. Legislation allowing Alaskan communities to legally incorporate as cities did not come about until 1900, and home rule for cities was extremely limited or unavailable until statehood took effect in 1959.


Starting in the 1890s and stretching in some places to the early 1910s, gold rushes in Alaska and the nearby Yukon Territory brought thousands of miners and settlers to Alaska.  Alaska was officially incorporated as an organized territory in 1912.  Alaska's capital, which had been in Sitka until 1906, was moved north to Juneau. Construction of the Alaska Governor's Mansion began that same year. European immigrants from Norway and Sweden also settled in southeast Alaska, where they entered the fishing and logging industries.
During World War II, the Aleutian Islands Campaign focused on the three outer Aleutian Islandsâ€“ Attu, Agattu and Kiskaâ€“ that were invaded by Japanese troops and occupied between June 1942 and August 1943. Unalaska/Dutch Harbor became a significant base for the U.S. Army Air Forces and Navy submariners.

The U.S. Lend-Lease program involved the flying of American warplanes through Canada to Fairbanks and thence Nome; Soviet pilots took possession of these aircraft, ferrying them to fight the German invasion of the Soviet Union. The construction of military bases contributed to the population growth of some Alaskan cities.


Statehood for Alaska was an important cause of James Wickersham early in his tenure as a congressional delegate.  Decades later, the statehood movement gained its first real momentum following a territorial referendum in 1946.  The Alaska Statehood Committee and Alaska's Constitutional Convention would soon follow.  Statehood supporters also found themselves fighting major battles against political foes, mostly in the U.S. Congress but also within Alaska.  Statehood was approved by Congress on July 7, 1958. Alaska was officially proclaimed a state on January 3, 1959.
In 1960, the Census Bureau reported Alaska's population as 77.2% White, 3% Black, and 18.8% American Indian and Alaska Native.

On March 27, 1964, the massive "Good Friday Earthquake" killed 133 people and destroyed several villages and portions of large coastal communities, mainly by the resultant tsunamis and landslides. It was the second-most-powerful earthquake in the recorded history of the world, with a moment magnitude of 9.2. It was over one thousand times more powerful than the 1989 San Francisco earthquake.  The time of day (5:36pm), time of year and location of the epicenter were all cited as factors in potentially sparing thousands of lives, particularly in Anchorage.

The 1968 discovery of oil at Prudhoe Bay and the 1977 completion of the Trans-Alaska Pipeline led to an oil boom.  Royalty revenues from oil have funded large state budgets from 1980 onward.  That same year, not coincidentally, Alaska repealed its state income tax.

In 1989, the Exxon Valdez hit a reef in the Prince William Sound, spilling over  of crude oil over  of coastline. Today, the battle between philosophies of development and conservation is seen in the contentious debate over oil drilling in the Arctic National Wildlife Refuge and the proposed Pebble Mine.

The Alaska Heritage Resources Survey (AHRS) is a restricted inventory of all reported historic and prehistoric sites within the state of Alaska and is maintained by the Office of History and Archaeology. The survey's inventory of cultural resources includes objects, structures, buildings, sites, districts, and travel ways, with a general provision that they are over 50 years old. As of January 31, 2012 over 35,000 sites have been reported.

The United States Census Bureau estimates that the population of Alaska was 736,732 on July 1, 2014, a 3.73% increase since the 2010 United States Census.

In 2010, Alaska ranked as the 47th state by population, ahead of North Dakota, Vermont, and Wyoming (and Washington, D.C.) Alaska is the least densely populated state, and one of the most sparsely populated areas in the world, at , with the next state, Wyoming, at . Alaska is the largest U.S. state by area, and the tenth wealthiest (per capita income). As of November 2014, the state's unemployment rate was 6.6%.

According to the 2010 United States Census, Alaska had a population of 710,231. In terms of race and ethnicity, the state was 66.7% White (64.1% Non-Hispanic White), 14.8% American Indian and Alaska Native, 5.4% Asian, 3.3% Black or African American, 1.0% Native Hawaiian and Other Pacific Islander, 1.6% from Some Other Race, and 7.3% from Two or More Races. Hispanics or Latinos of any race made up 5.5% of the population.

, 50.7% of Alaska's population younger than one year of age belonged to minority groups (i.e., did not have two parents of non-Hispanic white ancestry).

According to the 2011 American Community Survey, 82.4% of people over the age of five speak only English at home. About 3.5% speak Spanish at home. About 2.2% speak another Indo-European language at home and about 4.3% speak an Asian language at home. About 5.3% speak other languages at home.

The Alaska Native Language Center at the University of Alaska Fairbanks claims that at least 20 Alaskan native languages exist and there are also some languages with different dialects. Most of Alaska's native languages belong to either the Eskimoâ€“Aleut or Na-Dene language families however some languages are thought to be isolates (e.g. Haida) or have not yet been classified (e.g. Tsimshianic).
 nearly all of Alaska's native languages were classified as either threatened, shifting, moribund, nearly extinct, or dormant languages.

A total of 5.2% of Alaskans speak one of the state's 20 indigenous languages, known locally as "native languages". As the homeland of these two major language families of North America, Alaska has been described as the crossroads of the continent.

In October 2014, the governor of Alaska signed a bill declaring the state's 20 indigenous languages as official languages.


According to statistics collected by the Association of Religion Data Archives from 2010, about 34% of Alaska residents were members of religious congregations. 100,960 people identified as Evangelical Protestants, 50,866 as Roman Catholic, and 32,550 as mainline Protestants. Roughly 4% are Mormon, 0.5% are Jewish, 1% are Muslim, 0.5% are Buddhist, and 0.5% are Hindu.

The largest religious denominations in Alaska  were the Catholic Church with 50,866 adherents, non-denominational Evangelical Protestants with 38,070 adherents, The Church of Jesus Christ of Latter-day Saints with 32,170 adherents, and the Southern Baptist Convention with 19,891 adherents.
In 1795, the First Russian Orthodox Church was established in Kodiak. Intermarriage with Alaskan Natives helped the Russian immigrants integrate into society. As a result, an increasing number of Russian Orthodox churches gradually became established within Alaska. Alaska also has the largest Quaker population (by percentage) of any state. In 2009 there were 6,000 Jews in Alaska (for whom observance of halakha may pose special problems). Estimates for the number of Alaskan Muslims range from 2,000 to 5,000. In 2010, the local Muslim community broke ground on the first mosque in the state. Alaskan Hindus often share venues and celebrations with members of other Asian religious communities, including Sikhs and Jains.

Alaska has been identified, along with Pacific Northwest states Washington and Oregon, as being the least religious states of the USA, in terms of church membership.

The 2007 gross state product was $44.9billion, 45th in the nation. Its per capita personal income for 2007 was $40,042, ranking 15th in the nation. According to a 2013 study by Phoenix Marketing International, Alaska had the fifth-largest number of millionaires per capita in the United States, with a ratio of 6.75 percent. The oil and gas industry dominates the Alaskan economy, with more than 80% of the state's revenues derived from petroleum extraction. Alaska's main export product (excluding oil and natural gas) is seafood, primarily salmon, cod, Pollock and crab.

Agriculture represents a very small fraction of the Alaskan economy. Agricultural production is primarily for consumption within the state and includes nursery stock, dairy products, vegetables, and livestock. Manufacturing is limited, with most foodstuffs and general goods imported from elsewhere.

Employment is primarily in government and industries such as natural resource extraction, shipping, and transportation. Military bases are a significant component of the economy in both Fairbanks and Anchorage. Federal subsidies are also an important part of the economy, allowing the state to keep taxes low. Its industrial outputs are crude petroleum, natural gas, coal, gold, precious metals, zinc and other mining, seafood processing, timber and wood products. There is also a growing service and tourism sector. Tourists have contributed to the economy by supporting local lodging.

Alaska has vast energy resources, although its oil reserves have been largely depleted. Major oil and gas reserves were found in the Alaska North Slope (ANS) and Cook Inlet basins, but according to the Energy Information Administration, by 2014 Alaska had fallen to fourth place in the nation in crude oil production after Texas, North Dakota, and California. Prudhoe Bay on Alaska's North Slope is still the second highest-yielding oil field in the United States, typically producing about , although by early 2014 North Dakota's Bakken Formation was producing over . Prudhoe Bay was the largest conventional oil field ever discovered in North America, but was much smaller than Canada's enormous Athabasca oil sands field, which by 2014 was producing about  of unconventional oil, and had hundreds of years of producible reserves at that rate.

The Trans-Alaska Pipeline can transport and pump up to  of crude oil per day, more than any other crude oil pipeline in the United States. Additionally, substantial coal deposits are found in Alaska's bituminous, sub-bituminous, and lignite coal basins.  The United States Geological Survey estimates that there are  of undiscovered, technically recoverable gas from natural gas hydrates on the Alaskan North Slope. Alaska also offers some of the highest hydroelectric power potential in the country from its numerous rivers. Large swaths of the Alaskan coastline offer wind and geothermal energy potential as well.

Alaska's economy depends heavily on increasingly expensive diesel fuel for heating, transportation, electric power and light. Though wind and hydroelectric power are abundant and underdeveloped, proposals for statewide energy systems (e.g. with special low-cost electric interties) were judged uneconomical (at the time of the report, 2001) due to low (less than 50Â¢/gal) fuel prices, long distances and low population. The cost of a gallon of gas in urban Alaska today is usually 30â€“60Â¢ higher than the national average; prices in rural areas are generally significantly higher but vary widely depending on transportation costs, seasonal usage peaks, nearby petroleum development infrastructure and many other factors.
Alaska was 4th in domestically produced United States oil in February 2014.

The Alaska Permanent Fund is a constitutionally authorized appropriation of oil revenues, established by voters in 1976 to manage a surplus in state petroleum revenues from oil, largely in anticipation of the recently constructed Trans-Alaska Pipeline System.  The fund was originally proposed by Governor Keith Miller on the eve of the 1969 Prudhoe Bay lease sale, out of fear that the legislature would spend the entire proceeds of the sale (which amounted to $900 million) at once. It was later championed by Governor Jay Hammond and Kenai state representative Hugh Malone.  It has served as an attractive political prospect ever since, diverting revenues which would normally be deposited into the general fund.

The Alaska Constitution was written so as to discourage dedicating state funds for a particular purpose.  The Permanent Fund has become the rare exception to this, mostly due to the political climate of distrust existing during the time of its creation.  From its initial principal of $734,000, the fund has grown to $50billion as a result of oil royalties and capital investment programs. Most if not all the principal is invested conservatively outside Alaska.  This has led to frequent calls by Alaskan politicians for the Fund to make investments within Alaska, though such a stance has never gained momentum.

Starting in 1982, dividends from the fund's annual growth have been paid out each year to eligible Alaskans, ranging from an initial $1,000 in 1982 (equal to three years' payout, as the distribution of payments was held up in a lawsuit over the distribution scheme) to $3,269 in 2008 (which included a one-time $1,200 "Resource Rebate"). Every year, the state legislature takes out 8% from the earnings, puts 3% back into the principal for inflation proofing, and the remaining 5% is distributed to all qualifying Alaskans. To qualify for the Permanent Fund Dividend, one must have lived in the state for a minimum of 12 months, maintain constant residency subject to allowable absences, and not be subject to court judgments or criminal convictions which fall under various disqualifying classifications or may subject the payment amount to civil garnishment.

The cost of goods in Alaska has long been higher than in the contiguous 48 states. Federal government employees, particularly United States Postal Service (USPS) workers and active-duty military members, receive a Cost of Living Allowance usually set at 25% of base pay because, while the cost of living has gone down, it is still one of the highest in the country.

Rural Alaska suffers from extremely high prices for food and consumer goods compared to the rest of the country, due to the relatively limited transportation infrastructure.

Due to the northern climate and short growing season, relatively little farming occurs in Alaska. Most farms are in either the Matanuska Valley, about  northeast of Anchorage, or on the Kenai Peninsula, about  southwest of Anchorage. The short 100-day growing season limits the crops that can be grown, but the long sunny summer days make for productive growing seasons. The primary crops are potatoes, carrots, lettuce, and cabbage.

The Tanana Valley is another notable agricultural locus, especially the Delta Junction area, about  southeast of Fairbanks, with a sizable concentration of farms growing agronomic crops; these farms mostly lie north and east of Fort Greely.  This area was largely set aside and developed under a state program spearheaded by Hammond during his second term as governor. Delta-area crops consist predominately of barley and hay. West of Fairbanks lies another concentration of small farms catering to restaurants, the hotel and tourist industry, and community-supported agriculture.

Alaskan agriculture has experienced a surge in growth of market gardeners, small farms and farmers' markets in recent years, with the highest percentage increase (46%) in the nation in growth in farmers' markets in 2011, compared to 17% nationwide. The peony industry has also taken off, as the growing season allows farmers to harvest during a gap in supply elsewhere in the world, thereby filling a niche in the flower market.
Alaska, with no counties, lacks county fairs.  However, a small assortment of state and local fairs (with the Alaska State Fair in Palmer the largest), are held mostly in the late summer.  The fairs are mostly located in communities with historic or current agricultural activity, and feature local farmers exhibiting produce in addition to more high-profile commercial activities such as carnival rides, concerts and food.  "Alaska Grown" is used as an agricultural slogan.

Alaska has an abundance of seafood, with the primary fisheries in the Bering Sea and the North Pacific. Seafood is one of the few food items that is often cheaper within the state than outside it.  Many Alaskans take advantage of salmon seasons to harvest portions of their household diet while fishing for subsistence, as well as sport.  This includes fish taken by hook, net or wheel.

Hunting for subsistence, primarily caribou, moose, and Dall sheep is still common in the state, particularly in remote Bush communities. An example of a traditional native food is Akutaq, the Eskimo ice cream, which can consist of reindeer fat, seal oil, dried fish meat and local berries.

Alaska's reindeer herding is concentrated on Seward Peninsula, where wild caribou can be prevented from mingling and migrating with the domesticated reindeer.

Most food in Alaska is transported into the state from "Outside", and shipping costs make food in the cities relatively expensive. In rural areas, subsistence hunting and gathering is an essential activity because imported food is prohibitively expensive. Though most small towns and villages in Alaska lie along the coastline, the cost of importing food to remote villages can be high, because of the terrain and difficult road conditions, which change dramatically, due to varying climate and precipitation changes. The cost of transport can reach as high as 50Â¢ per pound ($1.10/kg) or more in some remote areas, during the most difficult times, if these locations can be reached at all during such inclement weather and terrain conditions. The cost of delivering a  of milk is about $3.50 in many villages where per capita income can be $20,000 or less. Fuel cost per gallon is routinely 20â€“30Â¢ higher than the continental United States average, with only Hawaii having higher prices.

Alaska has few road connections compared to the rest of the U.S. The state's road system covers a relatively small area of the state, linking the central population centers and the Alaska Highway, the principal route out of the state through Canada. The state capital, Juneau, is not accessible by road, only a car ferry, which has spurred several debates over the decades about moving the capital to a city on the road system, or building a road connection from Haines. The western part of Alaska has no road system connecting the communities with the rest of Alaska.

One unique feature of the Alaska Highway system is the Anton Anderson Memorial Tunnel, an active Alaska Railroad tunnel recently upgraded to provide a paved roadway link with the isolated community of Whittier on Prince William Sound to the Seward Highway about  southeast of Anchorage at Portage. At , the tunnel was the longest road tunnel in North America until 2007. The tunnel is the longest combination road and rail tunnel in North America.

Built around 1915, the Alaska Railroad (ARR) played a key role in the development of Alaska through the 20th century. It links north Pacific shipping through providing critical infrastructure with tracks that run from Seward to Interior Alaska by way of South Central Alaska, passing through Anchorage, Eklutna, Wasilla, Talkeetna, Denali, and Fairbanks, with spurs to Whittier, Palmer and North Pole. The cities, towns, villages, and region served by ARR tracks are known statewide as "The Railbelt". In recent years, the ever-improving paved highway system began to eclipse the railroad's importance in Alaska's economy.

The railroad played a vital role in Alaska's development, moving freight into Alaska while transporting natural resources southward (i.e., coal from the Usibelli coal mine near Healy to Seward and gravel from the Matanuska Valley to Anchorage). It is well known for its summertime tour passenger service.

The Alaska Railroad was one of the last railroads in North America to use cabooses in regular service and still uses them on some gravel trains. It continues to offer one of the last flag stop routes in the country. A stretch of about  of track along an area north of Talkeetna remains inaccessible by road; the railroad provides the only transportation to rural homes and cabins in the area. Until construction of the Parks Highway in the 1970s, the railroad provided the only land access to most of the region along its entire route.

In northern Southeast Alaska, the White Pass and Yukon Route also partly runs through the state from Skagway northwards into Canada (British Columbia and Yukon Territory), crossing the border at White Pass Summit. This line is now mainly used by tourists, often arriving by cruise liner at Skagway. It was featured in the 1983 BBC television series Great Little Railways.

The Alaska Rail network is not connected to Outside. In 2000, the U.S. Congress authorized $6million to study the feasibility of a rail link between Alaska, Canada, and the lower 48.

Alaska Rail Marine provides car float service between Whittier and Seattle.

Many cities, towns and villages in the state do not have road or highway access; the only modes of access involve travel by air, river, or the sea.

Alaska's well-developed state-owned ferry system (known as the Alaska Marine Highway) serves the cities of southeast, the Gulf Coast and the Alaska Peninsula. The ferries transport vehicles as well as passengers. The system also operates a ferry service from Bellingham, Washington and Prince Rupert, British Columbia in Canada through the Inside Passage to Skagway. The Inter-Island Ferry Authority also serves as an important marine link for many communities in the Prince of Wales Island region of Southeast and works in concert with the Alaska Marine Highway.

In recent years, cruise lines have created a summertime tourism market, mainly connecting the Pacific Northwest to Southeast Alaska and, to a lesser degree, towns along Alaska's gulf coast. The population of Ketchikan may rise by over 10,000 people on many days during the summer, as up to four large cruise ships at a time can dock, debarking thousands of passengers.

Cities not served by road, sea, or river can be reached only by air, foot, dogsled, or snowmachine, accounting for Alaska's extremely well developed bush air servicesâ€”an Alaskan novelty. Anchorage and, to a lesser extent Fairbanks, is served by many major airlines. Because of limited highway access, air travel remains the most efficient form of transportation in and out of the state. Anchorage recently completed extensive remodeling and construction at Ted Stevens Anchorage International Airport to help accommodate the upsurge in tourism (in 2012-2013, Alaska received almost 2 million visitors).

Regular flights to most villages and towns within the state that are commercially viable are challenging to provide, so they are heavily subsidized by the federal government through the Essential Air Service program. Alaska Airlines is the only major airline offering in-state travel with jet service (sometimes in combination cargo and passenger Boeing 737-400s) from Anchorage and Fairbanks to regional hubs like Bethel, Nome, Kotzebue, Dillingham, Kodiak, and other larger communities as well as to major Southeast and Alaska Peninsula communities.
The bulk of remaining commercial flight offerings come from small regional commuter airlines such as Ravn Alaska, PenAir, and Frontier Flying Service. The smallest towns and villages must rely on scheduled or chartered bush flying services using general aviation aircraft such as the Cessna Caravan, the most popular aircraft in use in the state. Much of this service can be attributed to the Alaska bypass mail program which subsidizes bulk mail delivery to Alaskan rural communities. The program requires 70% of that subsidy to go to carriers who offer passenger service to the communities.

Many communities have small air taxi services.  These operations originated from the demand for customized transport to remote areas.  Perhaps the most quintessentially Alaskan plane is the bush seaplane. The world's busiest seaplane base is Lake Hood, located next to Ted Stevens Anchorage International Airport, where flights bound for remote villages without an airstrip carry passengers, cargo, and many items from stores and warehouse clubs. In 2006 Alaska had the highest number of pilots per capita of any U.S. state.

Another Alaskan transportation method is the dogsled. In modern times (that is, any time after the mid-late 1920s), dog mushing is more of a sport than a true means of transportation. Various races are held around the state, but the best known is the Iditarod Trail Sled Dog Race, a  trail from Anchorage to Nome (although the distance varies from year to year, the official distance is set at ). The race commemorates the famous 1925 serum run to Nome in which mushers and dogs like Togo and Balto took much-needed medicine to the diphtheria-stricken community of Nome when all other means of transportation had failed. Mushers from all over the world come to Anchorage each March to compete for cash, prizes, and prestige. The "Serum Run" is another sled dog race that more accurately follows the route of the famous 1925 relay, leaving from the community of Nenana (southwest of Fairbanks) to Nome.

In areas not served by road or rail, primary transportation in summer is by all-terrain vehicle and in winter by snowmobile or "snow machine," as it is commonly referred to in Alaska.

Alaska's internet and other data transport systems are provided largely through the two major telecommunications companies: GCI and Alaska Communications. GCI owns and operates what it calls the Alaska United Fiber Optic system and as of late 2011 Alaska Communications advertised that it has "two fiber optic paths to the lower 48 and two more across Alaska. In January 2011, it was reported that a $1 billion project to run connect Asia and rural Alaska was being planned, aided in part by $350 million in stimulus from the federal government.


Like all other U.S. states, Alaska is governed as a republic, with three branches of government: an executive branch consisting of the Governor of Alaska and the other independently elected constitutional officers; a legislative branch consisting of the Alaska House of Representatives and Alaska Senate; and a judicial branch consisting of the Alaska Supreme Court and lower courts.

The state of Alaska employs approximately 16,000 people statewide.

The Alaska Legislature consists of a 40-member House of Representatives and a 20-member Senate. Senators serve four-year terms and House members two. The Governor of Alaska serves four-year terms. The lieutenant governor runs separately from the governor in the primaries, but during the general election, the nominee for governor and nominee for lieutenant governor run together on the same ticket.

Alaska's court system has four levels: the Alaska Supreme Court, the Alaska Court of Appeals, the superior courts and the district courts. The superior and district courts are trial courts. Superior courts are courts of general jurisdiction, while district courts only hear certain types of cases, including misdemeanor criminal cases and civil cases valued up to $100,000.

The Supreme Court and the Court of Appeals are appellate courts. The Court of Appeals is required to hear appeals from certain lower-court decisions, including those regarding criminal prosecutions, juvenile delinquency, and habeas corpus. The Supreme Court hears civil appeals and may in its discretion hear criminal appeals.

Although in its early years of statehood Alaska was a Democratic state, since the early 1970s it has been characterized as Republican-leaning. Local political communities have often worked on issues related to land use development, fishing, tourism, and individual rights. Alaska Natives, while organized in and around their communities, have been active within the Native corporations. These have been given ownership over large tracts of land, which require stewardship.

Alaska was formerly the only state in which possession of one ounce or less of marijuana in one's home was completely legal under state law, though the federal law remains in force.

The state has an independence movement favoring a vote on secession from the United States, with the Alaskan Independence Party.

Six Republicans and four Democrats have served as governor of Alaska.  In addition, Republican Governor Wally Hickel was elected to the office for a second term in 1990 after leaving the Republican party and briefly joining the Alaskan Independence Party ticket just long enough to be reelected. He subsequently officially rejoined the Republican party in 1994.

Alaska's voter initiative making marijuana legal takes effect 24 February 2015, placing Alaska alongside Colorado and Washington as the three U.S. states where recreational marijuana is legal. The new law means people over age 21 can consume small amounts of pot â€” if they can find it. It's still illegal to sell marijuana.

To finance state government operations, Alaska depends primarily on petroleum revenues and federal subsidies. This allows it to have the lowest individual tax burden in the United States. It is one of five states with no state sales tax, one of seven states that do not levy an individual income tax, and one of the two states that has neither. The Department of Revenue Tax Division reports regularly on the state's revenue sources. The Department also issues an annual summary of its operations, including new state laws that directly affect the tax division.

While Alaska has no state sales tax, 89 municipalities collect a local sales tax, from 1.0â€“7.5%, typically 3â€“5%. Other local taxes levied include raw fish taxes, hotel, motel, and bed-and-breakfast 'bed' taxes, severance taxes, liquor and tobacco taxes, gaming (pull tabs) taxes, tire taxes and fuel transfer taxes. A part of the revenue collected from certain state taxes and license fees (such as petroleum, aviation motor fuel, telephone cooperative) is shared with municipalities in Alaska.

Fairbanks has one of the highest property taxes in the state as no sales or income taxes are assessed in the Fairbanks North Star Borough (FNSB). A sales tax for the FNSB has been voted on many times, but has yet to be approved, leading law makers to increase taxes dramatically on goods such as liquor and tobacco.

In 2014 the Tax Foundation ranked Alaska as having the fourth most "business friendly" tax policy, behind only Wyoming, South Dakota, and Nevada.

Alaska regularly supports Republicans in presidential elections and has done so since statehood. Republicans have won the state's electoral college votes in all but one election that it has participated in (1964). No state has voted for a Democratic presidential candidate fewer times. Alaska was carried by Democratic nominee Lyndon B. Johnson during his landslide election in 1964, while the 1960 and 1968 elections were close. Since 1972, however, Republicans have carried the state by large margins. In 2008, Republican John McCain defeated Democrat Barack Obama in Alaska, 59.49% to 37.83%. McCain's running mate was Sarah Palin, the state's governor and the first Alaskan on a major party ticket. Obama lost Alaska again in 2012, but he captured 40% of the state's vote in that election, making him the first Democrat to do so since 1968.

The Alaska Bush, central Juneau, midtown and downtown Anchorage, and the areas surrounding the University of Alaska Fairbanks campus and Ester have been strongholds of the Democratic Party. The Matanuska-Susitna Borough, the majority of Fairbanks (including North Pole and the military base), and South Anchorage typically have the strongest Republican showing. , well over half of all registered voters have chosen "Non-Partisan" or "Undeclared" as their affiliation, despite recent attempts to close primaries to unaffiliated voters.

Because of its population relative to other U.S. states, Alaska has only one member in the U.S. House of Representatives. This seat is held by Republican Don Young, who was re-elected to his 21st consecutive term in 2012. Alaska's At-large congressional district is one of the largest parliamentary constituencies in the world.

In 2008, Governor Sarah Palin became the first Republican woman to run on a national ticket when she became John McCain's running mate.  She continued to be a prominent national figure even after resigning from the governor's job in July 2009.

Alaska's United States Senators belong to Class 2 and Class 3.  In 2008, Democrat Mark Begich, mayor of Anchorage, defeated long-time Republican senator Ted Stevens. Stevens had been convicted on seven felony counts of failing to report gifts on Senate financial discloser forms one week before the election. The conviction was set aside in April 2009 after evidence of prosecutorial misconduct emerged.

Republican Frank Murkowski held the state's other senatorial position. After being elected governor in 2002, he resigned from the Senate and appointed his daughter, State Representative Lisa Murkowski as his successor. She won full six-year terms in 2004 and 2010.
Alaska is not divided into counties, as most of the other U.S. states, but it is divided into boroughs. Many of the more densely populated parts of the state are part of Alaska's 16 boroughs, which function somewhat similarly to counties in other states. However, unlike county-equivalents in the other 49 states, the boroughs do not cover the entire land area of the state. The area not part of any borough is referred to as the Unorganized Borough.

The Unorganized Borough has no government of its own, but the U.S. Census Bureau in cooperation with the state divided the Unorganized Borough into 11 census areas solely for the purposes of statistical analysis and presentation. A recording district is a mechanism for administration of the public record in Alaska. The state is divided into 34 recording districts which are centrally administered under a State Recorder. All recording districts use the same acceptance criteria, fee schedule, etc., for accepting documents into the public record.

Whereas many U.S. states use a three-tiered system of decentralizationâ€”state/county/townshipâ€”most of Alaska uses only two tiersâ€”state/borough. Owing to the low population density, most of the land is located in the Unorganized Borough. As the name implies, it has no intermediate borough government but is administered directly by the state government. In 2000, 57.71% of Alaska's area has this status, with 13.05% of the population.

Anchorage merged the city government with the Greater Anchorage Area Borough in 1975 to form the Municipality of Anchorage, containing the city proper and the communities of Eagle River, Chugiak, Peters Creek, Girdwood, Bird, and Indian. Fairbanks has a separate borough (the Fairbanks North Star Borough) and municipality (the City of Fairbanks).

The state's most populous city is Anchorage, home to 278,700 people in 2006, 225,744 of whom live in the urbanized area. The richest location in Alaska by per capita income is Halibut Cove ($89,895). Yakutat City, Sitka, Juneau, and Anchorage are the four largest cities in the U.S. by area.

As reflected in the 2010 United States Census, Alaska has a total of 355 incorporated cities and census-designated places (CDPs).  The tally of cities includes four unified municipalities, essentially the equivalent of a consolidated cityâ€“county.  The majority of these communities are located in the rural expanse of Alaska known as "The Bush" and are unconnected to the contiguous North American road network.  The table at the bottom of this section lists the 100 largest cities and census-designated places in Alaska, in population order.

Of Alaska's 2010 Census population figure of 710,231, 20,429 people, or 2.88% of the population, did not live in an incorporated city or census-designated place.  Approximately three-quarters of that figure were people who live in urban and suburban neighborhoods on the outskirts of the city limits of Ketchikan, Kodiak, Palmer and Wasilla.  CDPs have not been established for these areas by the United States Census Bureau, except that seven CDPs were established for the Ketchikan-area neighborhoods in the 1980 Census (Clover Pass, Herring Cove, Ketchikan East, Mountain Point, North Tongass Highway, Pennock Island and Saxman East), but have not been used since.  The remaining population was scattered throughout Alaska, both within organized boroughs and in the Unorganized Borough, in largely remote areas.

The Alaska Department of Education and Early Development administers many school districts in Alaska. In addition, the state operates a boarding school, Mt. Edgecumbe High School in Sitka, and provides partial funding for other boarding schools, including Nenana Student Living Center in Nenana and The Galena Interior Learning Academy in Galena.

There are more than a dozen colleges and universities in Alaska. Accredited universities in Alaska include the University of Alaska Anchorage, University of Alaska Fairbanks, University of Alaska Southeast, and Alaska Pacific University. Alaska is the only state that has no institutions that are part of the NCAA Division I.

The Alaska Department of Labor and Workforce Development operates AVTEC, Alaska's Institute of Technology. Campuses in Seward and Anchorage offer 1 week to 11-month training programs in areas as diverse as Information Technology, Welding, Nursing, and Mechanics.

Alaska has had a problem with a "brain drain". Many of its young people, including most of the highest academic achievers, leave the state after high school graduation and do not return. , Alaska did not have a law school or medical school. The University of Alaska has attempted to combat this by offering partial four-year scholarships to the top 10% of Alaska high school graduates, via the Alaska Scholars Program.


The Alaska State Troopers are Alaska's statewide police force. They have a long and storied history, but were not an official organization until 1941. Before the force was officially organized, law enforcement in Alaska was handled by various federal agencies. Larger towns usually have their own local police and some villages rely on "Public Safety Officers" who have police training but do not carry firearms. In much of the state, the troopers serve as the only police force available. In addition to enforcing traffic and criminal law, wildlife Troopers enforce hunting and fishing regulations. Due to the varied terrain and wide scope of the Troopers' duties, they employ a wide variety of land, air, and water patrol vehicles.

Many rural communities in Alaska are considered "dry," having outlawed the importation of alcoholic beverages. Suicide rates for rural residents are higher than urban.

Domestic abuse and other violent crimes are also at high levels in the state; this is in part linked to alcohol abuse. Alaska has the highest rate of sexual assault in the nation, especially in rural areas. The average age of sexually assaulted victims is 16 years old. In four out of five cases, the suspects were relatives, friends or acquaintances.

Some of Alaska's popular annual events are the Iditarod Trail Sled Dog Race that starts in Anchorage and ends in Nome, World Ice Art Championships in Fairbanks, the Blueberry Festival and Alaska Hummingbird Festival in Ketchikan, the Sitka Whale Fest, and the Stikine River Garnet Fest in Wrangell. The Stikine River attracts the largest springtime concentration of American bald eagles in the world.

The Alaska Native Heritage Center celebrates the rich heritage of Alaska's 11 cultural groups. Their purpose is to encourage cross-cultural exchanges among all people and enhance self-esteem among Native people. The Alaska Native Arts Foundation promotes and markets Native art from all regions and cultures in the State, using the internet.


Influences on music in Alaska include the traditional music of Alaska Natives as well as folk music brought by later immigrants from Russia and Europe. Prominent musicians from Alaska include singer Jewel, traditional Aleut flautist Mary Youngblood, folk singer-songwriter Libby Roderick, Christian music singer/songwriter Lincoln Brewster, metal/post hardcore band 36 Crazyfists and the groups Pamyua and Portugal. The Man.

There are many established music festivals in Alaska, including the Alaska Folk Festival, the Fairbanks Summer Arts Festival, the , the Athabascan Old-Time Fiddling Festival, the Sitka Jazz Festival, and the Sitka Summer Music Festival. The most prominent orchestra in Alaska is the Anchorage Symphony Orchestra, though the Fairbanks Symphony Orchestra and Juneau Symphony are also notable. The Anchorage Opera is currently the state's only professional opera company, though there are several volunteer and semi-professional organizations in the state as well.

The official state song of Alaska is "Alaska's Flag", which was adopted in 1955; it celebrates the flag of Alaska.

Alaska's first independent picture entirely made in Alaska was The Chechahcos, produced by Alaskan businessman Austin E. Lathrop and filmed in and around Anchorage. Released in 1924 by the Alaska Moving Picture Corporation, it was the only film the company made.

One of the most prominent movies filmed in Alaska is MGM's Eskimo/Mala The Magnificent, starring Alaska Native Ray Mala. In 1932 an expedition set out from MGM's studios in Hollywood to Alaska to film what was then billed as "The Biggest Picture Ever Made."  Upon arriving in Alaska, they set up "Camp Hollywood" in Northwest Alaska, where they lived during the duration of the filming. Louis B. Mayer spared no expense in spite of the remote location, going so far as to hire the chef from the Hotel Roosevelt in Hollywood to prepare meals.

When Eskimo premiered at the Astor Theatre in New York City, the studio received the largest amount of feedback in its history to that point. Eskimo was critically acclaimed and released worldwide; as a result, Mala became an international movie star. Eskimo won the first Oscar for Best Film Editing at the Academy Awards, and showcased and preserved aspects of Inupiat culture on film.

The 1983 Disney movie Never Cry Wolf was at least partially shot in Alaska. The 1991 film White Fang, based on Jack London's novel and starring Ethan Hawke, was filmed in and around Haines. Steven Seagal's 1994 On Deadly Ground, starring Michael Caine, was filmed in part at the Worthington Glacier near Valdez. The 1999 John Sayles film Limbo, starring David Strathairn, Mary Elizabeth Mastrantonio, and Kris Kristofferson, was filmed in Juneau.

The psychological thriller Insomnia, starring Al Pacino and Robin Williams, was shot in Canada, but was set in Alaska. The 2007 film directed by Sean Penn, Into The Wild, was partially filmed and set in Alaska. The film, which is based on the novel of the same name, follows the adventures of Christopher McCandless, who died in a remote abandoned bus along the Stampede Trail west of Healy in 1992.

Many films and television shows set in Alaska are not filmed there; for example, Northern Exposure, set in the fictional town of Cicely, Alaska, was filmed in Roslyn, Washington. The 2007 horror feature 30 Days of Night is set in Barrow, but was filmed in New Zealand.

Many reality television shows are filmed in Alaska. In 2011 the Anchorage Daily News found ten set in the state.

State motto: North to the Future
Nicknames: "The Last Frontier" or "Land of the Midnight Sun" or "Seward's Icebox"
State bird: willow ptarmigan, adopted by the Territorial Legislature in 1955. It is a small () Arctic grouse that lives among willows and on open tundra and muskeg. Plumage is brown in summer, changing to white in winter. The willow ptarmigan is common in much of Alaska.
State fish: king salmon, adopted 1962.
State flower: wild/native forget-me-not, adopted by the Territorial Legislature in 1917. It is a perennial that is found throughout Alaska, from Hyder to the Arctic Coast, and west to the Aleutians.
State fossil: woolly mammoth, adopted 1986.
State gem: jade, adopted 1968.
State insect: four-spot skimmer dragonfly, adopted 1995.
State land mammal: moose, adopted 1998.
State marine mammal: bowhead whale, adopted 1983.
State mineral: gold, adopted 1968.
State song: "Alaska's Flag"
State sport: dog mushing, adopted 1972.
State tree: Sitka spruce, adopted 1962.
State dog: Alaskan Malamute, adopted 2010.
State soil: Tanana, adopted unknown.


Index of Alaska-related articles
Outline of Alaska â€“ organized list of topics about Alaska
Sports in Alaska
, project area of the American Land Conservancy
US federal government
Alaska state government

â€“ Annotated list of searchable databases produced by Alaska state agencies and compiled by the Government Documents Roundtable of the American Library Association.


Agriculture is the cultivation of animals, plants, fungi, and other life forms for food, fiber, biofuel, medicinal and other products used to sustain and enhance human life. Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that nurtured the development of civilization. The study of agriculture is known as agricultural science. The history of agriculture dates back thousands of years, and its development has been driven and defined by greatly different climates, cultures, and technologies. However, all farming generally relies on techniques to expand and maintain the lands that are suitable for raising domesticated species. For plants, this usually requires some form of irrigation, although there are methods of dryland farming. Livestock are raised in a combination of grassland-based and landless systems, in an industry that covers almost one-third of the world's ice- and water-free area. In the developed world, industrial agriculture based on large-scale monoculture has become the dominant system of modern farming, although there is growing support for sustainable agriculture, including permaculture and organic agriculture.

Until the Industrial Revolution, the vast majority of the human population labored in agriculture. Pre-industrial agriculture was typically subsistence agriculture/self-sufficiency in which farmers raised most of their crops for their own consumption instead of cash crops for trade. A remarkable shift in agricultural practices has occurred over the past century in response to new technologies, and the development of world markets. This also has led to technological improvements in agricultural techniques, such as the Haber-Bosch method for synthesizing ammonium nitrate which made the traditional practice of recycling nutrients with crop rotation and animal manure less important.

Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological improvements have sharply increased yields from cultivation, but at the same time have caused widespread ecological damage and negative human health effects. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and the health effects of the antibiotics, growth hormones, and other chemicals commonly used in industrial meat production. Genetically modified organisms are an increasing component of agriculture, although they are banned in several countries. Agricultural food production and water management are increasingly becoming global issues that are fostering debate on a number of fronts. Significant degradation of land and water resources, including the depletion of aquifers, has been observed in recent decades, and the effects of global warming on agriculture and of agriculture on global warming are still not fully understood.

The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials. Specific foods include cereals (grains), vegetables, fruits, oils, meats and spices. Fibers include cotton, wool, hemp, silk and flax. Raw materials include lumber and bamboo. Other useful materials are produced by plants, such as resins, dyes, drugs, perfumes, biofuels and ornamental products such as cut flowers and nursery plants. Over one third of the world's workers are employed in agriculture, second only to the services' sector, although the percentages of agricultural workers in developed countries has decreased significantly over the past several centuries.

The word agriculture is a late Middle English adaptation of Latin agricultÅ«ra, from ager, "field", and cultÅ«ra, "cultivation" or "growing". Agriculture usually refers to human activities, although it is also observed in certain species of ant, termite and ambrosia beetle. To practice agriculture means to use natural resources to "produce commodities which maintain life, including food, fiber, forest products, horticultural crops, and their related services." This definition includes arable farming or agronomy, and horticulture, all terms for the growing of plants, animal husbandry and forestry. A distinction is sometimes made between forestry and agriculture, based on the former's longer management rotations, extensive versus intensive management practices and development mainly by nature, rather than by man. Even then, it is acknowledged that there is a large amount of knowledge transfer and overlap between silviculture (the management of forests) and agriculture. In traditional farming, the two are often combined even on small landholdings, leading to the term agroforestry.

Agricultural practices such as irrigation, crop rotation, application of fertilizers and pesticides, and the domestication of livestock were developed long ago, but have made great progress in the past century. The history of agriculture has played a major role in human history, as agricultural progress has been a crucial factor in worldwide socio-economic change. Division of labour in agricultural societies made commonplace specializations rarely seen in hunter-gatherer cultures, which allowed the growth of towns and cities, and the complex societies we call civilizations. When farmers became capable of producing food beyond the needs of their own families, others in their society were free to devote themselves to projects other than food acquisition. Historians and anthropologists have long argued that the development of agriculture made civilization possible. According to geographer Jared Diamond, the costs of agriculture were: "the average daily number of work hours increased, nutrition deteriorated, infectious disease and body wear increased, and lifespan shortened."

Forest gardening, a plant-based food production system, is thought to be the world's oldest agroecosystem. Forest gardens originated in prehistoric times along jungle-clad river banks and in the wet foothills of monsoon regions. In the gradual process of a family improving their immediate environment, useful tree and vine species were identified, protected and improved whilst undesirable species were eliminated. Eventually superior foreign species were selected and incorporated into the family's garden.

The Fertile Crescent of Western Asia first saw the domestication of animals, starting the Neolithic Revolution. Between 10,000 and 13,000 years ago, the ancestors of modern cattle, sheep, goats and pigs were domesticated in this area. The gradual transition from wild harvesting to deliberate cultivation happened independently in several areas around the globe. Agriculture allowed for the support of an increased population, leading to larger societies and eventually the development of cities. It also created the need for greater organization of political power (and the creation of social stratification), as decisions had to be made regarding labor and harvest allocation and access rights to water and land. Agriculture bred immobility, as populations settled down for long periods of time, which led to the accumulation of material goods.

Early Neolithic villages show evidence of the ability to process grain, and the Near East is the ancient home of the ancestors of wheat, barley and peas. There is evidence of the cultivation of figs in the Jordan Valley as long as 11,300 years ago, and cereal (grain) production in Syria approximately 9,000 years ago. During the same period, farmers in China began to farm rice and millet, using man-made floods and fires as part of their cultivation regimen. Fiber crops were domesticated as early as food crops, with China domesticating hemp, cotton being developed independently in Africa and South America, and the Near East domesticating flax. The use of soil amendments, including manure, fish, compost and ashes, appears to have begun early, and developed independently in several areas of the world, including Mesopotamia, the Nile Valley and Eastern Asia.
Squash was grown in Mexico nearly 10,000 years ago, while maize-like plants, derived from the wild teosinte, began to be seen at around 9,000 years ago. The derivation of teosinte into modern corn was slow, however, and it took until 5,500 to 6,000 years ago to turn into what we know today as maize. It then gradually spread across North America and was the major crop of Native Americans at the time of European exploration. Beans were domesticated around the same time, and together these three plants formed the Three Sisters nutritional foundation of many native populations in North and Central America. Combined with peppers, these crops provided a balanced diet for much of the continent. Grapes were first grown for wine approximately 8,000 years ago, in the Southern Caucasus, and by 3000 BC had spread to the Fertile Crescent, the Jordan Valley and Egypt.

Agriculture advanced to Europe slightly later, reaching the northeast of the continent from the east around 4000 BC. The idea that agriculture spread to Europe, rather than independently developing there, has led to two main hypotheses. The first is a "wave of advance", which holds that agriculture traveled slowly and steadily across the continent, while the second, "population pulse" theory, holds that it moved in jumps. Also around 6000 years ago, horses first began to be domesticated in the Eurasian steppes. Initially used for food, it was quickly discovered that they were useful for field work and carrying goods and people. Around 5,000 years ago, sunflowers were first cultivated in North America, while South America's Andes region was developing the potato. A minor center of domestication, the indigenous peoples of the eastern United States appear to have domesticated numerous crops, including tobacco.

Beginning around 3000 BC, nomadic pastoralism, with societies focused on the care of livestock for subsistence, appeared independently in several areas in Europe and Asia. The main region was the steppes stretching from the Hungarian Plain to Manchuria, where cattle, sheep, horses, and to a lesser extent yaks and bactrian camels provided sustenance. The second was in Arabia, where one-humped camels were the main animal, with sheep, goats and horses also seen. The third area was a band of societies in areas of eastern and central Africa with a tropical savannah climate. Cattle and goats were found most often in this area, with smaller numbers of sheep, horses and camels. A fourth area, more minor than the others, was found in northern Europe and Asia and was focused on reindeer herding.

Between 2500 and 2000 BC, the simplest form of the plough, called the ard, spread throughout Europe, replacing the hoe. This change in equipment significantly increased cultivation ability, and affected the demand for land, as well as ideas about property, inheritance and family rights. Before this period, simple digging sticks or hoes were used. These tools would have also been easier to transport, which was a benefit as people only stayed until the soil's nutrients were depleted. However, as the continuous cultivating of smaller pieces of land became a sustaining practice throughout the world, ards were much more efficient than digging sticks. As humanity became more stationary, empires, such as the New Kingdom of Egypt and the Ancient Romans, arose, dependent upon agriculture to feed their growing populations, and slavery, which was used to provide the labor needed for continually intensifying agricultural processes. Agricultural technology continued to improve, allowing the expansion of available crop varieties, including a wide range of fruits, vegetables, oil crops, spices and other products. China was also an important center for agricultural technology development during this period. During the Zhou dynasty (1666â€“221 BC), the first canals were built, and irrigation was used extensively. The later Three Kingdoms and Northern and Southern dynasties (221â€“581 AD) brought the first biological pest control, extensive writings on agricultural topics and technological innovations such as steel and the wheelbarrow.

In the ancient world, fresh products, such as meats, dairy products and fresh fruits and vegetables, were likely consumed relatively close to where they were produced. Less perishable products, such as grains, preserved foods, olive oil and wine, were often traded over an extensive network of land and sea routes. The ancient trade in agricultural goods was well established, with wine traded in the Mediterranean region in the 6th century BC and Rome receiving extensive shipments of grain as tax payments by the 2nd century BC. Huge amounts of grain were transported, mainly by sea, and it was during this period that the subsidization of grain farming began, for the prevention of famine. Ancient Rome was a major center for agricultural trade. Trade routes stretched from Britain and Scandinavia in the west to India and China in the east, and included major crops, such as grain, wine and olive oil (also a fuel for oil lamps), as well as additional products, including spices, fabrics and drugs.

In Ancient Greece and Rome, many scholars documented farming techniques, including the use of fertilizers. Much of what was believed about farming and plant nutrition at this time was later found to be incorrect, but their theories provided the scientific foundation for the development of agricultural theories through the Middle Ages. Ideas about soil fertility and fertilization remained much the same from the time of Greco-Roman scholars until the 19th century, with correspondingly low crop yields. By the time of Alexander the Great's conquests (330â€“323 BC), the role of horses had developed, and they played a huge role in warfare and agriculture. Innovations continued to be developed which allowed them to work longer, harder and more efficiently. By medieval times they became the primary source of power for agriculture, transport and warfare, a position they held until the development of the steam and internal combustion engines. The Mayan culture developed several innovations in agriculture during its peak, which ranged from 400 BC to 900 AD and was heavily dependent upon agriculture to support its population. The Mayans used extensive canal and raised field systems to farm the large portions of swampland on the YucatÃ¡n Peninsula.

The Middle Ages saw significant improvements in the agricultural techniques and technology. During this time period, monasteries spread throughout Europe and became important centers for the collection of knowledge related to agriculture and forestry. The manorial system, which existed under different names throughout Europe and Asia, allowed large landowners significant control over both their land and its laborers, in the form of peasants or serfs. During the medieval period, the Arab world was critical in the exchange of crops and technology between the European, Asia and African continents. Besides transporting numerous crops, they introduced the concept of summer irrigation to Europe and developed the beginnings of the plantation system of sugarcane growing through the use of slaves for intensive cultivation. Population continued to increase along with land use. From 100 BC to 1600 AD, methane emissions, produced by domesticated animals and rice growing, increased substantially.

By 900 AD in Europe, developments in iron smelting allowed for increased production, leading to developments in the production of agricultural implements such as ploughs, hand tools and horse shoes. The plough was significantly improved, developing into the mouldboard plough, capable of turning over the heavy, wet soils of northern Europe. This led to the clearing of forests in that area and a significant increase in agricultural production, which in turn led to an increase in population. A similar plough, which may have developed independently, was also found in China as early as the 9th century. At the same time, farmers in Europe moved from a two field crop rotation to a three field crop rotation in which one field of three was left fallow every year. This resulted in increased productivity and nutrition, as the change in rotations led to different crops being planted, including legumes such as peas, lentils and beans. Inventions such as improved horse harnesses and the whippletree also changed methods of cultivation. Watermills were initially developed by the Romans, but were improved throughout the Middle Ages, along with windmills, and used to grind grains into flour, cut wood and process flax and wool, among other uses.

Crops included wheat, rye, barley and oats. Peas, beans, and vetches became common from the 13th century onward as a fodder crop for animals and also for their nitrogen-fixation fertilizing properties. Crop yields peaked in the 13th century, and stayed more or less steady until the 18th century. Though the limitations of medieval farming were once thought to have provided a ceiling for the population growth in the Middle Ages, recent studies have shown that the technology of medieval agriculture was always sufficient for the needs of the people under normal circumstances, and that it was only during exceptionally harsh times, such as the terrible weather of 1315â€“17, that the needs of the population could not be met. The Medieval Warm Period, between 900â€“1300 AD, brought generally warmer global temperatures, leading to increased harvests throughout Europe and a greater northern range for subtropical crops such as figs and olives. Greenland and Iceland were settled by Europeans during this period, and supported agricultural activities. The long-term warming period is generally thought to have occurred mainly in Europe, but other areas of the world experienced shorter warming periods at different times during this period, including China in the 11th and 12th centuries, with similar effects on agriculture. The climate variations found in Europe during the Medieval Warm Period returned to more moderate levels in the 15th century, and terminated in the Little Ice Age of the 16th-mid 19th centuries.


After 1492, a global exchange of previously local crops and livestock breeds occurred. Key crops involved in this exchange included maize, potatoes, sweet potatoes and manioc traveling from the New World to the Old, and several varieties of wheat, barley, rice and turnips going from the Old World to the New. There were very few livestock species in the New World, with horses, cattle, sheep and goats being completely unknown before their arrival with Old World settlers. Crops moving in both directions across the Atlantic Ocean caused population growth around the world, and had a lasting effect on many cultures.

After its introduction from South America to Spain in the late 1500s, the potato became an important staple crop throughout Europe by the late 1700s. The potato allowed farmers to produce more food, and initially added variety to the European diet. The nutrition boost caused by increased potato consumption resulted in lower disease rates, higher birth rates and lower mortality rates, causing a population boom throughout the British Empire, the US and Europe. The introduction of the potato also brought about the first intensive use of fertilizer, in the form of guano imported to Europe from Peru, and the first artificial pesticide, in the form of an arsenic compound used to fight Colorado potato beetles. Before the adoption of the potato as a major crop, the dependence on grain caused repetitive regional and national famines when the crops failed: 17 major famines in England alone between 1523 and 1623. Although initially almost eliminating the danger of famine, the resulting dependence on the potato eventually caused the European Potato Failure, a disastrous crop failure from disease resulting in widespread famine, and the death of over one million people in Ireland alone.

The British Agricultural Revolution, with its massive increases in agricultural productivity and net output, is a topic of ongoing debate among historians and agricultural scholars. The changes in agriculture in Britain between the 16th and 19th centuries would subsequently affect agriculture around the world. Major points of development included enclosure, mechanization, crop rotation and selective breeding. Prior to the 1960s, historians viewed the British Agricultural Revolution of having been "largely facilitated by a small number of key innovators," including Robert Bakewell, Thomas Coke and Charles Townshend. However, modern historians disperse much of the importance surrounding these individual men, and instead point to them holding a smaller position within a major societal shift regarding agriculture in Britain.

The agricultural changes, along with industrialization and migration, allowed the population of Britain, as well as other countries who followed its model, such as the US, Germany and Belgium, to escape from the Malthusian trap and increase both their population and their standard of living. It is estimated that the productivity of wheat in England went up from about 19 bushels per acre in 1720 to 21â€“22 bushels by the middle of the century and finally stabilized at around 30 bushels by 1840.

Premodern agriculture across Europe was characterized by the feudal open field system, where farmers worked on strips of land in fields that were held in common; this was inefficient and reduced the incentive to improve productivity. Many farms began to be enclosed by yeomen who improved the use of their land. This process of land reform accelerated in the 18th century with special acts of Parliament to expedite the legal process. The consolidation of large, privately owned holdings, encouraged the improvement of productivity through experimentation by enterprising landowners. By the 1750s, the market for agriculture was substantially commercialized - crop surpluses were routinely sold by the producers on the market or exported elsewhere.

These social changes were coupled with technical improvements. New methods of crop rotation and land use resulted in large additions to the amount of arable land. The four-field crop rotation was popularized by Charles Townshend in the 18th century. The system (wheat, turnips, barley and clover), opened up a fodder crop and grazing crop allowing livestock to be bred year-round. Yields of cereal crops increased as farmers utilized nitrogen-rich manure and nitrogen fixing-crops such as clover, increasing the available nitrogen in the soil and removing the limiting factor on cereal productions that had existed prior to the early 19th century. This improved production per farmer led to an increase in population and in the available workforce, creating the labor force needed for the Industrial Revolution.

The development of agriculture into its modern form was made possible through a continuing process of mechanization. Prior to this, basic agricultural tools had slowly been improved over centuries of use. The plough, for example, was a heavy implement with wheels in the 1500s. By the 1600s it was lighter, and by 1730, the Rotherham plough dramatically changed farming with no wheels, interchangeable parts, stronger construction and less weight. During the early 1800s, cast iron replaced wood for many parts, leading to longer-lasting implements. Seed drills had been under development since the early 1500s, but it was Jethro Tull's 1731 invention of a horse-drawn seed drill and horse hoe (a small plough to hoe between crop rows) that would eventually revolutionize planting in Britain, although they would not become popular until the early 1800s. Andrew Meikle patented the first practical threshing machine in 1784.

The Industrial Revolution caused a boom in international trade and shipping. Increased production caused a rise in the need for raw materials, with European merchants purchasing the majority of the goods. The value of goods traded worldwide increased by five times between 1750 and 1914, with annual shipping tonnages increasing from 4 million to 30 million tons between 1800 and 1900. In the second half of the 19th century, trade also expanded in the food (including grain and meat) and wool markets, and England (with the repeal of the Corn Laws in 1846) began to trade quantities of industrial products for wheat from around the world. The vast expansion of railroads that followed the invention of the steam engine further revolutionized world trade, especially in the Americas and East Asia, as goods could now be more easily traded across vast land distances. The developments of heat processing and refrigeration in the 19th century led to a similar revolution in the meat industry, as they allowed meat to be shipped long distances without spoiling. Countries in tropical locations, such as Australia and South America, were at the forefront of this effort.
In the mid-1800s, horse drawn machinery, such as the McCormick reaper, revolutionized harvesting, while inventions such as the cotton gin made possible the processing of large amounts of crops. During this same period, farmers began to use steam-powered threshers and tractors, although they were found to be expensive, dangerous and a fire hazard. The first gasoline-powered tractors were successfully developed around 1900, and in 1923, the International Harvester Farmall tractor became the first all-purpose tractor, and marked a major point in the replacement of draft animals (particularly horses) with machines. Since that time, self-propelled mechanical harvesters (combines), planters, transplanters and other equipment have been developed, further revolutionizing agriculture. These inventions allowed farming tasks to be done with a speed and on a scale previously impossible, leading modern farms to output much greater volumes of high-quality produce per land unit.

The scientific investigation of fertilization began at the Rothamsted Experimental Station in 1843 by John Bennet Lawes. He developed the first commercial process for fertilizer production - the obtaining of phosphate from the dissolution of coprolites in sulphuric acid. In 1909 the revolutionary Haber-Bosch method to synthesize ammonium nitrate was first demonstrated; it represented a major breakthrough and allowed crop yields to overcome previous constraints. In the years after World War II, the use of synthetic fertilizer increased rapidly, in sync with the increasing world population.


Despite the tremendous gains in agricultural productivity, famines continued to sweep the globe through the 20th century. Through the effects of climatic events, government policy, war and crop failure, millions of people died in each of at least ten famines between the 1920s and the 1990s.
The Green Revolution refers to a series of research, development, and technology transfer initiatives, occurring between the 1940s and the late 1970s, that increased agriculture production around the world, beginning most markedly in the late 1960s. It involved the development of high-yielding varieties of cereal grains, expansion of irrigation infrastructure, modernization of management techniques, distribution of hybridized seeds, synthetic fertilizers, and pesticides to farmers. The initiatives, led by Norman Borlaug, the "Father of the Green Revolution", are credited with saving hundreds of millions of people from starvation. Demographer Thomas Malthus in 1798 famously predicted that the Earth would not be able to support its growing population, but technologies such as those promoted by the Green Revolution have thus far allowed the world to produce a surplus of food.

Although the Green Revolution significantly increased rice yields in Asia, yield increases have not occurred in the past 15â€“20 years. The genetic yield potential has increased for wheat, but the yield potential for rice has not increased since 1966, and the yield potential for maize has "barely increased in 35 years". It takes a decade or two for herbicide-resistant weeds to emerge, and insects become resistant to insecticides within about a decade. Crop rotation helps to prevent resistances.

The cereals rice, corn, and wheat provide 60% of human food supply. Between 1700 and 1980, "the total area of cultivated land worldwide increased 466%" and yields increased dramatically, particularly because of selectively bred high-yielding varieties, fertilizers, pesticides, irrigation, and machinery. However, concerns have been raised over the sustainability of intensive agriculture. Intensive agriculture has become associated with decreased soil quality in India and Asia, and there has been increased concern over the effects of fertilizers and pesticides on the environment, particularly as population increases and food demand expands. The monocultures typically used in intensive agriculture increase the number of pests, which are controlled through pesticides. Integrated pest management (IPM), which "has been promoted for decades and has had some notable successes" has not significantly affected the use of pesticides because policies encourage the use of pesticides and IPM is knowledge-intensive. In the 21st century, plants have been used to grow biofuels, pharmaceuticals (including biopharmaceuticals), and bioplastics.

In the past century agriculture has been characterized by increased productivity, the substitution of synthetic fertilizers and pesticides for labor, water pollution, and farm subsidies. In recent years there has been a backlash against the external environmental effects of conventional agriculture, resulting in the organic and sustainable agriculture movements. One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies, also known as decoupling. The growth of organic farming has renewed research in alternative technologies such as integrated pest management and selective breeding. Recent mainstream technological developments include genetically modified food.

In 2007, higher incentives for farmers to grow non-food biofuel crops combined with other factors, such as overdevelopment of former farm lands, rising transportation costs, climate change, growing consumer demand in China and India, and population growth, caused food shortages in Asia, the Middle East, Africa, and Mexico, as well as rising food prices around the globe. As of December 2007, 37 countries faced food crises, and 20 had imposed some sort of food-price controls. Some of these shortages resulted in food riots and even deadly stampedes. The International Fund for Agricultural Development posits that an increase in smallholder agriculture may be part of the solution to concerns about food prices and overall food security. They in part base this on the experience of Vietnam, which went from a food importer to large food exporter and saw a significant drop in poverty, due mainly to the development of smallholder agriculture in the country.

Disease and land degradation are two of the major concerns in agriculture today. For example, an epidemic of stem rust on wheat caused by the Ug99 lineage is currently spreading across Africa and into Asia and is causing major concerns due to crop losses of 70% or more under some conditions. Approximately 40% of the world's agricultural land is seriously degraded. In Africa, if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa.

Agrarian structure is a long-term structure in the Braudelian understanding of the concept. On a larger scale the agrarian structure is more dependent on the regional, social, cultural and historical factors than on the stateâ€™s undertaken activities. Like in Poland, where despite running an intense agrarian policy for many years, the agrarian structure in 2002 has much in common with that found in 1921 soon after the partitions period.

In 2009, the agricultural output of China was the largest in the world, followed by the European Union, India and the United States, according to the International Monetary Fund (see below). Economists measure the total factor productivity of agriculture and by this measure agriculture in the United States is roughly 1.7 times more productive than it was in 1948.

, the International Labour Organization states that approximately one billion people, or over 1/3 of the available work force, are employed in the global agricultural sector. Agriculture constitutes approximately 70% of the global employment of children, and in many countries employs the largest percentage of women of any industry. The service sector only overtook the agricultural sector as the largest global employer in 2007. Between 1997 and 2007, the percentage of people employed in agriculture fell by over four percentage points, a trend that is expected to continue. The number of people employed in agriculture varies widely on a per-country basis, ranging from less than 2% in countries like the US and Canada to over 80% in many African nations. In developed countries, these figures are significantly lower than in previous centuries. During the 16th century in Europe, for example, between 55 and 75 percent of the population was engaged in agriculture, depending on the country. By the 19th century in Europe, this had dropped to between 35 and 65 percent. In the same countries today, the figure is less than 10%.

Agriculture remains a hazardous industry, and farmers worldwide remain at high risk of work-related injuries, lung disease, noise-induced hearing loss, skin diseases, as well as certain cancers related to chemical use and prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery, and a common cause of fatal agricultural injuries in developed countries is tractor rollovers. Pesticides and other chemicals used in farming can also be hazardous to worker health, and workers exposed to pesticides may experience illness or have children with birth defects. As an industry in which families commonly share in work and live on the farm itself, entire families can be at risk for injuries, illness, and death. Common causes of fatal injuries among young farm workers include drowning, machinery and motor vehicle-related accidents.

The International Labour Organization considers agriculture "one of the most hazardous of all economic sectors." It estimates that the annual work-related death toll among agricultural employees is at least 170,000, twice the average rate of other jobs. In addition, incidences of death, injury and illness related to agricultural activities often go unreported. The organization has developed the Safety and Health in Agriculture Convention, 2001, which covers the range of risks in the agriculture occupation, the prevention of these risks and the role that individuals and organizations engaged in agriculture should play.


Cropping systems vary among farms depending on the available resources and constraints; geography and climate of the farm; government policy; economic, social and political pressures; and the philosophy and culture of the farmer.

Shifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years. Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10â€“20). This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control. Annual cultivation is the next phase of intensity in which there is no fallow period. This requires even greater nutrient and pest control inputs.

Further industrialization led to the use of monocultures, when one cultivar is planted on a large acreage. Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers. Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time, are other kinds of annual cropping systems known as polycultures.

In subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation. In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry. In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual cropping is the dominant farming system.


Important categories of crops include cereals and pseudocereals, pulses (legumes), forage, and fruits and vegetables. Specific crops are cultivated in distinct growing regions throughout the world. In millions of metric tons, based on FAO estimate.
Animals, including horses, mules, oxen, water buffalo, camels, llamas, alpacas, donkeys, and dogs, are often used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers. Animal husbandry not only refers to the breeding and raising of animals for meat or to harvest animal products (like milk, eggs, or wool) on a continual basis, but also to the breeding and care of species for work and companionship.

Livestock production systems can be defined based on feed source, as grassland-based, mixed, and landless. , 30% of Earth's ice- and water-free area was used for producing livestock, with the sector employing approximately 1.3 billion people. Between the 1960s and the 2000s, there was a significant increase in livestock production, both by numbers and by carcass weight, especially among beef, pigs and chickens, the latter of which had production increased by almost a factor of 10. Non-meat animals, such as milk cows and egg-producing chickens, also showed significant production increases. Global cattle, sheep and goat populations are expected to continue to increase sharply through 2050. Aquaculture or fish farming, the production of fish for human consumption in confined operations, is one of the fastest growing sectors of food production, growing at an average of 9% a year between 1975 and 2007.

During the second half of the 20th century, producers using selective breeding focused on creating livestock breeds and crossbreeds that increased production, while mostly disregarding the need to preserve genetic diversity. This trend has led to a significant decrease in genetic diversity and resources among livestock breeds, leading to a corresponding decrease in disease resistance and local adaptations previously found among traditional breeds.

Grassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals. Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source. This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30â€“40 million pastoralists. Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastric (one stomach; mainly chickens and pigs) livestock. Manure is typically recycled in mixed systems as a fertilizer for crops.

Landless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in Organisation for Economic Co-operation and Development(OECD) member countries. Synthetic fertilizers are more heavily relied upon for crop production and manure utilization becomes a challenge as well as a source for pollution. Industrialized countries use these operations to produce much of the global supplies of poultry and pork. Scientists estimate that 75% of the growth in livestock production between 2003 and 2030 will be in confined animal feeding operations, sometimes called factory farming. Much of this growth is happening in developing countries in Asia, with much smaller amounts of growth in Africa. Some of the practices used in commercial livestock production, including the usage of growth hormones, are controversial.


Tillage is the practice of plowing soil to prepare for planting or for nutrient incorporation or for pest control. Tillage varies in intensity from conventional to no-till. It may improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO2, and reduces the abundance and diversity of soil organisms.

Pest control includes the management of weeds, insects, mites, and diseases. Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used. Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance. Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort.

Nutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of utilization of manure produced by livestock. Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and mined minerals. Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period. Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures.

Water management is needed where rainfall is insufficient or variable, which occurs to some degree in most regions of the world. Some farmers use irrigation to supplement rainfall. In other areas such as the Great Plains in the U.S. and Canada, farmers use a fallow year to conserve soil moisture to use for growing a crop in the following year. Agriculture represents 70% of freshwater use worldwide.

According to a report by the International Food Policy Research Institute, agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, the International Food Policy Research Institute found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.

"Payment for ecosystem services (PES) can further incentivise efforts to green the agriculture sector. This is an approach that verifies values and rewards the benefits of ecosystem services provided by green agricultural practices." "Innovative PES measures could include reforestation payments made by cities to upstream communities in rural areas of shared watersheds for improved quantities and quality of fresh water for municipal users. Ecoservice payments by farmers to upstream forest stewards for properly managing the flow of soil nutrients, and methods to monetise the carbon sequestration and emission reduction credit benefits of green agriculture practices in order to compensate farmers for their efforts to restore and build SOM and employ other practices." 

Crop alteration has been practiced by humankind for thousands of years, since the beginning of civilization. Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests. Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel. His work on dominant and recessive alleles, although initially largely ignored for almost 50 years, gave plant breeders a better understanding of genetics and breeding techniques. Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism.

Domestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants. Careful selection and breeding have had enormous effects on the characteristics of crop plants. Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand. Extensive X-ray and ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley.

The Green Revolution popularized the use of conventional hybridization to sharply increase yield by creating "high-yielding varieties". For example, average yields of corn (maize) in the USA have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001. Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990. South American average wheat yields are around 2 t/ha, African under 1 t/ha, and Egypt and Arabia up to 3.5 to 4 t/ha with irrigation. In contrast, the average wheat yield in countries such as France is over 8 t/ha. Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, growth control to avoid lodging).

Genetically modified organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology. Genetic engineering has expanded the genes available to breeders to utilize in creating desired germlines for new crops. Increased durability, nutritional content, insect and virus resistance and herbicide tolerance are a few of the attributes bred into crops through genetic engineering. For some, GMO crops cause food safety and food labeling concerns. Numerous countries have placed restrictions on the production, import or use of GMO foods and crops, which have been put in place due to concerns over potential health issues, declining agricultural diversity and contamination of non-GMO crops. Currently a global treaty, the Biosafety Protocol, regulates the trade of GMOs. There is ongoing discussion regarding the labeling of foods made from GMOs, and while the EU currently requires all GMO foods to be labeled, the US does not.

Herbicide-resistant seed has a gene implanted into its genome that allows the plants to tolerate exposure to herbicides, including glyphosates. These seeds allow the farmer to grow a crop that can be sprayed with herbicides to control weeds without harming the resistant crop. Herbicide-tolerant crops are used by farmers worldwide. With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate-based herbicide sprays. In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides. Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.

Other GMO crops used by growers include insect-resistant crops, which have a gene from the soil bacterium Bacillus thuringiensis (Bt), which produces a toxin specific to insects. These crops protect plants from damage by insects. Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species. In some cases, wild species are the primary source of resistance traits; some tomato cultivars that have gained resistance to at least 19 diseases did so through crossing with wild populations of tomatoes.

Agriculture imposes external costs upon society through pesticides, nutrient runoff, excessive water usage, loss of natural environment and assorted other problems. A 2000 assessment of agriculture in the UK determined total external costs for 1996 of Â£2,343 million, or Â£208 per hectare. A 2005 analysis of these costs in the USA concluded that cropland imposes approximately $5 to 16 billion ($30 to $96 per hectare), while livestock production imposes $714 million. Both studies, which focused solely on the fiscal impacts, concluded that more should be done to internalize external costs. Neither included subsidies in their analysis, but they noted that subsidies also influence the cost of agriculture to society. In 2010, the International Resource Panel of the United Nations Environment Programme published a report assessing the environmental impacts of consumption and production. The study found that agriculture and food consumption are two of the most important drivers of environmental pressures, particularly habitat change, climate change, water use and toxic emissions. The 2011 UNEP Green Economy report states that "gricultural operations, excluding land use changes, produce approximately 13 per cent of anthropogenic global GHG emissions. This includes GHGs emitted by the use of inorganic fertilisers agro-chemical pesticides and herbicides; (GHG emissions resulting from production of these inputs are included in industrial emissions); and fossil fuel-energy inputs. "On average we find that the total amount of fresh residues from agricultural and forestry production for second- generation biofuel production amounts to 3.8 billion tonnes per year between 2011 and 2050 (with an average annual growth rate of 11 per cent throughout the period analysed, accounting for higher growth during early years, 48 per cent for 2011-2020 and an average 2 per cent annual expansion after 2020)." 

A senior UN official and co-author of a UN report detailing this problem, Henning Steinfeld, said "Livestock are one of the most significant contributors to today's most serious environmental problems". Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet. It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO2 equivalents. By comparison, all transportation emits 13.5% of the CO2. It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO2,) and 37% of all human-induced methane (which is 23 times as warming as CO2.) It also generates 64% of the ammonia emission. Livestock expansion is cited as a key factor driving deforestation; in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feedcrops. Through deforestation and land degradation, livestock is also driving reductions in biodiversity. Furthermore, the UNEP states that "methane emissions from global livestock are projected to increase by 60 per cent by 2030 under current practices and consumption patterns." 


Land transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is considered the driving force in the loss of biodiversity. Estimates of the amount of land transformed by humans vary from 39 to 50%. Land degradation, the long-term decline in ecosystem function and productivity, is estimated to be occurring on 24% of land worldwide, with cropland overrepresented. The UN-FAO report cites land management as the driving factor behind degradation and reports that 1.5 billion people rely upon the degrading land. Degradation can be deforestation, desertification, soil erosion, mineral depletion, or chemical degradation (acidification and salinization).

Eutrophication, excessive nutrients in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses. Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land. These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems.

Agriculture accounts for 70% of withdrawals of freshwater resources. Agriculture is a major draw on water from aquifers, and currently draws from those underground water sources at an unsustainable rate. It is long known that aquifers in areas as diverse as northern China, the Upper Ganges and the western US are being depleted, and new research extends these problems to aquifers in Iran, Mexico and Saudi Arabia. Increasing pressure is being placed on water resources by industry and urban areas, meaning that water scarcity is increasing and agriculture is facing the challenge of producing more food for the world's growing population with reduced water resources. Agricultural water usage can also cause major environmental problems, including the destruction of natural wetlands, the spread of water-borne diseases, and land degradation through salinization and waterlogging, when irrigation is performed incorrectly.


Pesticide use has increased since 1950 to 2.5 million tons annually worldwide, yet crop loss from pests has remained relatively constant. The World Health Organization estimated in 1992 that 3 million pesticide poisonings occur annually, causing 220,000 deaths. Pesticides select for pesticide resistance in the pest population, leading to a condition termed the 'pesticide treadmill' in which pest resistance warrants the development of a new pesticide.

An alternative argument is that the way to 'save the environment' and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'. However, critics argue that a trade-off between the environment and a need for food is not inevitable, and that pesticides simply replace good agronomic practices such as crop rotation. The UNEP introduces the Pushâ€“pull agricultural pest management technique which involves intercropping that uses plant aromas to repel or push away pests while pulling in or attracting the right insects. "The implementation of push-pull in eastern Africa has significantly increased maize yields and the combined cultivation of N-fixing forage crops has enriched the soil and has also provided farmers with feed for livestock. With increased livestock operations, the farmers are able to produce meat, milk and other dairy products and they use the manure as organic fertiliser that returns nutrients to the fields." 


Climate change has the potential to affect agriculture through changes in temperature, rainfall (timing and quantity), CO2, solar radiation and the interaction of these elements. Extreme events, such as droughts and floods, are forecast to increase as climate change takes hold. Agriculture is among sectors most vulnerable to the impacts of climate change; water supply for example, will be critical to sustain agricultural production and provide the increase in food output required to sustain the world's growing population. Fluctuations in the flow of rivers are likely to increase in the twenty-first century. Based on the experience of countries in the Nile river basin (Ethiopia, Kenya and Sudan) and other developing countries, depletion of water resources during seasons crucial for agriculture can lead to a decline in yield by up to 50%. Transformational approaches will be needed to manage natural resources in the future. For example, policies, practices and tools promoting climate-smart agriculture will be important, as will better use of scientific information on climate for assessing risks and vulnerability. Planners and policy-makers will need to help create suitable policies that encourage funding for such agricultural transformation.

Agriculture can both mitigate or worsen global warming. Some of the increase in CO2 in the atmosphere comes from the decomposition of organic matter in the soil, and much of the methane emitted into the atmosphere is caused by the decomposition of organic matter in wet soils such as rice paddies, as well as the normal digestive activities of farm animals. Further, wet or anaerobic soils also lose nitrogen through denitrification, releasing the greenhouse gases nitric oxide and nitrous oxide. Changes in management can reduce the release of these greenhouse gases, and soil can further be used to sequester some of the CO2 in the atmosphere. Informed by the UNEP, "griculture also produces about 58 per cent of global nitrous oxide emissions and about 47 per cent of global methane emissions. Both of these gases have a far greater global warming potential per tonne than CO2 (298 times and 25 times respectively)." 

There are several factors within the field of agriculture that contribute to the large amount of CO2 emissions. The diversity of the sources ranges from the production of farming tools to the transport of harvested produce. Approximately 8% of the national carbon footprint is due to agricultural sources. Of that, 75% is of the carbon emissions released from the production of crop assisting chemicals. Factories producing insecticides, herbicides, fungicides, and fertilizers are a major culprit of the greenhouse gas. Productivity on the farm itself and the use of machinery is another source of the carbon emission. Almost all the industrial machines used in modern farming are powered by fossil fuels. These instruments are burning fossil fuels from the beginning of the process to the end. Tractors are the root of this source. The tractor is going to burn fuel and release CO2 just to run. The amount of emissions from the machinery increase with the attachment of different units and need for more power. During the soil preparation stage tillers and plows will be used to disrupt the soil. During growth watering pumps and sprayers are used to keep the crops hydrated. And when the crops are ready for picking a forage or combine harvester is used. These types of machinery all require additional energy which leads to increased carbon dioxide emissions from the basic tractors. The final major contribution to CO2 emissions in agriculture is in the final transport of produce. Local farming suffered a decline over the past century due to large amounts of farm subsidies. The majority of crops are shipped hundreds of miles to various processing plants before ending up in the grocery store. These shipments are made using fossil fuel burning modes of transportation. Inevitably these transport adds to carbon dioxide emissions.


Some major organizations are hailing farming within agroecosystems as the way forward for mainstream agriculture. Current farming methods have resulted in over-stretched water resources, high levels of erosion and reduced soil fertility. According to a report by the International Water Management Institute and UNEP, there is not enough water to continue farming using current practices; therefore how critical water, land, and ecosystem resources are used to boost crop yields must be reconsidered. The report suggested assigning value to ecosystems, recognizing environmental and livelihood tradeoffs, and balancing the rights of a variety of users and interests. Inequities that result when such measures are adopted would need to be addressed, such as the reallocation of water from poor to rich, the clearing of land to make way for
more productive farmland, or the preservation of a wetland system that limits fishing rights.

Technological advancements help provide farmers with tools and resources to make farming more sustainable. New technologies have given rise to innovations like conservation tillage, a farming process which helps prevent land loss to erosion, water pollution and enhances carbon sequestration.

According to a report by the International Food Policy Research Institute (IFPRI), agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, IFPRI found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.

Agricultural economics refers to economics as it relates to the "production, distribution and consumption of  goods and services". Combining agricultural production with general theories of marketing and business as a discipline of study began in the late 1800s, and grew significantly through the 20th century. Although the study of agricultural economics is relatively recent, major trends in agriculture have significantly affected national and international economies throughout history, ranging from tenant farmers and sharecropping in the post-American Civil War Southern United States to the European feudal system of manorialism. In the United States, and elsewhere, food costs attributed to food processing, distribution, and agricultural marketing, sometimes referred to as the value chain, have risen while the costs attributed to farming have declined. This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g. more highly processed products) provided by the supply chain. Market concentration has increased in the sector as well, and although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.

National government policies can significantly change the economic marketplace for agricultural products, in the form of taxation, subsidies, tariffs and other measures. Since at least the 1960s, a combination of import/export restrictions, exchange rate policies and subsidies have affected farmers in both the developing and developed world. In the 1980s, it was clear that non-subsidized farmers in developing countries were experiencing adverse affects from national policies that created artificially low global prices for farm products. Between the mid-1980s and the early 2000s, several international agreements were put into place that limited agricultural tariffs, subsidies and other trade restrictions.

However, , there was still a significant amount of policy-driven distortion in global agricultural product prices. The three agricultural products with the greatest amount of trade distortion were sugar, milk and rice, mainly due to taxation. Among the oilseeds, sesame had the greatest amount of taxation, but overall, feed grains and oilseeds had much lower levels of taxation than livestock products. Since the 1980s, policy-driven distortions have seen a greater decrease among livestock products than crops during the worldwide reforms in agricultural policy. Despite this progress, certain crops, such as cotton, still see subsidies in developed countries artificially deflating global prices, causing hardship in developing countries with non-subsidized farmers. Unprocessed commodities (i.e. corn, soybeans, cows) are generally graded to indicate quality. The quality affects the price the producer receives. Commodities are generally reported by production quantities, such as volume, number or weight.

Since the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides. The vast majority of this energy input comes from fossil fuel sources. Between the 1960â€“65 measuring cycle and the cycle from 1986 to 1990, the Green Revolution transformed agriculture around the globe, with world grain production increasing significantly (between 70% and 390% for wheat and 60% to 150% for rice, depending on geographic area) as world population doubled. Modern agriculture's heavy reliance on petrochemicals and mechanization has raised concerns that oil shortages could increase costs and reduce agricultural output, causing food shortages.

Modern or industrialized agriculture is dependent on fossil fuels in two fundamental ways: 1. direct consumption on the farm and 2. indirect consumption to manufacture inputs used on the farm. Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery; and use of gasoline, liquid propane, and electricity to power dryers, pumps, lights, heaters, and coolers. American farms directly consumed about 1.2 exajoules (1.1 quadrillion BTU) in 2002, or just over 1% of the nation's total energy.

Indirect consumption is mainly oil and natural gas used to manufacture fertilizers and pesticides, which accounted for 0.6 exajoules (0.6 quadrillion BTU) in 2002. The natural gas and coal consumed by the production of nitrogen fertilizer can account for over half of the agricultural energy usage. China utilizes mostly coal in the production of nitrogen fertilizer, while most of Europe uses large amounts of natural gas and small amounts of coal. According to a 2010 report published by The Royal Society, agriculture is increasingly dependent on the direct and indirect input of fossil fuels. Overall, the fuels used in agriculture vary based on several factors, including crop, production system and location. The energy used to manufacture farm machinery is also a form of indirect agricultural energy consumption. Together, direct and indirect consumption by US farms accounts for about 2% of the nation's energy use. Direct and indirect energy consumption by U.S. farms peaked in 1979, and has gradually declined over the past 30 years. Food systems encompass not just agricultural production, but also off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items. Agriculture accounts for less than one-fifth of food system energy use in the US.


In the event of a petroleum shortage (see peak oil for global concerns), organic agriculture can be more attractive than conventional practices that use petroleum-based pesticides, herbicides, or fertilizers. Some studies using modern organic-farming methods have reported yields as high as those available from conventional farming. In the aftermath of the fall of the Soviet Union, with shortages of conventional petroleum-based inputs, Cuba made use of mostly organic practices, including biopesticides, plant-based pesticides and sustainable cropping practices, to feed its populace. However, organic farming may be more labor-intensive and would require a shift of the workforce from urban to rural areas. The reconditioning of soil to restore nutrients lost during the use of monoculture agriculture techniques also takes time.

It has been suggested that rural communities might obtain fuel from the biochar and synfuel process, which uses agricultural waste to provide charcoal fertilizer, some fuel and food, instead of the normal food vs. fuel debate. As the synfuel would be used on-site, the process would be more efficient and might just provide enough fuel for a new organic-agriculture fusion.

It has been suggested that some transgenic plants may some day be developed which would allow for maintaining or increasing yields while requiring fewer fossil-fuel-derived inputs than conventional crops. The possibility of success of these programs is questioned by ecologists and economists concerned with unsustainable GMO practices such as terminator seeds. While there has been some research on sustainability using GMO crops, at least one prominent multi-year attempt by Monsanto Company has been unsuccessful, though during the same period traditional breeding techniques yielded a more sustainable variety of the same crop.

Agricultural policy is the set of government decisions and actions relating to domestic agriculture and imports of foreign agricultural products. Governments usually implement agricultural policies with the goal of achieving a specific outcome in the domestic agricultural product markets. Some overarching themes include risk management and adjustment (including policies related to climate change, food safety and natural disasters), economic stability (including policies related to taxes), natural resources and environmental sustainability (especially water policy), research and development, and market access for domestic commodities (including relations with global organizations and agreements with other countries). Agricultural policy can also touch on food quality, ensuring that the food supply is of a consistent and known quality, food security, ensuring that the food supply meets the population's needs, and conservation. Policy programs can range from financial programs, such as subsidies, to encouraging producers to enroll in voluntary quality assurance programs.

There are many influences on the creation of agricultural policy, including consumers, agribusiness, trade lobbies and other groups. Agribusiness interests hold a large amount of influence over policy making, in the form of lobbying and campaign contributions. Political action groups, including those interested in environmental issues and labor unions, also provide influence, as do lobbying organizations representing individual agricultural commodities. The Food and Agriculture Organization of the United Nations (FAO) leads international efforts to defeat hunger and provides a forum for the negotiation of global agricultural regulations and agreements. Dr. Samuel Jutzi, director of FAO's animal production and health division, states that lobbying by large corporations has stopped reforms that would improve human health and the environment. For example, proposals in 2010 for a voluntary code of conduct for the livestock industry that would have provided incentives for improving standards for health, and environmental regulations, such as the number of animals an area of land can support without long-term damage, were successfully defeated due to large food company pressure.

Aeroponics
Agricultural engineering
Agricultural value chain
Agroecology
Building-integrated agriculture
Contract farming
Corporate farming
Crofting
Ecoagriculture
Feed additive
Hill farming
List of documentary films about agriculture
Pharming (genetics)
Remote sensing
Subsistence economy
Vertical farming

Bolens, L. (1997). "Agriculture" in Selin, Helaine (ed.), Encyclopedia of the History of Science, Technology, and Medicine in Non-Western Cultures. Kluwer Academic Publishers, Dordrecht/Boston/London, pp.20â€“22.
Collinson, M. (ed.) A History of Farming Systems Research. CABI Publishing, 2000. ISBN 978-0-85199-405-5
Jared Diamond, Guns, germs and steel. A short history of everybody for the last 13,000 years, 1997.
Mazoyer, Marcel; Roudart, Laurence (2006). A history of world agriculture: from the Neolithic Age to the current crisis. Monthly Review Press, New York. ISBN 978-1-58367-121-4
Watson, A.M. (1983). Agricultural Innovation in the Early Islamic World, Cambridge University Press.

 of the Food and Agriculture Organization (FAO) of the United Nations
 of the United States Department of Agriculture (USDA)
 of the USDA's Agricultural Research Service
 from the Government Information Library of the University of Colorado, Boulder
 material from the World Bank Group


Aldous Leonard Huxley  (26 July 1894â€“ 22 November 1963) was an English writer, philosopher and a prominent member of the Huxley family.

He was best known for his novels including Brave New World, set in a dystopian London, and for non-fiction books, such as The Doors of Perception, which recalls experiences when taking a psychedelic drug, and a wide-ranging output of essays. Early in his career Huxley edited the magazine Oxford Poetry, and published short stories and poetry. Mid career and later, he published travel writing, film stories and scripts. He spent the later part of his life in the US, living in Los Angeles from 1937 until his death. In 1962, a year before his death, he was elected Companion of Literature by the Royal Society of Literature.

Huxley was a humanist, pacifist, and satirist.  Huxley later became interested in spiritual subjects such as parapsychology and philosophical mysticism, in particular, Universalism. By the end of his life, Huxley was widely acknowledged as one of the pre-eminent intellectuals of his time. He was nominated for the Nobel Prize in Literature in seven different years.


Aldous Huxley was born in Godalming, Surrey, England, in 1894. He was the third son of the writer and schoolmaster Leonard Huxley and his first wife, Julia Arnold, who founded Prior's Field School.  Julia was the niece of poet and critic Matthew Arnold and the sister of Mrs. Humphrey Ward.  Aldous was the grandson of Thomas Henry Huxley, the zoologist, agnostic and controversialist ("Darwin's Bulldog"). His brother Julian Huxley and half-brother Andrew Huxley also became outstanding biologists.  Aldous had another brother, Noel Trevelyan Huxley (1891â€“1914), who committed suicide after a period of clinical depression.

Huxley began his learning in his father's well-equipped botanical laboratory, then went to Hillside School, Malvern. His teacher was his mother, who supervised him for several years until she became terminally ill. After Hillside, he was educated at Eton College. Huxley's mother died in 1908 when he was 14. In 1911, he suffered an illness (keratitis punctata) which "left  practically blind for two to three years". Aldous volunteered to join the army at the outbreak of World War I, but was rejected on health grounds: he was half-blind in one eye. Once his eyesight recovered sufficiently, he was able to study English literature at Balliol College, Oxford. In 1916 he edited Oxford Poetry and later graduated (BA) with first class honours. His brother Julian wrote:
Following his education at Balliol, Huxley was financially indebted to his father and had to earn a living. He taught French for a year at Eton, where Eric Blair (later to become George Orwell) and Steven Runciman were among his pupils, but was remembered as an incompetent and hopeless teacher who couldn't keep discipline. Nevertheless, Blair and others were impressed by his use of words. For a short while in 1918, he was employed acquiring provisions at the Air Ministry.

Significantly, Huxley also worked for a time in the 1920s at the technologically advanced Brunner and Mond chemical plant in Billingham, Teesside, and the most recent introduction to his famous science fiction novel Brave New World (1932) states that this experience of "an ordered universe in a world of planless incoherence" was one source for the novel.

Huxley completed his first (unpublished) novel at the age of 17 and began writing seriously in his early 20s. His first published novels were social satires, beginning with Crome Yellow (1921).


During World War I, Huxley spent much of his time at Garsington Manor near Oxford, home of Lady Ottoline Morrell, working as a farm labourer. Here he met several Bloomsbury figures, including Bertrand Russell, Alfred North Whitehead and Clive Bell. Later, in Crome Yellow (1921) he caricatured the Garsington lifestyle. Jobs were very scarce, but in 1919 John Middleton Murry was reorganising the Athenaeum and invited Huxley to join the staff. He accepted immediately, and quickly married the Belgian refugee Maria Nys, also at Garsington. They lived with their young son in Italy part of the time in the 1920s, where Huxley would visit his friend D. H. Lawrence.  Following Lawrence's death in 1930, Huxley edited Lawrence's letters (1932).

Works of this period included important novels on the dehumanising aspects of scientific progress, most famously Brave New World, and on pacifist themes (for example, Eyeless in Gaza). In Brave New World, set in a dystopian London, Huxley portrays a society operating on the principles of mass production and Pavlovian conditioning. Huxley was strongly influenced by F. Matthias Alexander and included him as a character in Eyeless in Gaza.

Starting from this period, Huxley began to write and edit non-fiction works on pacifist issues, including Ends and Means, An Encyclopedia of Pacifism, and Pacifism and Philosophy, and was an active member of the Peace Pledge Union.

In 1937, Huxley moved to Hollywood, with his wife Maria, son Matthew, and friend Gerald Heard. He lived in the United States, mainly in southern California, until his death, but also for a time in Taos, New Mexico, where he wrote Ends and Means (published in 1937). In this work he examines the fact that although most people in modern civilisation agree that they want a world of "liberty, peace, justice, and brotherly love", they have not been able to agree on how to achieve it.

Heard introduced Huxley to Vedanta (Upanishad-centered philosophy), meditation, and vegetarianism through the principle of ahimsa. In 1938, Huxley befriended Jiddu Krishnamurti, whose teachings he greatly admired. He also became a Vedantist in the circle of Hindu Swami Prabhavananda, and introduced Christopher Isherwood to this circle. Not long after, Huxley wrote his book on widely held spiritual values and ideas, The Perennial Philosophy, which discussed the teachings of renowned mystics of the world. Huxley's book affirmed a sensibility that insists there are realities beyond the generally accepted "five senses" and that there is genuine meaning for humans beyond both sensual satisfactions and sentimentalities.

Huxley became a close friend of Remsen Bird, president of Occidental College. He spent much time at the college, which is in the Eagle Rock neighbourhood of Los Angeles. The college appears as "Tarzana College" in his satirical novel After Many a Summer (1939). The novel won Huxley that year's James Tait Black Memorial Prize for fiction. Huxley also incorporated Bird into the novel.

During this period, Huxley earned a substantial income as a Hollywood screenwriter; Christopher Isherwood, in his autobiography My Guru and His Disciple, states that Huxley earned over $3,000 per week (an enormous sum in those days) as a screenwriter, and that he used much of it to bring over Jewish and left-wing writer and artist refugees from Hitler's Germany to the US. In March 1938, his friend Anita Loos, a novelist and screenwriter, put him in touch with Metro-Goldwyn-Mayer who hired Huxley for Madame Curie which was originally to star Greta Garbo and be directed by George Cukor. (The film was eventually completed by MGM in 1943 with a different director and cast.) Huxley received screen credit for Pride and Prejudice (1940) and was paid for his work on a number of other films, including Jane Eyre (1944).  Huxley was commissioned by Walt Disney in 1945 to write a script based on Aliceâ€™s Adventures in Wonderland and the biography of the story's author Lewis Carroll.  The script was not used, however.

Huxley wrote an introduction to the posthumous publication of JD Unwin's 1940 book Hopousia or The Sexual and Economic Foundations of a New Society.

On 21 October 1949, Huxley wrote to George Orwell, author of Nineteen Eighty-Four, congratulating him on "how fine and how profoundly important the book is". In his letter to Orwell, he predicted:
Huxley had deeply felt apprehensions about the future the developed world might make for itself. From these, he made some warnings in his writings and talks. In a 1958 televised interview conducted by journalist Mike Wallace, Huxley outlined several major concerns: the difficulties and dangers of world overpopulation; the tendency toward distinctly hierarchical social organisation; the crucial importance of evaluating the use of technology in mass societies susceptible to wily persuasion; the tendency to promote modern politicians, to a naive public, as well-marketed commodities.

After World War II, Huxley applied for United States citizenship. His application was continuously deferred on the grounds that he would not say he would take up arms to defend the US. He claimed a philosophical, rather than a religious objection, and therefore was not exempt under the McCarran Act. He withdrew his application. Nevertheless, he remained in the country; and in 1959 he turned down an offer of a Knight Bachelor by the Macmillan government.

Beginning in 1939 and continuing until his death in 1963, Huxley had an extensive association with the Vedanta Society of Southern California, founded and headed by Swami Prabhavananda. Together with Gerald Heard, Christopher Isherwood, and other followers he was initiated by the Swami and was taught meditation and spiritual practices.

In 1944, Huxley wrote the introduction to the "Bhagavad Gita: The Song of God", translated by Swami Prabhavanada and Christopher Isherwood, which was published by The Vedanta Society of Southern California.

From 1941 until 1960, Huxley contributed 48 articles to Vedanta and the West, published by the Society. He also served on the editorial board with Isherwood, Heard, and playwright John van Druten from 1951 through 1962.

Huxley also occasionally lectured at the Hollywood and Santa Barbara Vedanta temples. Two of those lectures have been released on CD: Knowledge and Understanding and Who Are We from 1955. Nonetheless, Huxley's agnosticism, together with his speculative propensity, made it difficult for him to fully embrace any form of institutionalized religion. After the publication of The Doors of Perception, Huxley and the Swami disagreed about the meaning and importance of the Mescaline drug experience, which may have caused the relationship to cool, but Huxley continued to write articles for the Society's journal, lecture at the temple, and attend social functions.

There are differing accounts about the details of the quality of Huxley's eyesight at specific points in his life. Around 1939, Huxley encountered the Bates method for better eyesight, and a teacher, Margaret Darst Corbett, who was able to teach him in the method. In 1940, Huxley relocated from Hollywood to a  ranchito in the high desert hamlet of Llano, California, in northernmost Los Angeles County. Huxley then said that his sight improved dramatically with the Bates Method and the extreme and pure natural lighting of the southwestern American desert. He reported that, for the first time in over 25 years, he was able to read without glasses and without strain. He even tried driving a car along the dirt road beside the ranch. He wrote a book about his successes with the Bates Method, The Art of Seeing, which was published in 1942 (US), 1943 (UK). The book contained some generally disputed theories, and its publication created a growing degree of popular controversy about Huxley's eyesight.

It was, and is, widely believed that Huxley was nearly blind since the illness in his teens, despite the partial recovery which had enabled him to study at Oxford. For example, some ten years after publication of The Art of Seeing, in 1952, Bennett Cerf was present when Huxley spoke at a Hollywood banquet, wearing no glasses and apparently reading his paper from the lectern without difficulty: "Then suddenly he falteredâ€” and the disturbing truth became obvious. He wasn't reading his address at all. He had learned it by heart. To refresh his memory he brought the paper closer and closer to his eyes. When it was only an inch or so away he still couldn't read it, and had to fish for a magnifying glass in his pocket to make the typing visible to him. It was an agonising moment."

On the other hand, Huxley's second wife, Laura Archera Huxley, would later emphasise in her biographical account, This Timeless Moment: "One of the great achievements of his life: that of having regained his sight." After revealing a letter she wrote to the Los Angeles Times disclaiming the label of Huxley as a "poor fellow who can hardly see" by Walter C. Alvarez, she tempered this: "Although I feel it was an injustice to treat Aldous as though he were blind, it is true there were many indications of his impaired vision. For instance, although Aldous did not wear glasses, he would quite often use a magnifying lens." Laura Huxley proceeded to elaborate a few nuances of inconsistency peculiar to Huxley's vision. Her account, in this respect, is discernibly congruent with the following sample of Huxley's own words from The Art of Seeing: "The most characteristic fact about the functioning of the total organism, or any part of the organism, is that it is not constant, but highly variable." Nevertheless, the topic of Huxley's eyesight continues to endure similar, significant controversy, regardless of how trivial a subject matter it might initially appear.

American popular science author Steven Johnson, in his book Mind Wide Open, quotes Huxley about his difficulties with visual encoding: "I am and, for as long as I can remember, I have always been a poor visualizer.  Words, even the pregnant words of poets, do not evoke pictures in my mind. No hypnagogic visions greet me on the verge of sleep.  When I recall something, the memory does not present itself to me as a vividly seen event or object.  By an effort of the will, I can evoke a not very vivid image of what happened yesterday afternoon..."

Huxley married Maria Nys (10 September 1899â€“ 12 February 1955), a Belgian he met at Garsington, in 1919. They had one child, Matthew Huxley (19 April 1920â€“ 10 February 2005), who had a career as an author, anthropologist, and prominent epidemiologist.

In 1956, Huxley married Laura Archera (1911â€“2007), also an author. She wrote This Timeless Moment, a biography of Huxley. Laura felt inspired to illuminate the story of their marriage through Mary Ann Braubach's 2010 documentary, "Huxley on Huxley".

In 1960, Huxley was diagnosed with laryngeal cancer and, in the years that followed, with his health deteriorating, he wrote the Utopian novel Island, and gave lectures on "Human Potentialities" both at the University of California's San Francisco Medical Center and at the Esalen Institute, which lectures were fundamental to the beginning of the Human Potential Movement.

Huxley was a close friend of Jiddu Krishnamurti and Rosalind Rajagopal and was involved in the creation of the Happy Valley School (now Besant Hill School of Happy Valley) in Ojai, California.

The most substantial collection of Huxley's few remaining papers (following the destruction of most in a fire) is at the Library of the University of California, Los Angeles. Some are also at the Stanford University Libraries.

On his deathbed, unable to speak due to advanced laryngeal cancer, Huxley made a written request to his wife Laura for "LSD, 100Âµg, intramuscular". According to her account of his death in This Timeless Moment, she obliged with an injection at 11:45a.m. and a second dose a few hours later; Huxley died aged 69, at 5:20p.m. (17:20), on 22 November 1963.

Media coverage of Huxley's passingâ€” as with that of the author C. S. Lewisâ€” was overshadowed by the assassination of President John F. Kennedy on the same day. This coincidence served as the basis for Peter Kreeft's book , which imagines a conversation between the three men taking place in Purgatory following their deaths.

Huxley's ashes were interred in the family grave at the Watts Cemetery, home of the Watts Mortuary Chapel in Compton, a village near Guildford, Surrey, England.

Huxley had been a long-time friend of famous Russian composer Igor Stravinsky, who later dedicated his last orchestral composition to Huxley. Stravinsky began Variations in Santa FÃ©, New Mexico in July 1963, and completed the composition in Hollywood on 28 October 1964. It was first performed in Chicago on 17 April 1965, by the Chicago Symphony Orchestra conducted by Robert Craft.

1939 James Tait Black Memorial Prize 
1959 American Academy of Arts and Letters Award of Merit .
1962 Companion of Literature 

1968 Point Counter Point 
1971 The Devils 
1980 Brave New World 
1998 Brave New World 


1921 Crome Yellow
1923 Antic Hay
1925 Those Barren Leaves
1928 Point Counter Point
1932 Brave New World
1936 Eyeless in Gaza
1939 After Many a Summer
1944 Time Must Have a Stop
1948 Ape and Essence
1955 The Genius and the Goddess
1962 Island

1920 Limbo
1922 Mortal Coils
1924 Little Mexican 
1926 Two or Three Graces
1930 Brief Candles
1944 Collected Short Stories
1916 Oxford Poetry 
1916 The Burning Wheel
1917 Jonah
1918 The Defeat of Youth and Other Poems
1920 Leda
1925 Selected Poems
1929 Arabia Infelix and Other Poems
1931 The Cicadas and Other Poems
1971 Collected Poems

1923 On the Margin
1925 Along the Road
1926 Essays New and Old
1927 Proper Studies
1929 Do What You Will
1930 Vulgarity in Literature
1931 Music at Night
1932 Texts and Pretexts
1936 The Olive Tree and other essays
1937 Ends and Means
1940 Words and their Meanings
1942 The Art of Seeing
1945 The Perennial Philosophy
1946 Science, Liberty and Peace
1950 Themes and Variations
1954 The Doors of Perception
1956 Heaven and Hell
1956 Adonis and the Alphabet 
1958 Collected Essays
1958 Brave New World Revisited
1960 On Art and Artists
1963 Literature and Science
1977 Moksha: Writings on Psychedelics and the Visionary Experience 1931â€“63
1977 The Human Situation: Lectures at Santa Barbara, 1959

Brave New World
Ape and Essence
1940 Pride and Prejudice 
1943 Madame Curie 
1944 Jane Eyre 
1947 A Woman's Vengeance
1950 Prelude to Fame
1951 Original screenplay (rejected) for Disney's animated Alice in Wonderland
1971 Eyeless in Gaza 

1925 Along The Road: Notes and essays of a tourist
1926 
1934 

1967 The Crows of Pearblossom

1924 The Discovery 
1931 The World of Light
1948 Mortal Coils â€“ A Play 
1958 The Genius and the Goddess 
1967 The Ambassador of Captripedia
2000 Now More Than Ever 

1949 "Art and Religion
1950 "Foreword to an Essay on the Indian Philosophy of Peace"
1955 "Who Are We?"

1957 "The "Inanimate" is Alive"
1960 "Symbol and Immediate Experience"

1955 Knowledge and Understanding
1955 Who Are We?

1936 Pacifism and Philosophy
1937 An Encyclopedia of Pacifism 
1941 Grey Eminence
1953 The Devils of Loudun
1962 The Politics of Ecology
2007 Selected Letters


List of peace activists
Anderson, Jack 1982. "Ballet: Suzanne Farrell in 'Variations' Premiere". New York Times (4 July).
Barnes Clive. 1966. "Ballet: Still Another Balanchine-Stravinsky Pearl; City Troupe Performs in Premiere Here 'Variations' for Huxley at State Theater". New York Times (1 April): 28.
Spies, Claudio. 1965. "Notes on Stravinsky's Variations". Perspectives of New Music 4, no. 1 (Fall-Winter): 62â€“74. Reprinted in Perspectives on Schoenberg and Stravinsky, revised edition, edited by Benjamin Boretz and Edward T. Cone, . New York:W. W. Norton, 1972.
White, Eric Walter. 1979. Stravinsky: The Composer and His Works, second edition. Berkeley and Los Angeles: The University of California Press. ISBN 0-520-03985-8.
Atkins, John. Aldous Huxley: A Literary Study, J. Calder, 1956
Firchow, Peter. Aldous Huxley: Satirist and Novelist, U of Minnesota P, 1972
Firchow, Peter. The End of Utopia: A Study of Aldous Huxley's Brave New World, Bucknell UP, 1984
Huxley, Aldous. The Human Situation: Aldous Huxley Lectures at Santa Barbara 1959, Flamingo Modern Classic, 1994, ISBN 0-00-654732-X
Huxley, Laura Archera. This Timeless Moment, Celestial Arts, 2001, ISBN 0-89087-968-0
Meckier, Jerome. Aldous Huxley: Modern Satirical Novelist of ideas, Firchow and Nugel editors, LIT Verlag Berlin-Hamburg-MÃ¼nster, 2006, ISBN 3-8258-9668-4
Murray, Nicholas. Aldous Huxley, Macmillan, 2003, ISBN 0-312-30237-1
Rolo, Charles J. (ed.). The World of Aldous Huxley, Grosset Universal Library, 1947.
Sexton, James (ed.). Aldous Huxley: Selected Letters, Ivan R. Dee, 2007, ISBN 1-56663-629-9
Sawyer, Dana. Aldous Huxley, Crossroad Publishing Co., 2002, ISBN 0-8245-1987-6
Shaw, Jeffrey M. Illusions of Freedom: Thomas Merton and Jacques Ellul on Technology and the Human Condition. Eugene, OR: Wipf and Stock. 2014. ISBN 978-1625640581.
Watt, Conrad (ed.). Aldous Huxley, Routledge, 1997, ISBN 0-415-15915-6
 at the National Portrait Gallery
 on The Mike Wallace Interview 18 May 1958. (Video)
 at University of California, Los Angeles Library Special Collections.
 (1952)


Ada, ADA, or A.D.A. may refer to:
Ada (food), traditional Kerala delicacy, made with rice, coconut powder mix, and sugar


Ada (name), feminine given name (and list of people with the name)
St. Ada, 7th-century French abbess
Ada, Countess of Atholl (died 1264)
Ada, Countess of Holland
Ada de Warenne, Countess of Northumbria and Huntingdon (died 1178)
Ada of Caria, satrap deposed by her brother Idrieus and restored by Alexander the Great
Ada, sister of Charlemagne, for whom the Ada Gospels at Trier were produced
Ada Lovelace (Augusta Ada King, Countess of Lovelace) (1815â€“1852), English mathematician and writer


Ada, Croatia, village in Croatia, municipality Å odolovci, Vukovar-Syrmia County
Ada, Serbia, town and municipality

Ada, Ghana, or Ada Foah
Ada (Ghana parliament constituency)
Ada, Delta, Isoka town in Delta State, Nigeria
Ada Estate, luxury residential suburb of Dar es Salaam, Tanzania

Ada, Iran, a village in West Azerbaijan Province
Ada, Karaman, a village in the central district (Karaman) of Karaman Province, Turkey

Ada, Alabama, unincorporated community in Montgomery County, Alabama
Ada County, Idaho
Ada, Minnesota, Norman County, Minnesota
Ada, Ohio, Hardin County, Ohio
Ada, Oklahoma, Pontotoc County, Oklahoma
Ada, Oregon, Lane County, Oregon
Ada, West Virginia, Mercer County, West Virginia
Ada, Wisconsin, Sheboygan County, Wisconsin

, 1969 novel by Vladimir Nabokov
Alternative Distribution Alliance, music distributor owned by Warner Music Group

Ada TV, broadcasts in both satellite and terrestrial in the Turkish Republic of Northern Cyprus
Ada (film), 1961 film by Daniel Mann
ADA...A Way of Life, 2008 Bollywood musical by Tanvir Ahmed
Ada (dog actor), dog that played Colin on the sitcom Spaced

Ada (Castlevania), character in Castlevania: Legacy of Darkness
Ada Wong, character in the Resident Evil franchise
ADA, character in Zone of the Enders

Aeronautical Development Agency, agency of India's Ministry of Defence
Air Defense Artillery Branch (United States Army)
Amazon Development Agency, development agency in Brazil
American Decency Association, American political organization advocating against pornography and "indecent" media
Americans for Democratic Action, American liberal political advocacy organization
Americans with Disabilities Act of 1990, U.S. law that prohibits discrimination on the basis of disability
Amigos dos Amigos, drug cartel in the favelas of Rio de Janeiro
AngehÃ¶riger der Armee, or AdA, member of the Swiss Armed Forces
Anti-Deficiency Act, U.S. law that prohibits the federal government from incurring debts not authorized by Congress
Anti-Dumping Agreement, General Agreement on Tariffs and Trade (GATT) about anti-competitive dumping pricing practices
ArmÃ©e de l'Air or AdA, the French Air Force
Assistant District Attorney, US government attorney position
Austrian Development Agency, development aid agency
Australian Digital Alliance, copyright advocacy group
Association of Drainage Authorities, membership body for those involved in water level management in the United Kingdom

ADA collider, the first electronâ€“positron collider
Adenosine deaminase enzyme
Analog-to-digital converter/Digital-to-analog converter, seen as ADA or A/DA, conversion occurring in some electronic devices, often musical
Adangme language's ISO 639-2 code
Advanced Distribution Automation, the extension of intelligent control over an electrical power grid
After the Development of Agriculture, A.D.A., system of counting years, like Anno Domini and Anno Mundi
Ada (ship), wooden Ketch, wrecked near Newcastle, New South Wales, Australia
USS Little Ada (1864), steamer captured by the Union Navy during the American Civil War
Azodicarbonamide, a food additive also used in plastics production

523 Ada, minor planet orbiting the Sun

Ada (orchid), genus of orchids
Ada aurantiaca, species of orchid and the type species of its genus
Ada keiliana, species of orchid

Ada (programming language), programming language based on Pascal
Ada (computer virus)
Apple Design Awards

Adenosine deaminase, enzyme involved in purine metabolism
Ada (protein), enzyme induced by treatment of bacterial cells
Ada regulon, one of the Escherichia coli adaptive response proteins

Cyclone Ada (disambiguation), two tropical cyclones named Ada

AdA (physics) or Istituto Nazionale di Fisica Nucleare (National Institute for Nuclear Physics), Italy
American Dental Association
American Diabetes Association
American Dietetic Association

Ada Air, regional airline based in Tirana, Albania
Ada International Airport or Saipan International Airport, Saipan Island, Northern Mariana Islands
AerolÃ­nea de Antioquia, Colombian Airline
Airline Deregulation Act, 1978 US bill removing governmental control from commercial aviation
Adana ÅžakirpaÅŸa Airport's IATA code

Adah (disambiguation)
Adha (disambiguation)



Aberdeen is a city in Scotland, United Kingdom.

Aberdeen may also refer to:
Sierra Leone
Aberdeen, Sierra Leone
South Africa
Aberdeen, Eastern Cape

Hong Kong
Aberdeen, Hong Kong
Sri Lanka
Aberdeen Falls, a waterfall in Sri Lanka.

Aberdeen, New South Wales
Aberdeen, Tasmania

Aberdeen (UK Parliament constituency) 1832-1885
Aberdeen Burghs (UK Parliament constituency) 1801-1832
Aberdeen Central (Scottish Parliament constituency)
Aberdeen Central (UK Parliament constituency)
County of Aberdeen, a historic county of Scotland whose county town was Aberdeen

Canada
Aberdeen, Kamloops, an area in the City of Kamloops, British Columbia
Aberdeen, Abbotsford, a neighbourhood in the City of Abbotsford, British Columbia
Aberdeen Parish, New Brunswick
Aberdeen, Nova Scotia, part of the Municipality of Inverness County, Nova Scotia
Aberdeen, Grey County, Ontario
Aberdeen, Saskatchewan
Aberdeen No. 373, Saskatchewan
Aberdeen, community in the township of Champlain, Prescott and Russell County, Ontario
Aberdeen Township, Quebec, until 1960 part of Sheen-Esher-Aberdeen-et-Malakoff, now part of Rapides-des-Joachims, Quebec
United States of America
Aberdeen, Arkansas, 
Ab
